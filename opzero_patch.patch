diff '--color=auto' -uprN linux-5.9.1/arch/x86/kvm/Makefile pm-opzero/arch/x86/kvm/Makefile
--- linux-5.9.1/arch/x86/kvm/Makefile	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/arch/x86/kvm/Makefile	2022-12-12 17:21:17.544563406 +0900
@@ -1,5 +1,5 @@
 # SPDX-License-Identifier: GPL-2.0
-
+KASAN_SANITIZE := n
 ccflags-y += -Iarch/x86/kvm
 ccflags-$(CONFIG_KVM_WERROR) += -Werror
 
diff '--color=auto' -uprN linux-5.9.1/arch/x86/Makefile pm-opzero/arch/x86/Makefile
--- linux-5.9.1/arch/x86/Makefile	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/arch/x86/Makefile	2022-12-12 17:21:17.508563240 +0900
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0
 # Unified Makefile for i386 and x86_64
-
+KASAN_SANITIZE := n
 # select defconfig based on actual architecture
 ifeq ($(ARCH),x86)
   ifeq ($(shell uname -m),x86_64)
diff '--color=auto' -uprN linux-5.9.1/drivers/nvdimm/bus.c pm-opzero/drivers/nvdimm/bus.c
--- linux-5.9.1/drivers/nvdimm/bus.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/drivers/nvdimm/bus.c	2022-12-12 17:21:19.048570277 +0900
@@ -1100,6 +1100,8 @@ static int __nd_ioctl(struct nvdimm_bus
 			copy = 0;
 		if (copy && copy_from_user(&in_env[in_len], p + in_len, copy)) {
 			rc = -EFAULT;
+			printk(KERN_ERR "ERROR HERE1: copy(%u), in_len(%u) "
+			"in_size(%u)\n", copy, in_len, in_size);
 			goto out;
 		}
 		in_len += in_size;
@@ -1127,6 +1129,7 @@ static int __nd_ioctl(struct nvdimm_bus
 		if (out_size == UINT_MAX) {
 			dev_dbg(dev, "%s unknown output size cmd: %s field: %d\n",
 					dimm_name, cmd_name, i);
+			printk(KERN_ERR "ERROR HERE2\n");
 			rc = -EFAULT;
 			goto out;
 		}
@@ -1136,6 +1139,7 @@ static int __nd_ioctl(struct nvdimm_bus
 			copy = 0;
 		if (copy && copy_from_user(&out_env[out_len],
 					p + in_len + out_len, copy)) {
+			printk(KERN_ERR "ERROR HERE3\n");
 			rc = -EFAULT;
 			goto out;
 		}
@@ -1157,6 +1161,7 @@ static int __nd_ioctl(struct nvdimm_bus
 	}
 
 	if (copy_from_user(buf, p, buf_len)) {
+			printk(KERN_ERR "ERROR HERE4\n");
 		rc = -EFAULT;
 		goto out;
 	}
@@ -1178,8 +1183,10 @@ static int __nd_ioctl(struct nvdimm_bus
 				clear_err->cleared);
 	}
 
-	if (copy_to_user(p, buf, buf_len))
+	if (copy_to_user(p, buf, buf_len)) {
+			printk(KERN_ERR "ERROR HERE5\n");
 		rc = -EFAULT;
+	}
 
 out_unlock:
 	nvdimm_bus_unlock(dev);
@@ -1190,11 +1197,201 @@ out:
 	vfree(buf);
 	return rc;
 }
+int my_memcpy(void *dest, const void *src, size_t len) 
+{
+	memcpy(dest,src,len);
+	return 0;
+}
+static int dev__nd_ioctl(struct nvdimm_bus *nvdimm_bus, struct nvdimm *nvdimm,
+		int read_only, unsigned int ioctl_cmd, unsigned long arg)
+{
+	struct nvdimm_bus_descriptor *nd_desc = nvdimm_bus->nd_desc;
+	const struct nd_cmd_desc *desc = NULL;
+	unsigned int cmd = _IOC_NR(ioctl_cmd);
+	struct device *dev = &nvdimm_bus->dev;
+	void __user *p = (void __user *) arg;
+	char *out_env = NULL, *in_env = NULL;
+	const char *cmd_name, *dimm_name;
+	u32 in_len = 0, out_len = 0;
+	unsigned int func = cmd;
+	unsigned long cmd_mask;
+	struct nd_cmd_pkg pkg;
+	int rc, i, cmd_rc;
+	void *buf = NULL;
+	u64 buf_len = 0;
+
+	if (nvdimm) {
+		desc = nd_cmd_dimm_desc(cmd);
+		cmd_name = nvdimm_cmd_name(cmd);
+		cmd_mask = nvdimm->cmd_mask;
+		dimm_name = dev_name(&nvdimm->dev);
+	} else {
+		desc = nd_cmd_bus_desc(cmd);
+		cmd_name = nvdimm_bus_cmd_name(cmd);
+		cmd_mask = nd_desc->cmd_mask;
+		dimm_name = "bus";
+	}
+
+	/* Validate command family support against bus declared support */
+	if (cmd == ND_CMD_CALL) {
+		unsigned long *mask;
+
+		if (copy_from_user(&pkg, p, sizeof(pkg)))
+			return -EFAULT;
+
+		if (nvdimm) {
+			if (pkg.nd_family > NVDIMM_FAMILY_MAX)
+				return -EINVAL;
+			mask = &nd_desc->dimm_family_mask;
+		} else {
+			if (pkg.nd_family > NVDIMM_BUS_FAMILY_MAX)
+				return -EINVAL;
+			mask = &nd_desc->bus_family_mask;
+		}
+
+		if (!test_bit(pkg.nd_family, mask))
+			return -EINVAL;
+	}
+
+	if (!desc ||
+	    (desc->out_num + desc->in_num == 0) ||
+	    cmd > ND_CMD_CALL ||
+	    !test_bit(cmd, &cmd_mask))
+		return -ENOTTY;
+
+	/* fail write commands (when read-only) */
+	if (read_only)
+		switch (cmd) {
+		case ND_CMD_VENDOR:
+		case ND_CMD_SET_CONFIG_DATA:
+		case ND_CMD_ARS_START:
+		case ND_CMD_CLEAR_ERROR:
+		case ND_CMD_CALL:
+			dev_dbg(dev, "'%s' command while read-only.\n",
+					nvdimm ? nvdimm_cmd_name(cmd)
+					: nvdimm_bus_cmd_name(cmd));
+			return -EPERM;
+		default:
+			break;
+		}
+
+	/* process an input envelope */
+	in_env = kzalloc(ND_CMD_MAX_ENVELOPE, GFP_KERNEL);
+	if (!in_env)
+		return -ENOMEM;
+	for (i = 0; i < desc->in_num; i++) {
+		u32 in_size, copy;
+
+		in_size = nd_cmd_in_size(nvdimm, cmd, desc, i, in_env);
+		if (in_size == UINT_MAX) {
+			dev_err(dev, "%s:%s unknown input size cmd: %s field: %d\n",
+					__func__, dimm_name, cmd_name, i);
+			rc = -ENXIO;
+			goto out;
+		}
+		if (in_len < ND_CMD_MAX_ENVELOPE)
+			copy = min_t(u32, ND_CMD_MAX_ENVELOPE - in_len, in_size);
+		else
+			copy = 0;
+		if (copy && my_memcpy(&in_env[in_len], p + in_len, copy)) {
+			rc = -EFAULT;
+			printk(KERN_ERR "ERROR HERE1: copy(%u), in_len(%u) "
+			"in_size(%u)\n", copy, in_len, in_size);
+			goto out;
+		}
+		in_len += in_size;
+	}
+
+	if (cmd == ND_CMD_CALL) {
+		func = pkg.nd_command;
+		dev_dbg(dev, "%s, idx: %llu, in: %u, out: %u, len %llu\n",
+				dimm_name, pkg.nd_command,
+				in_len, out_len, buf_len);
+	}
+
+	/* process an output envelope */
+	out_env = kzalloc(ND_CMD_MAX_ENVELOPE, GFP_KERNEL);
+	if (!out_env) {
+		rc = -ENOMEM;
+		goto out;
+	}
+
+	for (i = 0; i < desc->out_num; i++) {
+		u32 out_size = nd_cmd_out_size(nvdimm, cmd, desc, i,
+				(u32 *) in_env, (u32 *) out_env, 0);
+		u32 copy;
+
+		if (out_size == UINT_MAX) {
+			dev_dbg(dev, "%s unknown output size cmd: %s field: %d\n",
+					dimm_name, cmd_name, i);
+			printk(KERN_ERR "ERROR HERE2\n");
+			rc = -EFAULT;
+			goto out;
+		}
+		if (out_len < ND_CMD_MAX_ENVELOPE)
+			copy = min_t(u32, ND_CMD_MAX_ENVELOPE - out_len, out_size);
+		else
+			copy = 0;
+		if (copy && my_memcpy(&out_env[out_len],
+					p + in_len + out_len, copy)) {
+			printk(KERN_ERR "ERROR HERE3\n");
+			rc = -EFAULT;
+			goto out;
+		}
+		out_len += out_size;
+	}
+
+	buf_len = (u64) out_len + (u64) in_len;
+	if (buf_len > ND_IOCTL_MAX_BUFLEN) {
+		dev_dbg(dev, "%s cmd: %s buf_len: %llu > %d\n", dimm_name,
+				cmd_name, buf_len, ND_IOCTL_MAX_BUFLEN);
+		rc = -EINVAL;
+		goto out;
+	}
+
+	buf = vmalloc(buf_len);
+	if (!buf) {
+		rc = -ENOMEM;
+		goto out;
+	}
+
+	if (my_memcpy(buf, p, buf_len)) {
+			printk(KERN_ERR "ERROR HERE4\n");
+		rc = -EFAULT;
+		goto out;
+	}
 
-enum nd_ioctl_mode {
-	BUS_IOCTL,
-	DIMM_IOCTL,
-};
+	nd_device_lock(dev);
+	nvdimm_bus_lock(dev);
+	rc = nd_cmd_clear_to_send(nvdimm_bus, nvdimm, func, buf);
+	if (rc)
+		goto out_unlock;
+
+	rc = nd_desc->ndctl(nd_desc, nvdimm, cmd, buf, buf_len, &cmd_rc);
+	if (rc < 0)
+		goto out_unlock;
+
+	if (!nvdimm && cmd == ND_CMD_CLEAR_ERROR && cmd_rc >= 0) {
+		struct nd_cmd_clear_error *clear_err = buf;
+
+		nvdimm_account_cleared_poison(nvdimm_bus, clear_err->address,
+				clear_err->cleared);
+	}
+
+	if (my_memcpy(p, buf, buf_len)) {
+			printk(KERN_ERR "ERROR HERE5\n");
+		rc = -EFAULT;
+	}
+
+out_unlock:
+	nvdimm_bus_unlock(dev);
+	nd_device_unlock(dev);
+out:
+	kfree(in_env);
+	kfree(out_env);
+	vfree(buf);
+	return rc;
+}
 
 static int match_dimm(struct device *dev, void *data)
 {
@@ -1255,6 +1452,49 @@ static long nd_ioctl(struct file *file,
 	return rc;
 }
 
+long dev_nd_ioctl(const char *name, unsigned int cmd, unsigned long arg,
+		enum nd_ioctl_mode mode)
+{
+	struct nvdimm_bus *nvdimm_bus, *found = NULL;
+	struct nvdimm *nvdimm = NULL;
+	int rc, ro;
+
+	ro = 0;
+	mutex_lock(&nvdimm_bus_list_mutex);
+	list_for_each_entry(nvdimm_bus, &nvdimm_bus_list, list) {
+		if (mode == DIMM_IOCTL) {
+			struct device *dev;
+
+			dev = device_find_child_by_name(&nvdimm_bus->dev,
+					name);
+			if (!dev)
+				continue;
+			nvdimm = to_nvdimm(dev);
+			found = nvdimm_bus;
+		}
+
+		if (found) {
+			atomic_inc(&nvdimm_bus->ioctl_active);
+			break;
+		}
+	}
+	mutex_unlock(&nvdimm_bus_list_mutex);
+
+	if (!found)
+		return -ENXIO;
+
+	nvdimm_bus = found;
+	rc = dev__nd_ioctl(nvdimm_bus, nvdimm, ro, cmd, arg);
+
+	if (nvdimm)
+		put_device(&nvdimm->dev);
+	if (atomic_dec_and_test(&nvdimm_bus->ioctl_active))
+		wake_up(&nvdimm_bus->wait);
+
+	return rc;
+}
+EXPORT_SYMBOL_GPL(dev_nd_ioctl);
+
 static long bus_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 {
 	return nd_ioctl(file, cmd, arg, BUS_IOCTL);
diff '--color=auto' -uprN linux-5.9.1/drivers/nvdimm/nd.h pm-opzero/drivers/nvdimm/nd.h
--- linux-5.9.1/drivers/nvdimm/nd.h	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/drivers/nvdimm/nd.h	2022-12-12 17:21:19.052570296 +0900
@@ -412,4 +412,11 @@ static inline bool is_bad_pmem(struct ba
 resource_size_t nd_namespace_blk_validate(struct nd_namespace_blk *nsblk);
 const u8 *nd_dev_to_uuid(struct device *dev);
 bool pmem_should_map_pages(struct device *dev);
+enum nd_ioctl_mode {
+	BUS_IOCTL,
+	DIMM_IOCTL,
+};
+
+long dev_nd_ioctl(const char *name, unsigned int cmd, unsigned long arg,
+		enum nd_ioctl_mode mode);
 #endif /* __ND_H__ */
diff '--color=auto' -uprN linux-5.9.1/fs/dax.c pm-opzero/fs/dax.c
--- linux-5.9.1/fs/dax.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/dax.c	2022-12-12 17:21:19.560572608 +0900
@@ -1046,7 +1046,6 @@ int dax_iomap_zero(loff_t pos, unsigned
 	void *kaddr;
 	bool page_aligned = false;
 
-
 	if (IS_ALIGNED(sector << SECTOR_SHIFT, PAGE_SIZE) &&
 	    IS_ALIGNED(size, PAGE_SIZE))
 		page_aligned = true;
@@ -1074,6 +1073,7 @@ int dax_iomap_zero(loff_t pos, unsigned
 	dax_read_unlock(id);
 	return 0;
 }
+EXPORT_SYMBOL(dax_iomap_zero);
 
 static loff_t
 dax_iomap_actor(struct inode *inode, loff_t pos, loff_t length, void *data,
diff '--color=auto' -uprN linux-5.9.1/fs/dcache.c pm-opzero/fs/dcache.c
--- linux-5.9.1/fs/dcache.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/dcache.c	2022-12-12 17:21:19.560572608 +0900
@@ -370,8 +370,12 @@ static void dentry_unlink_inode(struct d
 		fsnotify_inoderemove(inode);
 	if (dentry->d_op && dentry->d_op->d_iput)
 		dentry->d_op->d_iput(dentry, inode);
-	else
-		iput(inode);
+	else{
+		if (IS_DAX(inode))
+			iput_zero(inode);
+		else
+			iput(inode);
+	}
 }
 
 /*
diff '--color=auto' -uprN linux-5.9.1/fs/delay_free_inode.c pm-opzero/fs/delay_free_inode.c
--- linux-5.9.1/fs/delay_free_inode.c	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/fs/delay_free_inode.c	2022-12-12 17:21:19.560572608 +0900
@@ -0,0 +1,415 @@
+#include "delay_free_inode.h"
+
+static struct task_struct *thread;
+static struct list_head block_list;
+static struct kmem_cache* allocator; 
+static struct kmem_cache* ext4_free_data_cachep; 
+static spinlock_t iq_lock;
+static unsigned long count_free_blocks;
+static unsigned long num_free_blocks;
+static int thread_running;
+static int thread_control;
+
+static int kt_free_inode(void);
+
+static struct kobject *frinode_kobj;
+
+struct frinode_attr{
+	struct kobj_attribute attr;
+	int value;
+};
+
+static struct frinode_attr frinode_value;
+static struct frinode_attr frinode_notify;
+
+static struct attribute *frinode_attrs[] = {
+	&frinode_value.attr.attr,
+	&frinode_notify.attr.attr,
+	NULL
+};
+
+static struct attribute_group frinode_group = {
+	.attrs = frinode_attrs,
+};
+
+static ssize_t frinode_show(struct kobject *kobj, struct kobj_attribute *attr,
+		char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "Number of free-ed inode: %lu\nThread running: %d\n", 
+			count_free_blocks, thread_running);
+
+}
+
+static ssize_t frinode_store(struct kobject *kobj, struct kobj_attribute *attr,
+		const char *buf, size_t len)
+{
+	struct frinode_attr *frinode = container_of(attr, struct frinode_attr, attr);
+	int was_on = 0;
+	if (frinode -> value)
+		was_on = 1;
+
+	sscanf(buf, "%d", &frinode->value);
+	sysfs_notify(frinode_kobj, NULL, "frinode_notify");
+	if(frinode->value) {
+		if (was_on == 0) {
+			thread_control = 1;
+			thread = kthread_create((int(*)(void*))kt_free_inode, NULL,
+						 "kt_free_inode");
+			wake_up_process(thread);
+		}
+	}
+	else if (frinode -> value == 0) {
+		if (was_on)
+			thread_control = 0;
+	}
+	return len;
+}
+
+static struct frinode_attr frinode_value = {
+	.attr = __ATTR(frinode_value, 0644, frinode_show, frinode_store),
+	.value = 0,
+};
+
+static struct frinode_attr frinode_notify = {
+	.attr = __ATTR(frinode_notify, 0644, frinode_show, frinode_store),
+	.value = 0,
+};
+
+
+/* read_inode put inode in to the queue 
+ * so that the thread can periodically wake up
+ * and truncate the inode that needs to be deleted
+ */
+//void delay_iput(struct inode *inode)
+//{
+//	struct free_inode_t *new = kmem_cache_alloc(allocator, GFP_KERNEL);
+//	if(new) 
+//		new -> inode = inode;
+//	spin_lock(&iq_lock);
+//	list_add_tail(&new->ls, &block_list);
+//	spin_unlock(&iq_lock);
+//}
+//EXPORT_SYMBOL(delay_iput);
+
+/* ext4_delay_free_block
+ * Delay the call for ext4_free_block
+ * 
+ */
+
+void ext4_delay_free_inode(struct inode * inode, ext4_fsblk_t block, 
+		unsigned long count, int flag)
+{
+	/* 
+	 * Here we need to put the information into the list.
+	 * Need to use kmem_cache_alloc
+	 * */
+	struct free_block_t* new = kmem_cache_alloc(allocator, GFP_KERNEL);
+	if(new) {
+		new -> inode = inode;
+		new -> block = block;
+		new -> flag = flag;
+		new -> count = count;
+	}
+
+	spin_lock(&iq_lock);
+	list_add_tail(&new -> ls, &block_list);
+	spin_unlock(&iq_lock);
+
+}
+EXPORT_SYMBOL(ext4_delay_free_inode);
+
+static int free_blocks(struct free_block_t *entry)
+{
+	struct inode *inode = entry -> inode;
+	ext4_fsblk_t block = entry -> block;
+	unsigned long count = entry -> count;
+	int flags = flags;
+	unsigned int overflow;
+	struct super_block *sb = inode -> i_sb; 
+	struct ext4_group_desc *gdp;
+	ext4_grpblk_t bit;
+	ext4_group_t block_group;
+	struct buffer_head *gd_bh;
+	struct buffer_head *bitmap_bh = NULL;
+	struct ext4_sb_info *sbi = EXT4_SB(sb);
+	struct ext4_buddy e4b;
+	unsigned int count_clusters;
+	int err = 0;
+	int ret;
+	unsigned int credits;
+	handle_t *handle = NULL;
+do_more:
+	overflow = 0;
+	ext4_get_group_no_and_offset(sb, block, &block_group,
+			&bit);
+	/* We need to free blocks in the list one by one
+	 * Need to clear the block bitmap, zeroout the blocks we
+	 * have, and change the associated group descriptor
+	 * */
+	if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT( ext4_get_group_info(sb,
+						block_group))))
+		return 1;
+	if (EXT4_C2B(sbi, bit) + count > EXT4_BLOCKS_PER_GROUP(sb)) {
+		overflow = EXT4_C2B(sbi, bit) + count -
+			EXT4_BLOCKS_PER_GROUP(sb);
+		count -= overflow;
+	}
+	count_clusters = EXT4_NUM_B2C(sbi, count);
+	bitmap_bh = ext4_read_block_bitmap(sb, block_group);
+	if (IS_ERR(bitmap_bh)) {
+		err = PTR_ERR(bitmap_bh);
+		bitmap_bh = NULL;
+		goto error_return;
+	}
+	gdp = ext4_get_group_desc(sb, block_group, &gd_bh);
+	if (!gdp) {
+		err = -EIO;
+		goto error_return;
+	}
+
+	if (in_range(ext4_block_bitmap(sb, gdp), block, count) ||
+			in_range(ext4_inode_bitmap(sb, gdp), block, count) ||
+			in_range(block, ext4_inode_table(sb, gdp),
+				sbi->s_itb_per_group) ||
+			in_range(block + count - 1, ext4_inode_table(sb, gdp),
+				sbi->s_itb_per_group)) {
+
+		ext4_error(sb, "Freeing blocks in system zone - "
+				"Block = %llu, count = %lu", block, count);
+		/* err = 0. ext4_std_error should be a no op */
+		goto error_return;
+	}
+
+	/* Modified for zeroout data blocks while trucate for dax
+	 * */
+	if (IS_DAX(inode)) {
+		/* use iomap_zero_range need to find from and length */
+		struct iomap dax_iomap, srcmap;
+		loff_t written;
+		dax_iomap.addr = block << inode->i_blkbits;
+		dax_iomap.offset = 0;
+		dax_iomap.bdev = inode -> i_sb -> s_bdev;
+		dax_iomap.dax_dev = EXT4_SB(inode -> i_sb)->s_daxdev;
+		srcmap.type = 2;
+
+		written = iomap_zero_range_actor(inode, 0, inode->i_sb->s_blocksize*count, 
+				NULL, &dax_iomap, &srcmap);
+	}
+
+	/* New handle for journaling
+	 * */
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		credits = ext4_writepage_trans_blocks(inode);
+	else
+		credits = ext4_blocks_for_truncate(inode);
+	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
+
+	BUFFER_TRACE(bitmap_bh, "getting write access");
+	err = ext4_journal_get_write_access(handle, bitmap_bh);
+	if (err)
+		goto error_return;
+
+	/*
+	 * We are about to modify some metadata.  Call the journal APIs
+	 * to unshare ->b_data if a currently-committing transaction is
+	 * using it
+	 */
+	BUFFER_TRACE(gd_bh, "get_write_access");
+	err = ext4_journal_get_write_access(handle, gd_bh);
+	if (err)
+		goto error_return;
+#ifdef AGGRESSIVE_CHECK
+	{
+		int i;
+		for (i = 0; i < count_clusters; i++)
+			BUG_ON(!mb_test_bit(bit + i, bitmap_bh->b_data));
+	}
+#endif
+	/* __GFP_NOFAIL: retry infinitely, ignore TIF_MEMDIE and memcg limit. */
+	err = ext4_mb_load_buddy_gfp(sb, block_group, &e4b,
+			GFP_NOFS|__GFP_NOFAIL);
+	if (err)
+		goto error_return;
+
+
+	/*
+	 * We need to make sure we don't reuse the freed block until after the
+	 * transaction is committed. We make an exception if the inode is to be
+	 * written in writeback mode since writeback mode has weak data
+	 * consistency guarantees.
+	 */
+	if (ext4_handle_valid(handle) &&
+			((flags & EXT4_FREE_BLOCKS_METADATA) ||
+			 !ext4_should_writeback_data(inode))) {
+		struct ext4_free_data *new_entry;
+		/*
+		 * We use __GFP_NOFAIL because ext4_free_blocks() is not allowed
+		 * to fail.
+		 */
+		new_entry = kmem_cache_alloc(ext4_free_data_cachep,
+				GFP_NOFS|__GFP_NOFAIL);
+		new_entry->efd_start_cluster = bit;
+		new_entry->efd_group = block_group;
+		new_entry->efd_count = count_clusters;
+		new_entry->efd_tid = handle->h_transaction->t_tid;
+
+		ext4_lock_group(sb, block_group);
+		mb_clear_bits(bitmap_bh->b_data, bit, count_clusters);
+		ext4_mb_free_metadata(handle, &e4b, new_entry);
+	} else {
+		/* need to update group_info->bb_free and bitmap
+		 * with group lock held. generate_buddy look at
+		 * them with group lock_held
+		 */
+		if (test_opt(sb, DISCARD)) {
+			err = ext4_issue_discard(sb, block_group, bit, count,
+					NULL);
+			if (err && err != -EOPNOTSUPP)
+				ext4_msg(sb, KERN_WARNING, "discard request in"
+						" group:%d block:%d count:%lu failed"
+						" with %d", block_group, bit, count,
+						err);
+		} else
+			EXT4_MB_GRP_CLEAR_TRIMMED(e4b.bd_info);
+
+		ext4_lock_group(sb, block_group);
+		mb_clear_bits(bitmap_bh->b_data, bit, count_clusters);
+		mb_free_blocks(inode, &e4b, bit, count_clusters);
+	}
+
+	ret = ext4_free_group_clusters(sb, gdp) + count_clusters;
+	ext4_free_group_clusters_set(sb, gdp, ret);
+	ext4_block_bitmap_csum_set(sb, block_group, gdp, bitmap_bh);
+	ext4_group_desc_csum_set(sb, block_group, gdp);
+	ext4_unlock_group(sb, block_group);
+
+	if (sbi->s_log_groups_per_flex) {
+		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
+		atomic64_add(count_clusters,
+				&sbi_array_rcu_deref(sbi, s_flex_groups,
+					flex_group)->free_clusters);
+	}
+
+	/*
+	 * on a bigalloc file system, defer the s_freeclusters_counter
+	 * update to the caller (ext4_remove_space and friends) so they
+	 * can determine if a cluster freed here should be rereserved
+	 */
+	if (!(flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)) {
+		if (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE))
+			dquot_free_block(inode, EXT4_C2B(sbi, count_clusters));
+		percpu_counter_add(&sbi->s_freeclusters_counter,
+				count_clusters);
+	}
+
+	ext4_mb_unload_buddy(&e4b);
+
+	/* We dirtied the bitmap block */
+	BUFFER_TRACE(bitmap_bh, "dirtied bitmap block");
+	err = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);
+
+	/* And the group descriptor block */
+	BUFFER_TRACE(gd_bh, "dirtied group descriptor block");
+	ret = ext4_handle_dirty_metadata(handle, NULL, gd_bh);
+	if (!err)
+		err = ret;
+
+	ext4_journal_stop(handle);
+
+	if (overflow && !err) {
+		block += count;
+		count = overflow;
+		put_bh(bitmap_bh);
+		goto do_more;
+	}
+error_return:
+	brelse(bitmap_bh);
+	ext4_std_error(sb, err);
+	return 0;
+}
+static int kt_free_inode(void)
+{
+	thread_running = 1;
+	while(thread_control) {
+		/* Do rest of the free blocks */			
+		struct free_block_t *entry;
+		int err;
+		spin_lock(&iq_lock);
+		while (!list_empty(&block_list)) {
+			spin_unlock(&iq_lock);
+			err = 0;
+			spin_lock(&iq_lock);
+			entry = list_first_entry(&block_list,
+					struct free_inode_t, ls);
+			spin_unlock(&iq_lock);
+			if (entry -> inode == NULL) {
+				spin_lock(&iq_lock);
+				list_del(&entry -> ls);
+				spin_unlock(&iq_lock);
+				kmem_cache_free(allocator, entry);
+				spin_lock(&iq_lock);
+				continue;
+			}
+			num_free_blocks += entry->count;
+			err = free_blocks(entry);
+			if (err) {
+				printk(KERN_ERR "Free blocks error!!!!\n");
+			}
+
+			spin_lock(&iq_lock);
+			list_del(&entry -> ls);
+			spin_unlock(&iq_lock);
+
+			kmem_cache_free(allocator, entry);
+			count_free_blocks++;
+
+			/* This lock is for loop condition check */
+			spin_lock(&iq_lock);
+		}
+		spin_unlock(&iq_lock);
+		msleep(100);
+	}
+	thread_running = 0;
+	return 0;
+}
+
+static int __init kt_free_inode_init(void) 
+{
+	int ret = 0;
+	thread_control = 0;
+	thread_running = 0;
+	count_free_blocks = 0;
+	num_free_blocks = 0;
+	spin_lock_init(&iq_lock);
+	INIT_LIST_HEAD(&block_list);
+	ext4_free_data_cahcep = KMEM_CACHE(ext4_free_data,
+			SLAB_RECLAIM_ACCOUNT);
+	allocator = kmem_cache_create("delay_iput", sizeof(struct free_inode_t),
+			0, 0, NULL);
+	if(allocator == NULL) {
+		printk(KERN_ERR "%s: kmem cache create failed!!!!\n",__func__);
+		return -1;
+	}
+	frinode_kobj = kobject_create_and_add("free_inode", NULL);
+	ret = sysfs_create_group(frinode_kobj, &frinode_group);
+	if(ret) {
+		printk("%s: sysfs_create_group() failed. ret=%d\n", __func__,
+				ret);
+	}
+
+	return 0;
+}
+
+static void __exit kt_free_inode_cleanup(void)
+{
+	printk(KERN_ERR "Cleaning up kt_free_inode module...\n");
+	if(kthread_stop(thread)){
+		printk(KERN_ERR "kt_free_inode: Thread Stopped!\n");
+	}
+	kmem_cache_shrink(allocator);
+	kmem_cache_shrink(ext4_free_data_cachep);
+}
+
+
+module_init(kt_free_inode_init);
+module_exit(kt_free_inode_cleanup);
diff '--color=auto' -uprN linux-5.9.1/fs/delay_free_inode.h pm-opzero/fs/delay_free_inode.h
--- linux-5.9.1/fs/delay_free_inode.h	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/fs/delay_free_inode.h	2022-12-12 17:21:19.560572608 +0900
@@ -0,0 +1,34 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/delay.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/slab.h>
+#include <linux/export.h>
+#include <asm/spinlock.h>
+#include <linux/list.h>
+#include <linux/gfp.h>
+
+noinline_for_stack int ext4_mb_load_buddy_gfp(struct super_block *sb,
+		ext4_group_t group, struct ext4_buddy *e4b, gfp_t gfp);
+void ext4_mb_unload_buddy(struct ext4_buddy *e4b);
+void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,
+		    int first, int count);
+void mb_clear_bits(void *bm, int cur, int len);
+inline int ext4_issue_discard(struct super_block *sb,
+		ext4_group_t block_group, ext4_grpblk_t cluster, int count,
+		struct bio **biop);
+noinline_for_stack int ext4_mb_free_metadata(handle_t *handle,
+		struct ext4_buddy *e4b, struct ext4_free_data *new_entry);
+
+struct free_inode_t {
+	struct inode *inode;
+	ext4_fsblk_t block;
+	unsigned long count;
+	int flag;
+	struct list_head ls;
+};
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/acl.c pm-opzero/fs/ext4/acl.c
--- linux-5.9.1/fs/ext4/acl.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/acl.c	2022-12-12 17:21:19.568572644 +0900
@@ -259,6 +259,11 @@ retry:
 	}
 out_stop:
 	ext4_journal_stop(handle);
+	if(IS_DAX(inode)) {
+		if(error == -ENOSPC && ext4_should_retry_alloc_dax(inode->i_sb,
+			&retries, 1))
+			goto retry;
+	}
 	if (error == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))
 		goto retry;
 	return error;
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/balloc.c pm-opzero/fs/ext4/balloc.c
--- linux-5.9.1/fs/ext4/balloc.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/balloc.c	2022-12-12 17:21:19.568572644 +0900
@@ -562,6 +562,8 @@ ext4_read_block_bitmap(struct super_bloc
  * Check if filesystem has nclusters free & available for allocation.
  * On success return 1, return 0 on failure.
  */
+long get_num_pz_blocks(void);
+int ext4_free_num_blocks(long count);
 static int ext4_has_free_clusters(struct ext4_sb_info *sbi,
 				  s64 nclusters, unsigned int flags)
 {
@@ -580,17 +582,28 @@ static int ext4_has_free_clusters(struct
 	rsv = (ext4_r_blocks_count(sbi->s_es) >> sbi->s_cluster_bits) +
 	      resv_clusters;
 
-	if (free_clusters - (nclusters + rsv + dirty_clusters) <
-					EXT4_FREECLUSTERS_WATERMARK) {
-		free_clusters  = percpu_counter_sum_positive(fcc);
-		dirty_clusters = percpu_counter_sum_positive(dcc);
-	}
+//	if (free_clusters - (nclusters + rsv + dirty_clusters) <
+//					EXT4_FREECLUSTERS_WATERMARK) {
+//		free_clusters  = percpu_counter_sum_positive(fcc);
+//		dirty_clusters = percpu_counter_sum_positive(dcc);
+//	}
 	/* Check whether we have space after accounting for current
 	 * dirty clusters & root reserved clusters.
 	 */
 	if (free_clusters >= (rsv + nclusters + dirty_clusters))
 		return 1;
 
+	/* Check if pre-zero blocks are left
+	 * */
+	if( nclusters <= get_num_pz_blocks()) {
+		/* zero out and free those blocks 
+		 * */	
+		//call free blocks for nclusters
+		if(ext4_free_num_blocks(nclusters + dirty_clusters)) {
+			return 1;
+		}
+	}
+
 	/* Hm, nope.  Are (enough) root reserved clusters available? */
 	if (uid_eq(sbi->s_resuid, current_fsuid()) ||
 	    (!gid_eq(sbi->s_resgid, GLOBAL_ROOT_GID) && in_group_p(sbi->s_resgid)) ||
@@ -606,7 +619,7 @@ static int ext4_has_free_clusters(struct
 		if (free_clusters >= (nclusters + dirty_clusters))
 			return 1;
 	}
-
+	
 	return 0;
 }
 
@@ -644,6 +657,27 @@ int ext4_should_retry_alloc(struct super
 	jbd_debug(1, "%s: retrying operation after ENOSPC\n", sb->s_id);
 	jbd2_journal_force_commit_nested(EXT4_SB(sb)->s_journal);
 	return 1;
+}
+
+int ext4_should_retry_alloc_dax(struct super_block *sb, int *retries, unsigned int len)
+{
+	if(len <= get_num_pz_blocks()) {
+		if(ext4_free_num_blocks(len)) 
+		goto out;	
+	}
+
+	if (!ext4_has_free_clusters(EXT4_SB(sb), 1, 0) ||
+	    (*retries)++ > 1 ||
+	    !EXT4_SB(sb)->s_journal)
+		return 0;
+
+	smp_mb();
+	if (EXT4_SB(sb)->s_mb_free_pending == 0)
+		return 0;
+out:
+	jbd_debug(1, "%s: retrying operation after ENOSPC\n", sb->s_id);
+	jbd2_journal_force_commit_nested(EXT4_SB(sb)->s_journal);
+	return 1;
 }
 
 /*
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/delay_free_block.c pm-opzero/fs/ext4/delay_free_block.c
--- linux-5.9.1/fs/ext4/delay_free_block.c	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/fs/ext4/delay_free_block.c	2022-12-12 17:21:19.568572644 +0900
@@ -0,0 +1,716 @@
+/*
+ * Delay free block is mainly for zerooutting the blocks at the background. So
+ * that the allocation does not require zeroing. 
+ * */
+
+#include "delay_free_block_test.h"
+#include "linux/fs.h"
+
+
+static struct task_struct *thread;
+static struct task_struct *thread_monitoring;
+static struct list_head block_list;
+static struct kmem_cache* allocator;
+static struct kmem_cache* ndctl_alloc;
+static struct kmem_cache* ext4_free_data_cachep;
+static spinlock_t fb_list_lock; 
+static spinlock_t kt_free_lock; 
+static unsigned long count_free_blocks;
+static unsigned long num_free_blocks;
+static unsigned long num_freeing_blocks;
+static atomic64_t total_blocks;
+static unsigned long read_bytes, write_bytes;
+static unsigned long zspeed;
+static int thread_control;
+static struct ndctl_cmd *pcmd;
+static meminfo output[6];
+static meminfo res;
+static meminfo init_rw[6];
+static input_info input;
+static struct nd_cmd_vendor_tail *tail;
+static char *blkdev_name;
+struct block_device *blkdev;
+struct super_block *real_super;
+static struct free_block_t *tmp_entry;
+static int zero_ratio;
+static int rc;
+static int kt_free_block(void);
+static void monitor_media(void);
+static void flush(void);
+static int was_on = 0;
+void ext4_delay_free_block(struct inode * inode, ext4_fsblk_t block, 
+		unsigned long count, int flag);
+
+static struct kobject *frblk_kobj;
+
+struct frblk_attr{
+	struct kobj_attribute attr;
+	int value;
+};
+
+static struct frblk_attr frblk_value;
+static struct frblk_attr frblk_notify;
+static struct frblk_attr target_blkdev;
+
+static struct attribute *frblk_attrs[] = {
+	&frblk_value.attr.attr,
+	&frblk_notify.attr.attr,
+	&target_blkdev.attr.attr,
+	NULL
+};
+
+static struct attribute_group frblk_group = {
+	.attrs = frblk_attrs,
+};
+
+static ssize_t frblk_show(struct kobject *kobj, struct kobj_attribute *attr,
+		char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE,"MediaReads_0: %lu\n"
+					"MediaWrites_0: %lu\n"
+					"Zero Ratio: %d\n"
+					"Zero_speed: %lu\n",
+			read_bytes/(1<<20), write_bytes/(1<<20),
+			zero_ratio, zspeed);
+}
+
+static ssize_t frblk_store(struct kobject *kobj, struct kobj_attribute *attr,
+		const char *buf, size_t len)
+{
+	struct frblk_attr *frblk = container_of(attr, struct frblk_attr, attr);
+
+	sscanf(buf, "%d", &frblk->value);
+	sysfs_notify(frblk_kobj, NULL, "frblk_notify");
+	if (frblk->value == 1) {
+		if (was_on == 0) {
+			int i;
+			for(i = 0; i < 6; i++) {
+				char dev_name[6];
+				snprintf(dev_name, 6, "nmem%d", i);
+				rc = dev_nd_ioctl(dev_name, ND_IOCTL_VENDOR, 
+					(unsigned long)&pcmd->cmd_buf, DIMM_IOCTL);	
+				if(rc) {
+					printk(KERN_ERR "%s: Error on nd_ioctl\n", 
+						__func__);
+				}
+				memcpy(&init_rw[i], tail->out_buf, sizeof(meminfo));
+			}
+			//devnum = ((259 & 0xfff) << 20) | (1 & 0xff);
+			blkdev = lookup_bdev(blkdev_name);
+                        if (!blkdev) {
+                                printk(KERN_ERR "No block device\n");
+                                goto out;
+                        }
+			real_super = get_active_super(blkdev);
+                        if (!real_super) {
+                                printk(KERN_ERR "No super block for blkdev\n");
+                                goto out;
+                        }
+
+			thread_monitoring =
+				kthread_create((int(*)(void*))monitor_media, NULL,
+						"monitor_media");
+			thread = kthread_create((int(*)(void*))kt_free_block,
+					NULL, "kt_free_block");
+                        was_on = 1;
+			wake_up_process(thread_monitoring);
+			wake_up_process(thread);
+		}
+	} 
+	else if (frblk->value == 0) {
+		if (was_on) {
+			thread_control = 0;	
+                        was_on = 0;
+			flush();
+                        deactivate_super(real_super);
+                        blkdev = NULL;
+                        real_super = NULL;
+                }
+	}
+	else if (frblk->value == 3) {
+		//flush();
+	}
+out:
+	return len;
+}
+
+static struct frblk_attr frblk_value = {
+	.attr = __ATTR(frblk_value, 0644, frblk_show, frblk_store),
+	.value = 0,
+};
+
+static struct frblk_attr frblk_notify = {
+	.attr = __ATTR(frblk_notify, 0644, frblk_show, frblk_store),
+	.value = 0,
+};
+
+static ssize_t blkdevname_show(struct kobject *kobj, struct kobj_attribute *attr,
+		char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%s\n", blkdev_name);
+}
+
+static ssize_t blkdevname_store(struct kobject *kobj, struct kobj_attribute *attr,
+		const char *buf, size_t len)
+{
+	sscanf(buf, "%s", blkdev_name);
+        return len;
+}
+
+static struct frblk_attr target_blkdev = {
+	.attr = __ATTR(target_blkdev, 0644, blkdevname_show, blkdevname_store),
+	.value = 0,
+};
+
+/* Get total blocks in the free_block list
+ * */
+long get_num_pz_blocks(void)
+{
+	return atomic64_read(&total_blocks);
+}
+EXPORT_SYMBOL(get_num_pz_blocks);
+
+/* ext4_delay_free_block
+ * Store freeing blocks in to the list
+ */
+void ext4_delay_free_block(struct inode * inode, ext4_fsblk_t block, 
+		unsigned long count, int flag)
+{
+	
+	spin_lock(&fb_list_lock);
+	struct free_block_t *new = kmem_cache_alloc(allocator, GFP_KERNEL);
+	if(new) {
+		new -> inode = inode;
+		new -> block = block;
+		new -> flag = flag;
+		new -> count = count;
+	}
+
+	list_add_tail(&new -> ls, &block_list);
+	spin_unlock(&fb_list_lock);
+
+	atomic64_add(count, &total_blocks);
+}
+EXPORT_SYMBOL(ext4_delay_free_block);
+
+static inline loff_t 
+ext4_iomap_dax_zero_range(loff_t pos, loff_t count, struct iomap *iomap)
+{
+	loff_t written = 0;
+	int status;
+
+	do {
+		unsigned offset, bytes;
+
+		offset = offset_in_page(pos);
+		bytes = min_t(loff_t, PAGE_SIZE - offset, count);
+
+		status = dax_iomap_zero(pos, offset, bytes, iomap);
+		if (status < 0)
+			return status;
+
+		pos += bytes;
+		count -= bytes;
+		written += bytes;
+	} while (count > 0);
+
+	return written;
+}
+
+/* We need to free blocks in the list one by one
+ * Need to clear the block bitmap, zeroout the blocks we
+ * have, and change the associated group descriptor
+ */
+static int free_blocks(struct free_block_t *entry)
+{
+	struct inode *inode = entry -> inode;
+	ext4_fsblk_t block = entry -> block;
+	unsigned long count = entry -> count;
+	int flags = entry -> flag;
+	unsigned int overflow;
+	struct super_block *sb = real_super; 
+	struct ext4_group_desc *gdp;
+	ext4_grpblk_t bit;
+	ext4_group_t block_group;
+	struct buffer_head *gd_bh;
+	struct buffer_head *bitmap_bh = NULL;
+	struct ext4_sb_info *sbi = EXT4_SB(sb);
+	struct ext4_buddy e4b;
+	unsigned int count_clusters;
+	int err = 0;
+	int ret;
+	struct iomap dax_iomap;
+	loff_t written = 0;
+	handle_t *handle = NULL;
+
+do_more:
+	overflow = 0;
+	ext4_get_group_no_and_offset(sb, block, &block_group,
+			&bit);
+	/* We need to free blocks in the list one by one
+	 * Need to clear the block bitmap, zeroout the blocks we
+	 * have, and change the associated group descriptor
+	 * */
+	if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT( ext4_get_group_info(sb,
+						block_group))))
+		return 1;
+	if (EXT4_C2B(sbi, bit) + count > EXT4_BLOCKS_PER_GROUP(sb)) {
+		overflow = EXT4_C2B(sbi, bit) + count -
+			EXT4_BLOCKS_PER_GROUP(sb);
+		count -= overflow;
+	}
+	count_clusters = EXT4_NUM_B2C(sbi, count);
+	bitmap_bh = ext4_read_block_bitmap(sb, block_group);
+	if (IS_ERR(bitmap_bh)) {
+		err = PTR_ERR(bitmap_bh);
+		bitmap_bh = NULL;
+		goto error_return;
+	}
+	gdp = ext4_get_group_desc(sb, block_group, &gd_bh);
+	if (!gdp) {
+		err = -EIO;
+		goto error_return;
+	}
+
+//	if (in_range(ext4_block_bitmap(sb, gdp), block, count) ||
+//			in_range(ext4_inode_bitmap(sb, gdp), block, count) ||
+//			in_range(block, ext4_inode_table(sb, gdp),
+//				sbi->s_itb_per_group) ||
+//			in_range(block + count - 1, ext4_inode_table(sb, gdp),
+//				sbi->s_itb_per_group)) {
+//
+//		ext4_error(sb, "Freeing blocks in system zone - "
+//				"Block = %llu, count = %lu", block, count);
+//		/* err = 0. ext4_std_error should be a no op */
+//		goto error_return;
+//	}
+
+	/* Modified for zeroout data blocks while trucate for dax
+	 * */
+	/* use iomap_zero_range need to find from and length */
+	dax_iomap.addr = block << inode->i_blkbits;
+	dax_iomap.offset = 0;
+	dax_iomap.length = real_super->s_blocksize * count; 
+	dax_iomap.bdev = blkdev;
+	dax_iomap.dax_dev = fs_dax_get_by_bdev(blkdev);
+	written = ext4_iomap_dax_zero_range(0, dax_iomap.length, 
+				&dax_iomap);
+
+	/* New handle for journaling
+	 * */
+//	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+//		credits = ext4_writepage_trans_blocks(inode);
+//	else
+//		credits = ext4_blocks_for_truncate(inode);
+//	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
+
+	/* __GFP_NOFAIL: retry infinitely, ignore TIF_MEMDIE and memcg limit. */
+	err = ext4_mb_load_buddy_gfp(sb, block_group, &e4b,
+			GFP_NOFS|__GFP_NOFAIL);
+	if (err)
+		goto error_return;
+
+
+	/*
+	 * We need to make sure we don't reuse the freed block until after the
+	 * transaction is committed. We make an exception if the inode is to be
+	 * written in writeback mode since writeback mode has weak data
+	 * consistency guarantees.
+	 */
+	if (0 && ext4_handle_valid(handle) &&
+			((flags & EXT4_FREE_BLOCKS_METADATA) ||
+			 !ext4_should_writeback_data(inode))) {
+		struct ext4_free_data *new_entry;
+		/*
+		 * We use __GFP_NOFAIL because ext4_free_blocks() is not allowed
+		 * to fail.
+		 */
+		new_entry = kmem_cache_alloc(ext4_free_data_cachep,
+				GFP_NOFS|__GFP_NOFAIL);
+		new_entry->efd_start_cluster = bit;
+		new_entry->efd_group = block_group;
+		new_entry->efd_count = count_clusters;
+		new_entry->efd_tid = handle->h_transaction->t_tid;
+
+		ext4_lock_group(sb, block_group);
+		mb_clear_bits(bitmap_bh->b_data, bit, count_clusters);
+		ext4_mb_free_metadata(handle, &e4b, new_entry);
+	} else {
+		/* need to update group_info->bb_free and bitmap
+		 * with group lock held. generate_buddy look at
+		 * them with group lock_held
+		 */
+		if (test_opt(sb, DISCARD)) {
+			err = ext4_issue_discard(sb, block_group, bit, count,
+					NULL);
+			if (err && err != -EOPNOTSUPP)
+				ext4_msg(sb, KERN_WARNING, "discard request in"
+						" group:%d block:%d count:%lu failed"
+						" with %d", block_group, bit, count,
+						err);
+		} else
+			EXT4_MB_GRP_CLEAR_TRIMMED(e4b.bd_info);
+
+		ext4_lock_group(sb, block_group);
+		mb_clear_bits(bitmap_bh->b_data, bit, count_clusters);
+		mb_free_blocks(inode, &e4b, bit, count_clusters);
+	}
+
+	ret = ext4_free_group_clusters(sb, gdp) + count_clusters;
+	ext4_free_group_clusters_set(sb, gdp, ret);
+	ext4_block_bitmap_csum_set(sb, block_group, gdp, bitmap_bh);
+	ext4_group_desc_csum_set(sb, block_group, gdp);
+	ext4_unlock_group(sb, block_group);
+
+	if (sbi->s_log_groups_per_flex) {
+		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
+		atomic64_add(count_clusters,
+				&sbi_array_rcu_deref(sbi, s_flex_groups,
+					flex_group)->free_clusters);
+	}
+
+	/*
+	 * on a bigalloc file system, defer the s_freeclusters_counter
+	 * update to the caller (ext4_remove_space and friends) so they
+	 * can determine if a cluster freed here should be rereserved
+	 */
+	if (!(flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)) {
+		if (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE))
+			dquot_free_block_nodirty(inode, EXT4_C2B(sbi, count_clusters));
+		percpu_counter_add(&sbi->s_freeclusters_counter,
+				count_clusters);
+	}
+
+	ext4_mb_unload_buddy(&e4b);
+//	ext4_journal_stop(handle);
+
+	if (overflow && !err) {
+		block += count;
+		count = overflow;
+		put_bh(bitmap_bh);
+		goto do_more;
+	}
+error_return:
+	brelse(bitmap_bh);
+	ext4_std_error(sb, err);
+
+	/* TODO: For unlink inode, you should set inode new bit after all the
+	 * blocks have been freed and zeroed, to reuse that inode
+	 * Need to make a tree to store the count of contiguous blocks of inode
+	 * which are getting freed later and when the count is zero, inode can
+	 * be freed finally
+	 * */
+
+	return 0;
+}
+
+/* TODO: Free and zeroout those blocks for nclusters
+ * Number of pre-zero blocks must be checked before
+ * Called by ext4_has_free_clusters()
+ * */
+int ext4_free_num_blocks(long count) 
+{
+	struct free_block_t *entry;
+	int err;
+
+	while(count){
+		err = 0;
+//		printk(KERN_ERR "%s: in lock\n", __func__);
+//
+		spin_lock(&fb_list_lock);
+		entry = list_first_entry(&block_list,
+				struct free_block_t, ls);
+		//Check if this entry has more blocks than needed
+		if(count < entry->count) {
+			//Free only needed amount and
+			//update the entry info (pblk, count)
+			tmp_entry->inode = entry -> inode;
+			tmp_entry->block = entry -> block;
+			tmp_entry->count = count;
+
+			entry -> block += count;
+			entry -> count -= count;
+			spin_unlock(&fb_list_lock);
+
+			err = free_blocks(tmp_entry);
+			atomic64_sub(count, &total_blocks);
+
+			break;
+
+		} else { // Free this entry and subtract the amount from count
+			tmp_entry->inode = entry -> inode;
+			tmp_entry->block = entry -> block;
+			tmp_entry->count = entry -> count;
+			list_del(&entry -> ls);
+			kmem_cache_free(allocator, entry);
+			spin_unlock(&fb_list_lock);
+
+			count -= tmp_entry->count;
+			err = free_blocks(tmp_entry);
+			
+			atomic64_sub(tmp_entry->count, &total_blocks);
+		}
+	}
+	return 1;
+}
+EXPORT_SYMBOL(ext4_free_num_blocks);
+
+static int kt_free_block(void)
+{
+	while(was_on) {
+		if((long)atomic64_read(&total_blocks) >= 10000) {
+                        //spin_lock(&kt_free_lock);
+			ext4_free_num_blocks(10000);
+                        //spin_unlock(&kt_free_lock);
+			num_freeing_blocks += 10000;
+			if(zspeed > 40)
+				msleep(1000/(zspeed/40)-1);
+		} else {
+			msleep(10000);
+		}
+	}
+	//flush();
+        //thread_flushing = kthread_create((int(*)(void*))flush, NULL, "flush_thread");
+	//wake_up_process(thread_flushing);
+	/*
+	while(thread_control) {
+		//
+		struct free_block_t *entry;
+		int err;
+		spin_lock(&fb_list_lock);
+		while (!list_empty(&block_list) && thread_control) {
+			err = 0;
+			entry = list_first_entry(&block_list,
+					struct free_block_t, ls);
+			if (entry -> inode == NULL) {
+				list_del(&entry -> ls);
+				spin_unlock(&fb_list_lock);
+
+				atomic64_sub(entry->count, &total_blocks);
+
+				kmem_cache_free(allocator, entry);
+				spin_lock(&fb_list_lock);
+				continue;
+			}
+			list_del(&entry -> ls);
+			spin_unlock(&fb_list_lock);
+
+			num_freeing_blocks += entry->count;
+			err = free_blocks(entry);
+			num_free_blocks += entry->count;
+			
+			atomic64_sub(entry->count, &total_blocks);
+			kmem_cache_free(allocator, entry);
+			count_free_blocks++;
+
+		//
+			spin_lock(&fb_list_lock);
+		}
+		spin_unlock(&fb_list_lock);
+		msleep(1000);
+	}*/
+	return 0;
+}
+
+static void flush(void)
+{
+	struct free_block_t *entry;
+	int err;
+	spin_lock(&fb_list_lock);
+	while (!list_empty(&block_list)) {
+		err = 0;
+		entry = list_first_entry(&block_list,
+				struct free_block_t, ls);
+		if (entry -> inode == NULL) {
+			printk(KERN_ERR "NULL inode in flush\n");
+			list_del(&entry -> ls);
+			spin_unlock(&fb_list_lock);
+			atomic64_sub(entry-> count, &total_blocks);
+			spin_lock(&fb_list_lock);
+			kmem_cache_free(allocator, entry);
+			continue;
+		}
+		list_del(&entry -> ls);
+		spin_unlock(&fb_list_lock);
+
+		num_freeing_blocks += entry->count;
+		err = free_blocks(entry);
+		num_free_blocks += entry->count;
+		
+		atomic64_sub(entry->count, &total_blocks);
+		count_free_blocks++;
+
+		spin_lock(&fb_list_lock);
+		kmem_cache_free(allocator, entry);
+	}
+	spin_unlock(&fb_list_lock);
+	
+	//deactivate_super(real_super);
+	//blkdev = NULL;
+        //real_super = NULL;
+}
+
+static void monitor_media(void)
+{
+	struct ext4_sb_info *sbi;
+	uint64_t zblocks;
+	sbi = EXT4_SB(real_super);
+	zblocks = atomic64_read(&total_blocks);
+	while(was_on) {
+		int i;
+		//int idle = 0;
+		char dev_name[6];
+		u64 bfree, pz_blocks;
+		unsigned long read_write, zio, zfree;
+		res.MediaReads.Uint64 = 0;
+		res.MediaReads.Uint64_1 = 0;
+		res.MediaWrites.Uint64 = 0;
+		res.MediaWrites.Uint64_1 = 0;
+
+		for(i = 0; i < 6; i++) {
+			snprintf(dev_name, 6, "nmem%d", i);
+			rc = dev_nd_ioctl(dev_name, ND_IOCTL_VENDOR, 
+				(unsigned long)&pcmd->cmd_buf, DIMM_IOCTL);	
+		//	if(rc) 
+		//		printk(KERN_ERR "%s: Error on nd_ioctl\n", __func__);
+			memcpy(&output[i], tail->out_buf, sizeof(meminfo));
+			res.MediaReads.Uint64 += output[i].MediaReads.Uint64 -
+						 init_rw[i].MediaReads.Uint64;
+			res.MediaReads.Uint64_1 += output[i].MediaReads.Uint64_1
+						   - init_rw[i].MediaReads.Uint64_1;
+			res.MediaWrites.Uint64 += output[i].MediaWrites.Uint64 -
+						  init_rw[i].MediaWrites.Uint64;
+			res.MediaWrites.Uint64_1 += output[i].MediaWrites.Uint64_1
+						    - init_rw[i].MediaWrites.Uint64_1;
+			init_rw[i].MediaReads.Uint64 = output[i].MediaReads.Uint64;
+			init_rw[i].MediaReads.Uint64_1 = output[i].MediaReads.Uint64_1;
+			init_rw[i].MediaWrites.Uint64 = output[i].MediaWrites.Uint64;
+			init_rw[i].MediaWrites.Uint64_1 = output[i].MediaWrites.Uint64_1;
+
+		}
+		/* Calculate the current speed of read and write
+		 * Also needs to store before, current and take good care of
+		 * number of freed amount of zeroed blocks with current I/O
+		 * speed 
+		 * */
+		read_bytes = res.MediaReads.Uint64 * 64 < num_freeing_blocks*4096 ?
+				0 : res.MediaReads.Uint64 * 64
+					- num_freeing_blocks * 4096;
+		write_bytes = res.MediaWrites.Uint64 * 64 < num_freeing_blocks * 4096 ?
+				0 : res.MediaWrites.Uint64*64 
+					- num_freeing_blocks * 4096;
+		num_freeing_blocks = 0;
+	
+		read_write = (10*read_bytes/25+write_bytes)/(1<<20);
+		if (read_write >= 8000)
+			read_write = 8000;
+		zio = min_t(u64, 8000 - read_write, 4000);
+		bfree =	percpu_counter_sum_positive(&sbi->s_freeclusters_counter)  - 
+			percpu_counter_sum_positive(&sbi->s_dirtyclusters_counter);
+		pz_blocks = (u64) atomic64_read(&total_blocks);
+		zero_ratio = 100 * pz_blocks / ( bfree + pz_blocks );
+		
+		if(zero_ratio <= 10)
+			zfree = 150;
+		else if(zero_ratio <= 20)
+			zfree = 304;
+		else if(zero_ratio <= 30)
+			zfree = 464;
+		else if(zero_ratio <= 40)
+			zfree = 635;
+		else if(zero_ratio <= 50)
+			zfree = 823;
+		else if(zero_ratio <= 60)
+			zfree = 1039;
+		else if(zero_ratio <= 70)
+			zfree = 1300;
+		else if(zero_ratio <= 80)
+			zfree = 1647;
+		else if(zero_ratio <= 90)
+			zfree = 2208;
+		else
+			zfree = 3969;
+		zspeed = max(zio, zfree);
+		msleep(1000);
+	}
+}
+
+int __init kt_free_block_init(void) 
+{
+	int ret = 0;
+	size_t size;
+	rc = 0;
+	count_free_blocks = 0;
+	num_free_blocks = 0;
+	num_freeing_blocks = 0;
+	atomic64_set(&total_blocks, 0);
+	thread_control = 0;
+	input.MemoryPage = 0x0;
+	res.MediaReads.Uint64 = 0;
+	res.MediaReads.Uint64_1 = 0;
+	res.MediaWrites.Uint64 = 0;
+	res.MediaWrites.Uint64_1 = 0;
+	read_bytes = 0;
+	write_bytes = 0;
+	zspeed = 1;
+	spin_lock_init(&fb_list_lock);
+	spin_lock_init(&kt_free_lock);
+	INIT_LIST_HEAD(&block_list);
+	
+	ext4_free_data_cachep = KMEM_CACHE(ext4_free_data,
+			SLAB_RECLAIM_ACCOUNT);
+	allocator = kmem_cache_create("zero_waiting_block_list", sizeof(struct
+				free_block_t), 0, 0, NULL);
+	if(allocator == NULL) {
+		printk(KERN_ERR "Kmem_cache create failed!!!!\n");
+		return -1;
+	}
+        blkdev_name = kmalloc(30, GFP_KERNEL);
+        if (!blkdev_name) {
+		printk(KERN_ERR "kmalloc for dev name failed!!!!\n");
+		return -1;
+        }
+        strncpy(blkdev_name, "/dev/pmem0", 10);
+	tmp_entry = kmem_cache_alloc(allocator, GFP_KERNEL);
+
+	frblk_kobj = kobject_create_and_add("free_block", NULL);
+	ret = sysfs_create_group(frblk_kobj, &frblk_group);
+	if(ret) {
+		printk("%s: sysfs_create_group() failed. ret=%d\n", __func__,
+				ret);
+	}
+
+	/* Initialize nd_ioctl commands and buffer
+	 * */
+	size = sizeof(*pcmd) + sizeof(struct nd_cmd_vendor_hdr) 
+		+ sizeof(struct nd_cmd_vendor_tail) + 128 + 128;
+	ndctl_alloc = kmem_cache_create("pm_monitoring", size, 0, 0, NULL);
+	pcmd = kmem_cache_alloc(ndctl_alloc, GFP_KERNEL);
+	pcmd->type = ND_CMD_VENDOR;
+	pcmd->size = size;
+	pcmd->status = 1;
+	pcmd->vendor->opcode = (uint32_t) (0x03 << 8 | 0x08);
+	pcmd->vendor->in_length = 128;
+	memcpy(pcmd->vendor->in_buf, &input, sizeof(input_info));
+	tail = (struct nd_cmd_vendor_tail *) 
+		(pcmd->cmd_buf + sizeof(struct nd_cmd_vendor_hdr)
+		 + pcmd->vendor->in_length);
+	tail->out_length = (u32) 128;
+	return 1;
+}
+
+void __exit kt_free_block_cleanup(void)
+{
+	printk(KERN_INFO "Cleaning up kt_free_block module...\n");
+	kmem_cache_free(ndctl_alloc, pcmd);
+	kmem_cache_free(allocator, tmp_entry);
+	kmem_cache_shrink(ext4_free_data_cachep);
+        kfree(blkdev_name);
+	kmem_cache_shrink(allocator);
+	kmem_cache_shrink(ndctl_alloc);
+}
+
+
+module_init(kt_free_block_init);
+module_exit(kt_free_block_cleanup);
+
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/delay_free_block.h pm-opzero/fs/ext4/delay_free_block.h
--- linux-5.9.1/fs/ext4/delay_free_block.h	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/fs/ext4/delay_free_block.h	2022-12-12 17:21:19.568572644 +0900
@@ -0,0 +1,42 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/delay.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/slab.h>
+#include <linux/slab_def.h>
+#include <linux/export.h>
+#include <asm/spinlock.h>
+#include <linux/semaphore.h>
+#include <linux/list.h>
+#include <linux/gfp.h>
+#include <linux/iomap.h>
+#include <linux/quotaops.h>
+#include <linux/buffer_head.h>
+#include "ext4.h"
+#include "ext4_jbd2.h"
+#include "mballoc.h"
+#include "truncate.h"
+
+noinline_for_stack int ext4_mb_load_buddy_gfp(struct super_block *sb,
+		ext4_group_t group, struct ext4_buddy *e4b, gfp_t gfp);
+void ext4_mb_unload_buddy(struct ext4_buddy *e4b);
+void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,
+		    int first, int count);
+void mb_clear_bits(void *bm, int cur, int len);
+inline int ext4_issue_discard(struct super_block *sb,
+		ext4_group_t block_group, ext4_grpblk_t cluster, int count,
+		struct bio **biop);
+noinline_for_stack int ext4_mb_free_metadata(handle_t *handle,
+		struct ext4_buddy *e4b, struct ext4_free_data *new_entry);
+
+struct free_block_t {
+	struct inode * inode;
+	ext4_fsblk_t block;
+	int flag;
+	unsigned long count; 
+
+	struct list_head ls;
+};
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/delay_free_block_test.h pm-opzero/fs/ext4/delay_free_block_test.h
--- linux-5.9.1/fs/ext4/delay_free_block_test.h	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/fs/ext4/delay_free_block_test.h	2022-12-12 17:21:19.568572644 +0900
@@ -0,0 +1,90 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/kobject.h>
+#include <linux/sysfs.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/slab.h>
+#include <linux/slab_def.h>
+#include <linux/export.h>
+#include <asm/spinlock.h>
+#include <linux/semaphore.h>
+#include <linux/list.h>
+#include <linux/gfp.h>
+#include <linux/iomap.h>
+#include <linux/quotaops.h>
+#include <linux/buffer_head.h>
+#include <linux/mm.h>
+#include <linux/iomap.h>
+#include <linux/dax.h>
+#include "../../drivers/nvdimm/nd.h"
+#include "ext4.h"
+#include "ext4_jbd2.h"
+#include "mballoc.h"
+#include "truncate.h"
+
+noinline_for_stack int ext4_mb_load_buddy_gfp(struct super_block *sb,
+		ext4_group_t group, struct ext4_buddy *e4b, gfp_t gfp);
+void ext4_mb_unload_buddy(struct ext4_buddy *e4b);
+void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,
+		int first, int count);
+void mb_clear_bits(void *bm, int cur, int len);
+inline int ext4_issue_discard(struct super_block *sb,
+		ext4_group_t block_group, ext4_grpblk_t cluster, int count,
+		struct bio **biop);
+noinline_for_stack int ext4_mb_free_metadata(handle_t *handle,
+		struct ext4_buddy *e4b, struct ext4_free_data *new_entry);
+
+typedef struct {
+	uint64_t Uint64;
+	uint64_t Uint64_1;
+} UINT128;
+
+typedef uint8_t UINT8;
+
+typedef struct {
+	UINT128 MediaReads;
+	UINT128 MediaWrites;
+	UINT128 ReadRequests;
+	UINT128 WriteRequests;
+	UINT8 Reserved[64];
+} meminfo;
+
+typedef struct {
+	UINT8 MemoryPage;
+	UINT8 Reserved[127];
+} input_info;
+
+typedef struct { 
+	union {
+		meminfo test[0];
+		struct nd_cmd_vendor_hdr vendor[0];
+		char cmd_buf[0];
+	};
+} cmd;
+
+struct ndctl_cmd { 
+	int refcount;
+	int type;
+	int size;
+	int status;
+	union {
+		struct nd_cmd_get_config_size get_size[0];
+		struct nd_cmd_get_config_data_hdr get_data[0];
+		struct nd_cmd_set_config_hdr set_data[0];
+		struct nd_cmd_vendor_hdr vendor[0];
+		char cmd_buf[0];
+	};
+};
+
+struct free_block_t {
+	struct inode *inode;
+	ext4_fsblk_t block;
+	int flag;
+	unsigned long count; 
+	struct list_head ls;
+};
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/ext4.h pm-opzero/fs/ext4/ext4.h
--- linux-5.9.1/fs/ext4/ext4.h	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/ext4.h	2022-12-12 17:21:19.568572644 +0900
@@ -2476,6 +2476,8 @@ extern struct ext4_group_desc * ext4_get
 						    ext4_group_t block_group,
 						    struct buffer_head ** bh);
 extern int ext4_should_retry_alloc(struct super_block *sb, int *retries);
+extern int ext4_should_retry_alloc_dax(struct super_block *sb, int *retries,
+					unsigned int len);
 
 extern struct buffer_head *ext4_read_block_bitmap_nowait(struct super_block *sb,
 						ext4_group_t block_group,
@@ -2692,10 +2694,12 @@ extern ext4_group_t ext4_mb_prefetch(str
 				     unsigned int nr, int *cnt);
 extern void ext4_mb_prefetch_fini(struct super_block *sb, ext4_group_t group,
 				  unsigned int nr);
-
 extern void ext4_free_blocks(handle_t *handle, struct inode *inode,
 			     struct buffer_head *bh, ext4_fsblk_t block,
 			     unsigned long count, int flags);
+extern void ext4_free_blocks_zero(handle_t *handle, struct inode *inode,
+			     struct buffer_head *bh, ext4_fsblk_t block,
+			     unsigned long count, int flags);
 extern int ext4_mb_alloc_groupinfo(struct super_block *sb,
 				   ext4_group_t ngroups);
 extern int ext4_mb_add_groupinfo(struct super_block *sb,
@@ -2746,6 +2750,7 @@ extern int  ext4_write_inode(struct inod
 extern int  ext4_setattr(struct dentry *, struct iattr *);
 extern int  ext4_getattr(const struct path *, struct kstat *, u32, unsigned int);
 extern void ext4_evict_inode(struct inode *);
+extern void ext4_evict_zero_inode(struct inode *);
 extern void ext4_clear_inode(struct inode *);
 extern int  ext4_file_getattr(const struct path *, struct kstat *, u32, unsigned int);
 extern int  ext4_sync_inode(handle_t *, struct inode *);
@@ -2755,6 +2760,7 @@ extern int ext4_get_inode_loc(struct ino
 extern int ext4_inode_attach_jinode(struct inode *inode);
 extern int ext4_can_truncate(struct inode *inode);
 extern int ext4_truncate(struct inode *);
+extern int ext4_truncate_zero(struct inode *);
 extern int ext4_break_layouts(struct inode *);
 extern int ext4_punch_hole(struct inode *inode, loff_t offset, loff_t length);
 extern void ext4_set_inode_flags(struct inode *, bool init);
@@ -3408,6 +3414,10 @@ extern int ext4_check_blockref(const cha
 struct ext4_ext_path;
 struct ext4_extent;
 
+
+int __init kt_free_block_init(void);
+void __exit kt_free_block_cleanup(void);
+
 /*
  * Maximum number of logical blocks in a file; ext4_extent's ee_block is
  * __le32.
@@ -3419,8 +3429,11 @@ extern int ext4_ext_index_trans_blocks(s
 extern int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 			       struct ext4_map_blocks *map, int flags);
 extern int ext4_ext_truncate(handle_t *, struct inode *);
+extern int ext4_ext_truncate_zero(handle_t *, struct inode *);
 extern int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,
 				 ext4_lblk_t end);
+extern int ext4_ext_remove_space_zero(struct inode *inode, ext4_lblk_t start,
+				 ext4_lblk_t end);
 extern void ext4_ext_init(struct super_block *);
 extern void ext4_ext_release(struct super_block *);
 extern long ext4_fallocate(struct file *file, int mode, loff_t offset,
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/extents.c pm-opzero/fs/ext4/extents.c
--- linux-5.9.1/fs/ext4/extents.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/extents.c	2022-12-12 17:21:19.568572644 +0900
@@ -394,7 +394,7 @@ static int ext4_valid_extent_entries(str
 	return 1;
 }
 
-static int __ext4_ext_check(const char *function, unsigned int line,
+int __ext4_ext_check(const char *function, unsigned int line,
 			    struct inode *inode, struct ext4_extent_header *eh,
 			    int depth, ext4_fsblk_t pblk)
 {
@@ -450,6 +450,7 @@ corrupted:
 			     max, le16_to_cpu(eh->eh_depth), depth);
 	return err;
 }
+EXPORT_SYMBOL(__ext4_ext_check);
 
 #define ext4_ext_check(inode, eh, depth, pblk)			\
 	__ext4_ext_check(__func__, __LINE__, (inode), (eh), (depth), (pblk))
@@ -483,7 +484,7 @@ static void ext4_cache_extents(struct in
 	}
 }
 
-static struct buffer_head *
+struct buffer_head *
 __read_extent_tree_block(const char *function, unsigned int line,
 			 struct inode *inode, ext4_fsblk_t pblk, int depth,
 			 int flags)
@@ -525,7 +526,7 @@ errout:
 	return ERR_PTR(err);
 
 }
-
+EXPORT_SYMBOL(__read_extent_tree_block);
 #define read_extent_tree_block(inode, pblk, depth, flags)		\
 	__read_extent_tree_block(__func__, __LINE__, (inode), (pblk),   \
 				 (depth), (flags))
@@ -2280,8 +2281,55 @@ static int ext4_ext_rm_idx(handle_t *han
 		return err;
 	ext_debug(inode, "index is empty, remove it, free block %llu\n", leaf);
 	trace_ext4_ext_rm_idx(inode, leaf);
-
 	ext4_free_blocks(handle, inode, NULL, leaf, 1,
+		EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);
+	while (--depth >= 0) {
+		if (path->p_idx != EXT_FIRST_INDEX(path->p_hdr))
+			break;
+		path--;
+		err = ext4_ext_get_access(handle, inode, path);
+		if (err)
+			break;
+		path->p_idx->ei_block = (path+1)->p_idx->ei_block;
+		err = ext4_ext_dirty(handle, inode, path);
+		if (err)
+			break;
+	}
+	return err;
+}
+
+static int ext4_ext_rm_idx_zero(handle_t *handle, struct inode *inode,
+			struct ext4_ext_path *path, int depth)
+{
+	int err;
+	ext4_fsblk_t leaf;
+
+	/* free index block */
+	depth--;
+	path = path + depth;
+	leaf = ext4_idx_pblock(path->p_idx);
+	if (unlikely(path->p_hdr->eh_entries == 0)) {
+		EXT4_ERROR_INODE(inode, "path->p_hdr->eh_entries == 0");
+		return -EFSCORRUPTED;
+	}
+	err = ext4_ext_get_access(handle, inode, path);
+	if (err)
+		return err;
+
+	if (path->p_idx != EXT_LAST_INDEX(path->p_hdr)) {
+		int len = EXT_LAST_INDEX(path->p_hdr) - path->p_idx;
+		len *= sizeof(struct ext4_extent_idx);
+		memmove(path->p_idx, path->p_idx + 1, len);
+	}
+
+	le16_add_cpu(&path->p_hdr->eh_entries, -1);
+	err = ext4_ext_dirty(handle, inode, path);
+	if (err)
+		return err;
+	ext_debug(inode, "index is empty, remove it, free block %llu\n", leaf);
+	trace_ext4_ext_rm_idx(inode, leaf);
+
+	ext4_free_blocks_zero(handle, inode, NULL, leaf, 1,
 			 EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);
 
 	while (--depth >= 0) {
@@ -2299,6 +2347,7 @@ static int ext4_ext_rm_idx(handle_t *han
 	return err;
 }
 
+
 /*
  * ext4_ext_calc_credits_for_single_extent:
  * This routine returns max. credits that needed to insert an extent
@@ -2451,6 +2500,127 @@ static int ext4_remove_blocks(handle_t *
 			if (ext4_is_pending(inode, partial->lblk))
 				flags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;
 			ext4_free_blocks(handle, inode, NULL,
+				EXT4_C2B(sbi, partial->pclu),
+				sbi->s_cluster_ratio, flags);
+			
+			if (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)
+				ext4_rereserve_cluster(inode, partial->lblk);
+		}
+		partial->state = initial;
+	}
+
+	num = le32_to_cpu(ex->ee_block) + ee_len - from;
+	pblk = ext4_ext_pblock(ex) + ee_len - num;
+
+	/*
+	 * We free the partial cluster at the end of the extent (if any),
+	 * unless the cluster is used by another extent (partial_cluster
+	 * state is nofree).  If a partial cluster exists here, it must be
+	 * shared with the last block in the extent.
+	 */
+	flags = get_default_free_blocks_flags(inode);
+
+	/* partial, left end cluster aligned, right end unaligned */
+	if ((EXT4_LBLK_COFF(sbi, to) != sbi->s_cluster_ratio - 1) &&
+	    (EXT4_LBLK_CMASK(sbi, to) >= from) &&
+	    (partial->state != nofree)) {
+		if (ext4_is_pending(inode, to))
+			flags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;
+		ext4_free_blocks(handle, inode, NULL,
+			EXT4_PBLK_CMASK(sbi, last_pblk),
+			sbi->s_cluster_ratio, flags);
+		if (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)
+			ext4_rereserve_cluster(inode, to);
+		partial->state = initial;
+		flags = get_default_free_blocks_flags(inode);
+	}
+
+	flags |= EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER;
+
+	/*
+	 * For bigalloc file systems, we never free a partial cluster
+	 * at the beginning of the extent.  Instead, we check to see if we
+	 * need to free it on a subsequent call to ext4_remove_blocks,
+	 * or at the end of ext4_ext_rm_leaf or ext4_ext_remove_space.
+	 */
+	flags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;
+	ext4_free_blocks(handle, inode, NULL, pblk, num, flags);
+
+	/* reset the partial cluster if we've freed past it */
+	if (partial->state != initial && partial->pclu != EXT4_B2C(sbi, pblk))
+		partial->state = initial;
+
+	/*
+	 * If we've freed the entire extent but the beginning is not left
+	 * cluster aligned and is not marked as ineligible for freeing we
+	 * record the partial cluster at the beginning of the extent.  It
+	 * wasn't freed by the preceding ext4_free_blocks() call, and we
+	 * need to look farther to the left to determine if it's to be freed
+	 * (not shared with another extent). Else, reset the partial
+	 * cluster - we're either  done freeing or the beginning of the
+	 * extent is left cluster aligned.
+	 */
+	if (EXT4_LBLK_COFF(sbi, from) && num == ee_len) {
+		if (partial->state == initial) {
+			partial->pclu = EXT4_B2C(sbi, pblk);
+			partial->lblk = from;
+			partial->state = tofree;
+		}
+	} else {
+		partial->state = initial;
+	}
+
+	return 0;
+}
+
+static int ext4_remove_blocks_zero(handle_t *handle, struct inode *inode,
+			      struct ext4_extent *ex,
+			      struct partial_cluster *partial,
+			      ext4_lblk_t from, ext4_lblk_t to)
+{
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	unsigned short ee_len = ext4_ext_get_actual_len(ex);
+	ext4_fsblk_t last_pblk, pblk;
+	ext4_lblk_t num;
+	int flags;
+
+	/* only extent tail removal is allowed */
+	if (from < le32_to_cpu(ex->ee_block) ||
+	    to != le32_to_cpu(ex->ee_block) + ee_len - 1) {
+		ext4_error(sbi->s_sb,
+			   "strange request: removal(2) %u-%u from %u:%u",
+			   from, to, le32_to_cpu(ex->ee_block), ee_len);
+		return 0;
+	}
+
+#ifdef EXTENTS_STATS
+	spin_lock(&sbi->s_ext_stats_lock);
+	sbi->s_ext_blocks += ee_len;
+	sbi->s_ext_extents++;
+	if (ee_len < sbi->s_ext_min)
+		sbi->s_ext_min = ee_len;
+	if (ee_len > sbi->s_ext_max)
+		sbi->s_ext_max = ee_len;
+	if (ext_depth(inode) > sbi->s_depth_max)
+		sbi->s_depth_max = ext_depth(inode);
+	spin_unlock(&sbi->s_ext_stats_lock);
+#endif
+
+	trace_ext4_remove_blocks(inode, ex, from, to, partial);
+
+	/*
+	 * if we have a partial cluster, and it's different from the
+	 * cluster of the last block in the extent, we free it
+	 */
+	last_pblk = ext4_ext_pblock(ex) + ee_len - 1;
+
+	if (partial->state != initial &&
+	    partial->pclu != EXT4_B2C(sbi, last_pblk)) {
+		if (partial->state == tofree) {
+			flags = get_default_free_blocks_flags(inode);
+			if (ext4_is_pending(inode, partial->lblk))
+				flags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;
+			ext4_free_blocks_zero(handle, inode, NULL,
 					 EXT4_C2B(sbi, partial->pclu),
 					 sbi->s_cluster_ratio, flags);
 			if (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)
@@ -2476,7 +2646,7 @@ static int ext4_remove_blocks(handle_t *
 	    (partial->state != nofree)) {
 		if (ext4_is_pending(inode, to))
 			flags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;
-		ext4_free_blocks(handle, inode, NULL,
+		ext4_free_blocks_zero(handle, inode, NULL,
 				 EXT4_PBLK_CMASK(sbi, last_pblk),
 				 sbi->s_cluster_ratio, flags);
 		if (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)
@@ -2494,7 +2664,7 @@ static int ext4_remove_blocks(handle_t *
 	 * or at the end of ext4_ext_rm_leaf or ext4_ext_remove_space.
 	 */
 	flags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;
-	ext4_free_blocks(handle, inode, NULL, pblk, num, flags);
+	ext4_free_blocks_zero(handle, inode, NULL, pblk, num, flags);
 
 	/* reset the partial cluster if we've freed past it */
 	if (partial->state != initial && partial->pclu != EXT4_B2C(sbi, pblk))
@@ -2523,6 +2693,7 @@ static int ext4_remove_blocks(handle_t *
 	return 0;
 }
 
+
 /*
  * ext4_ext_rm_leaf() Removes the extents associated with the
  * blocks appearing between "start" and "end".  Both "start"
@@ -2725,6 +2896,210 @@ ext4_ext_rm_leaf(handle_t *handle, struc
 			if (ext4_is_pending(inode, partial->lblk))
 				flags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;
 			ext4_free_blocks(handle, inode, NULL,
+				EXT4_C2B(sbi, partial->pclu),
+				sbi->s_cluster_ratio, flags);
+			if (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)
+				ext4_rereserve_cluster(inode, partial->lblk);
+		}
+		partial->state = initial;
+	}
+
+	/* if this leaf is free, then we should
+	 * remove it from index block above */
+	if (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)
+		err = ext4_ext_rm_idx(handle, inode, path, depth);
+
+out:
+	return err;
+}
+
+static int
+ext4_ext_rm_leaf_zero(handle_t *handle, struct inode *inode,
+		 struct ext4_ext_path *path,
+		 struct partial_cluster *partial,
+		 ext4_lblk_t start, ext4_lblk_t end)
+{
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	int err = 0, correct_index = 0;
+	int depth = ext_depth(inode), credits, revoke_credits;
+	struct ext4_extent_header *eh;
+	ext4_lblk_t a, b;
+	unsigned num;
+	ext4_lblk_t ex_ee_block;
+	unsigned short ex_ee_len;
+	unsigned unwritten = 0;
+	struct ext4_extent *ex;
+	ext4_fsblk_t pblk;
+
+	/* the header must be checked already in ext4_ext_remove_space() */
+	ext_debug(inode, "truncate since %u in leaf to %u\n", start, end);
+	if (!path[depth].p_hdr)
+		path[depth].p_hdr = ext_block_hdr(path[depth].p_bh);
+	eh = path[depth].p_hdr;
+	if (unlikely(path[depth].p_hdr == NULL)) {
+		EXT4_ERROR_INODE(inode, "path[%d].p_hdr == NULL", depth);
+		return -EFSCORRUPTED;
+	}
+	/* find where to start removing */
+	ex = path[depth].p_ext;
+	if (!ex)
+		ex = EXT_LAST_EXTENT(eh);
+
+	ex_ee_block = le32_to_cpu(ex->ee_block);
+	ex_ee_len = ext4_ext_get_actual_len(ex);
+
+	trace_ext4_ext_rm_leaf(inode, start, ex, partial);
+
+	while (ex >= EXT_FIRST_EXTENT(eh) &&
+			ex_ee_block + ex_ee_len > start) {
+
+		if (ext4_ext_is_unwritten(ex))
+			unwritten = 1;
+		else
+			unwritten = 0;
+
+		ext_debug(inode, "remove ext %u:[%d]%d\n", ex_ee_block,
+			  unwritten, ex_ee_len);
+		path[depth].p_ext = ex;
+
+		a = ex_ee_block > start ? ex_ee_block : start;
+		b = ex_ee_block+ex_ee_len - 1 < end ?
+			ex_ee_block+ex_ee_len - 1 : end;
+
+		ext_debug(inode, "  border %u:%u\n", a, b);
+
+		/* If this extent is beyond the end of the hole, skip it */
+		if (end < ex_ee_block) {
+			/*
+			 * We're going to skip this extent and move to another,
+			 * so note that its first cluster is in use to avoid
+			 * freeing it when removing blocks.  Eventually, the
+			 * right edge of the truncated/punched region will
+			 * be just to the left.
+			 */
+			if (sbi->s_cluster_ratio > 1) {
+				pblk = ext4_ext_pblock(ex);
+				partial->pclu = EXT4_B2C(sbi, pblk);
+				partial->state = nofree;
+			}
+			ex--;
+			ex_ee_block = le32_to_cpu(ex->ee_block);
+			ex_ee_len = ext4_ext_get_actual_len(ex);
+			continue;
+		} else if (b != ex_ee_block + ex_ee_len - 1) {
+			EXT4_ERROR_INODE(inode,
+					 "can not handle truncate %u:%u "
+					 "on extent %u:%u",
+					 start, end, ex_ee_block,
+					 ex_ee_block + ex_ee_len - 1);
+			err = -EFSCORRUPTED;
+			goto out;
+		} else if (a != ex_ee_block) {
+			/* remove tail of the extent */
+			num = a - ex_ee_block;
+		} else {
+			/* remove whole extent: excellent! */
+			num = 0;
+		}
+		/*
+		 * 3 for leaf, sb, and inode plus 2 (bmap and group
+		 * descriptor) for each block group; assume two block
+		 * groups plus ex_ee_len/blocks_per_block_group for
+		 * the worst case
+		 */
+		credits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));
+		if (ex == EXT_FIRST_EXTENT(eh)) {
+			correct_index = 1;
+			credits += (ext_depth(inode)) + 1;
+		}
+		credits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);
+		/*
+		 * We may end up freeing some index blocks and data from the
+		 * punched range. Note that partial clusters are accounted for
+		 * by ext4_free_data_revoke_credits().
+		 */
+		revoke_credits =
+			ext4_free_metadata_revoke_credits(inode->i_sb,
+							  ext_depth(inode)) +
+			ext4_free_data_revoke_credits(inode, b - a + 1);
+
+		err = ext4_datasem_ensure_credits(handle, inode, credits,
+						  credits, revoke_credits);
+		if (err) {
+			if (err > 0)
+				err = -EAGAIN;
+			goto out;
+		}
+
+		err = ext4_ext_get_access(handle, inode, path + depth);
+		if (err)
+			goto out;
+
+		err = ext4_remove_blocks_zero(handle, inode, ex, partial, a, b);
+		if (err)
+			goto out;
+
+		if (num == 0)
+			/* this extent is removed; mark slot entirely unused */
+			ext4_ext_store_pblock(ex, 0);
+
+		ex->ee_len = cpu_to_le16(num);
+		/*
+		 * Do not mark unwritten if all the blocks in the
+		 * extent have been removed.
+		 */
+		if (unwritten && num)
+			ext4_ext_mark_unwritten(ex);
+		/*
+		 * If the extent was completely released,
+		 * we need to remove it from the leaf
+		 */
+		if (num == 0) {
+			if (end != EXT_MAX_BLOCKS - 1) {
+				/*
+				 * For hole punching, we need to scoot all the
+				 * extents up when an extent is removed so that
+				 * we dont have blank extents in the middle
+				 */
+				memmove(ex, ex+1, (EXT_LAST_EXTENT(eh) - ex) *
+					sizeof(struct ext4_extent));
+
+				/* Now get rid of the one at the end */
+				memset(EXT_LAST_EXTENT(eh), 0,
+					sizeof(struct ext4_extent));
+			}
+			le16_add_cpu(&eh->eh_entries, -1);
+		}
+
+		err = ext4_ext_dirty(handle, inode, path + depth);
+		if (err)
+			goto out;
+
+		ext_debug(inode, "new extent: %u:%u:%llu\n", ex_ee_block, num,
+				ext4_ext_pblock(ex));
+		ex--;
+		ex_ee_block = le32_to_cpu(ex->ee_block);
+		ex_ee_len = ext4_ext_get_actual_len(ex);
+	}
+
+	if (correct_index && eh->eh_entries)
+		err = ext4_ext_correct_indexes(handle, inode, path);
+
+	/*
+	 * If there's a partial cluster and at least one extent remains in
+	 * the leaf, free the partial cluster if it isn't shared with the
+	 * current extent.  If it is shared with the current extent
+	 * we reset the partial cluster because we've reached the start of the
+	 * truncated/punched region and we're done removing blocks.
+	 */
+	if (partial->state == tofree && ex >= EXT_FIRST_EXTENT(eh)) {
+		pblk = ext4_ext_pblock(ex) + ex_ee_len - 1;
+		if (partial->pclu != EXT4_B2C(sbi, pblk)) {
+			int flags = get_default_free_blocks_flags(inode);
+
+			if (ext4_is_pending(inode, partial->lblk))
+				flags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;
+			ext4_free_blocks_zero(handle, inode, NULL,
 					 EXT4_C2B(sbi, partial->pclu),
 					 sbi->s_cluster_ratio, flags);
 			if (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)
@@ -2742,11 +3117,12 @@ out:
 	return err;
 }
 
+
 /*
  * ext4_ext_more_to_rm:
  * returns 1 if current index has to be freed (even partial)
  */
-static int
+int
 ext4_ext_more_to_rm(struct ext4_ext_path *path)
 {
 	BUG_ON(path->p_idx == NULL);
@@ -2762,6 +3138,7 @@ ext4_ext_more_to_rm(struct ext4_ext_path
 		return 0;
 	return 1;
 }
+EXPORT_SYMBOL(ext4_ext_more_to_rm);
 
 int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,
 			  ext4_lblk_t end)
@@ -2994,6 +3371,269 @@ again:
 		if (ext4_is_pending(inode, partial.lblk))
 			flags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;
 		ext4_free_blocks(handle, inode, NULL,
+			EXT4_C2B(sbi, partial.pclu),
+			sbi->s_cluster_ratio, flags);
+		if (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)
+			ext4_rereserve_cluster(inode, partial.lblk);
+		partial.state = initial;
+	}
+
+	/* TODO: flexible tree reduction should be here */
+	if (path->p_hdr->eh_entries == 0) {
+		/*
+		 * truncate to zero freed all the tree,
+		 * so we need to correct eh_depth
+		 */
+		err = ext4_ext_get_access(handle, inode, path);
+		if (err == 0) {
+			ext_inode_hdr(inode)->eh_depth = 0;
+			ext_inode_hdr(inode)->eh_max =
+				cpu_to_le16(ext4_ext_space_root(inode, 0));
+			err = ext4_ext_dirty(handle, inode, path);
+		}
+	}
+out:
+	ext4_ext_drop_refs(path);
+	kfree(path);
+	path = NULL;
+	if (err == -EAGAIN)
+		goto again;
+	ext4_journal_stop(handle);
+
+	return err;
+}
+
+int ext4_ext_remove_space_zero(struct inode *inode, ext4_lblk_t start,
+			  ext4_lblk_t end)
+{
+	struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+	int depth = ext_depth(inode);
+	struct ext4_ext_path *path = NULL;
+	struct partial_cluster partial;
+	handle_t *handle;
+	int i = 0, err = 0;
+
+	partial.pclu = 0;
+	partial.lblk = 0;
+	partial.state = initial;
+
+	ext_debug(inode, "truncate since %u to %u\n", start, end);
+
+	/* probably first extent we're gonna free will be last in block */
+	handle = ext4_journal_start_with_revoke(inode, EXT4_HT_TRUNCATE,
+			depth + 1,
+			ext4_free_metadata_revoke_credits(inode->i_sb, depth));
+	if (IS_ERR(handle))
+		return PTR_ERR(handle);
+
+again:
+	trace_ext4_ext_remove_space(inode, start, end, depth);
+
+	/*
+	 * Check if we are removing extents inside the extent tree. If that
+	 * is the case, we are going to punch a hole inside the extent tree
+	 * so we have to check whether we need to split the extent covering
+	 * the last block to remove so we can easily remove the part of it
+	 * in ext4_ext_rm_leaf().
+	 */
+	if (end < EXT_MAX_BLOCKS - 1) {
+		struct ext4_extent *ex;
+		ext4_lblk_t ee_block, ex_end, lblk;
+		ext4_fsblk_t pblk;
+
+		/* find extent for or closest extent to this block */
+		path = ext4_find_extent(inode, end, NULL,
+					EXT4_EX_NOCACHE | EXT4_EX_NOFAIL);
+		if (IS_ERR(path)) {
+			ext4_journal_stop(handle);
+			return PTR_ERR(path);
+		}
+		depth = ext_depth(inode);
+		/* Leaf not may not exist only if inode has no blocks at all */
+		ex = path[depth].p_ext;
+		if (!ex) {
+			if (depth) {
+				EXT4_ERROR_INODE(inode,
+						 "path[%d].p_hdr == NULL",
+						 depth);
+				err = -EFSCORRUPTED;
+			}
+			goto out;
+		}
+
+		ee_block = le32_to_cpu(ex->ee_block);
+		ex_end = ee_block + ext4_ext_get_actual_len(ex) - 1;
+
+		/*
+		 * See if the last block is inside the extent, if so split
+		 * the extent at 'end' block so we can easily remove the
+		 * tail of the first part of the split extent in
+		 * ext4_ext_rm_leaf().
+		 */
+		if (end >= ee_block && end < ex_end) {
+
+			/*
+			 * If we're going to split the extent, note that
+			 * the cluster containing the block after 'end' is
+			 * in use to avoid freeing it when removing blocks.
+			 */
+			if (sbi->s_cluster_ratio > 1) {
+				pblk = ext4_ext_pblock(ex) + end - ee_block + 1;
+				partial.pclu = EXT4_B2C(sbi, pblk);
+				partial.state = nofree;
+			}
+
+			/*
+			 * Split the extent in two so that 'end' is the last
+			 * block in the first new extent. Also we should not
+			 * fail removing space due to ENOSPC so try to use
+			 * reserved block if that happens.
+			 */
+			err = ext4_force_split_extent_at(handle, inode, &path,
+							 end + 1, 1);
+			if (err < 0)
+				goto out;
+
+		} else if (sbi->s_cluster_ratio > 1 && end >= ex_end &&
+			   partial.state == initial) {
+			/*
+			 * If we're punching, there's an extent to the right.
+			 * If the partial cluster hasn't been set, set it to
+			 * that extent's first cluster and its state to nofree
+			 * so it won't be freed should it contain blocks to be
+			 * removed. If it's already set (tofree/nofree), we're
+			 * retrying and keep the original partial cluster info
+			 * so a cluster marked tofree as a result of earlier
+			 * extent removal is not lost.
+			 */
+			lblk = ex_end + 1;
+			err = ext4_ext_search_right(inode, path, &lblk, &pblk,
+						    &ex);
+			if (err)
+				goto out;
+			if (pblk) {
+				partial.pclu = EXT4_B2C(sbi, pblk);
+				partial.state = nofree;
+			}
+		}
+	}
+	/*
+	 * We start scanning from right side, freeing all the blocks
+	 * after i_size and walking into the tree depth-wise.
+	 */
+	depth = ext_depth(inode);
+	if (path) {
+		int k = i = depth;
+		while (--k > 0)
+			path[k].p_block =
+				le16_to_cpu(path[k].p_hdr->eh_entries)+1;
+	} else {
+		path = kcalloc(depth + 1, sizeof(struct ext4_ext_path),
+			       GFP_NOFS | __GFP_NOFAIL);
+		if (path == NULL) {
+			ext4_journal_stop(handle);
+			return -ENOMEM;
+		}
+		path[0].p_maxdepth = path[0].p_depth = depth;
+		path[0].p_hdr = ext_inode_hdr(inode);
+		i = 0;
+
+		if (ext4_ext_check(inode, path[0].p_hdr, depth, 0)) {
+			err = -EFSCORRUPTED;
+			goto out;
+		}
+	}
+	err = 0;
+
+	while (i >= 0 && err == 0) {
+		if (i == depth) {
+			/* this is leaf block */
+			err = ext4_ext_rm_leaf_zero(handle, inode, path,
+					       &partial, start, end);
+			/* root level has p_bh == NULL, brelse() eats this */
+			brelse(path[i].p_bh);
+			path[i].p_bh = NULL;
+			i--;
+			continue;
+		}
+
+		/* this is index block */
+		if (!path[i].p_hdr) {
+			ext_debug(inode, "initialize header\n");
+			path[i].p_hdr = ext_block_hdr(path[i].p_bh);
+		}
+
+		if (!path[i].p_idx) {
+			/* this level hasn't been touched yet */
+			path[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);
+			path[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;
+			ext_debug(inode, "init index ptr: hdr 0x%p, num %d\n",
+				  path[i].p_hdr,
+				  le16_to_cpu(path[i].p_hdr->eh_entries));
+		} else {
+			/* we were already here, see at next index */
+			path[i].p_idx--;
+		}
+
+		ext_debug(inode, "level %d - index, first 0x%p, cur 0x%p\n",
+				i, EXT_FIRST_INDEX(path[i].p_hdr),
+				path[i].p_idx);
+		if (ext4_ext_more_to_rm(path + i)) {
+			struct buffer_head *bh;
+			/* go to the next level */
+			ext_debug(inode, "move to level %d (block %llu)\n",
+				  i + 1, ext4_idx_pblock(path[i].p_idx));
+			memset(path + i + 1, 0, sizeof(*path));
+			bh = read_extent_tree_block(inode,
+				ext4_idx_pblock(path[i].p_idx), depth - i - 1,
+				EXT4_EX_NOCACHE);
+			if (IS_ERR(bh)) {
+				/* should we reset i_size? */
+				err = PTR_ERR(bh);
+				break;
+			}
+			/* Yield here to deal with large extent trees.
+			 * Should be a no-op if we did IO above. */
+			cond_resched();
+			if (WARN_ON(i + 1 > depth)) {
+				err = -EFSCORRUPTED;
+				break;
+			}
+			path[i + 1].p_bh = bh;
+
+			/* save actual number of indexes since this
+			 * number is changed at the next iteration */
+			path[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);
+			i++;
+		} else {
+			/* we finished processing this index, go up */
+			if (path[i].p_hdr->eh_entries == 0 && i > 0) {
+				/* index is empty, remove it;
+				 * handle must be already prepared by the
+				 * truncatei_leaf() */
+				err = ext4_ext_rm_idx_zero(handle, inode, path, i);
+			}
+			/* root level has p_bh == NULL, brelse() eats this */
+			brelse(path[i].p_bh);
+			path[i].p_bh = NULL;
+			i--;
+			ext_debug(inode, "return to level %d\n", i);
+		}
+	}
+
+	trace_ext4_ext_remove_space_done(inode, start, end, depth, &partial,
+					 path->p_hdr->eh_entries);
+
+	/*
+	 * if there's a partial cluster and we have removed the first extent
+	 * in the file, then we also free the partial cluster, if any
+	 */
+	if (partial.state == tofree && err == 0) {
+		int flags = get_default_free_blocks_flags(inode);
+
+		if (ext4_is_pending(inode, partial.lblk))
+			flags |= EXT4_FREE_BLOCKS_RERESERVE_CLUSTER;
+		ext4_free_blocks_zero(handle, inode, NULL,
 				 EXT4_C2B(sbi, partial.pclu),
 				 sbi->s_cluster_ratio, flags);
 		if (flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)
@@ -4377,6 +5017,47 @@ retry_remove_space:
 	return err;
 }
 
+int ext4_ext_truncate_zero(handle_t *handle, struct inode *inode)
+{
+	struct super_block *sb = inode->i_sb;
+	ext4_lblk_t last_block;
+	int err = 0;
+
+	/*
+	 * TODO: optimization is possible here.
+	 * Probably we need not scan at all,
+	 * because page truncation is enough.
+	 */
+
+	/* we have to know where to truncate from in crash case */
+	EXT4_I(inode)->i_disksize = inode->i_size;
+	err = ext4_mark_inode_dirty(handle, inode);
+	if (err)
+		return err;
+
+	last_block = (inode->i_size + sb->s_blocksize - 1)
+			>> EXT4_BLOCK_SIZE_BITS(sb);
+retry:
+	err = ext4_es_remove_extent(inode, last_block,
+				    EXT_MAX_BLOCKS - last_block);
+	if (err == -ENOMEM) {
+		cond_resched();
+		congestion_wait(BLK_RW_ASYNC, HZ/50);
+		goto retry;
+	}
+	if (err)
+		return err;
+retry_remove_space:
+	err = ext4_ext_remove_space_zero(inode, last_block, EXT_MAX_BLOCKS - 1);
+	if (err == -ENOMEM) {
+		cond_resched();
+		congestion_wait(BLK_RW_ASYNC, HZ/50);
+		goto retry_remove_space;
+	}
+	return err;
+}
+
+
 static int ext4_alloc_file_blocks(struct file *file, ext4_lblk_t offset,
 				  ext4_lblk_t len, loff_t new_size,
 				  int flags)
@@ -4451,6 +5132,14 @@ retry:
 		if (unlikely(ret2))
 			break;
 	}
+	if(IS_DAX(inode)) {
+		if (ret == -ENOSPC &&
+				ext4_should_retry_alloc_dax(inode->i_sb,
+				&retries, len)) {
+			ret = 0;
+			goto retry;
+		}
+	}
 	if (ret == -ENOSPC &&
 			ext4_should_retry_alloc(inode->i_sb, &retries)) {
 		ret = 0;
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/file.c pm-opzero/fs/ext4/file.c
--- linux-5.9.1/fs/ext4/file.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/file.c	2022-12-12 17:21:19.568572644 +0900
@@ -704,7 +704,11 @@ retry:
 	result = dax_iomap_fault(vmf, pe_size, &pfn, &error, &ext4_iomap_ops);
 	if (write) {
 		ext4_journal_stop(handle);
-
+		if (IS_DAX(inode)) {
+			if((result & VM_FAULT_ERROR) && error == -ENOSPC &&
+				ext4_should_retry_alloc_dax(sb, &retries, 1))
+				goto retry;
+		}
 		if ((result & VM_FAULT_ERROR) && error == -ENOSPC &&
 		    ext4_should_retry_alloc(sb, &retries))
 			goto retry;
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/inline.c pm-opzero/fs/ext4/inline.c
--- linux-5.9.1/fs/ext4/inline.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/inline.c	2022-12-12 17:21:19.568572644 +0900
@@ -618,6 +618,11 @@ retry:
 			ext4_orphan_del(NULL, inode);
 	}
 
+	if(IS_DAX(inode)) {
+		if (ret == -ENOSPC && ext4_should_retry_alloc_dax(inode->i_sb,
+			&retries, 1))
+			goto retry;
+	}
 	if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))
 		goto retry;
 
@@ -900,6 +905,12 @@ retry_journal:
 							    inode,
 							    flags,
 							    fsdata);
+		if(IS_DAX(inode)) {
+			if (ret == -ENOSPC &&
+				ext4_should_retry_alloc_dax(inode->i_sb,
+				&retries, 1))
+				goto retry_journal;
+		}
 		if (ret == -ENOSPC &&
 		    ext4_should_retry_alloc(inode->i_sb, &retries))
 			goto retry_journal;
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/inode.c pm-opzero/fs/ext4/inode.c
--- linux-5.9.1/fs/ext4/inode.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/inode.c	2022-12-12 17:21:19.572572663 +0900
@@ -41,10 +41,13 @@
 #include <linux/iomap.h>
 #include <linux/iversion.h>
 
+#include "ext4.h"
 #include "ext4_jbd2.h"
+#include "ext4_extents.h"
 #include "xattr.h"
 #include "acl.h"
 #include "truncate.h"
+//#include "delay_truncate.h"
 
 #include <trace/events/ext4.h>
 
@@ -330,6 +333,367 @@ no_delete:
 	ext4_clear_inode(inode);	/* We must guarantee clearing of inode... */
 }
 
+//static int zeroout_rm_idx(struct inode *inode, struct ext4_ext_path *path,
+//			  int depth)
+//{
+//	int err;
+//	ext4_fsblk_t leaf;
+//	depth--;
+//	path = path + depth;
+//	leaf = ext4_idx_pblock(path->p_idx);
+//	if (unlikely(path->p_hdr->eh_entries == 0)) {
+//		EXT4_ERROR_INODE(inode, "path->p_hdr->eh_entries == 0");
+//		return -EFSCORRUPTED;
+//	}
+//	le16_add_cpu(&path->p_hdr->eh_entries, -1);
+//	while(--depth >= 0) {
+//		if (path->p_idx != EXT_FIRST_INDEX(path->p_hdr))
+//			break;
+//		path--;
+//	//	path->p_idx->ei_block = (path+1)->p_idx->ei_block;
+//		
+//	}
+//	return err;
+//}
+//
+//static int zeroout_leaf(struct inode *inode, struct ext4_ext_path *path,
+//			ext4_lblk_t start, ext4_lblk_t end)
+//{
+//	int depth = ext_depth(inode);
+//	struct ext4_extent_header *eh;
+//	ext4_lblk_t ex_ee_block;
+//	unsigned short ex_ee_len;
+//	struct ext4_extent *ex;
+//	if (!path[depth].p_hdr)
+//		path[depth].p_hdr = ext_block_hdr(path[depth].p_bh);
+//	eh = path[depth].p_hdr;
+//	if (unlikely(path[depth].p_hdr == NULL)) {
+//		EXT4_ERROR_INODE(inode, "path[%d].p_hdr == NULL", depth);
+//		return -EFSCORRUPTED;
+//	}
+//	/* 
+//	 * find where to start zeroing
+//	 * Here, we need to be careful not to free any of the extents
+//	 */
+//	ex = path[depth].p_ext;
+//	if (!ex)
+//		ex = EXT_LAST_EXTENT(eh);
+//
+//	ex_ee_block = le32_to_cpu(ex->ee_block);
+//	ex_ee_len = ext4_ext_get_actual_len(ex);
+//
+//	while (ex >= EXT_FIRST_EXTENT(eh) &&
+//			ex_ee_block + ex_ee_len > start) {
+//		
+//		ext4_fsblk_t block = ext4_ext_pblock(ex);
+//		struct iomap dax_iomap, srcmap;
+//		loff_t written;
+//		dax_iomap.addr = block << inode->i_blkbits;
+//		dax_iomap.offset = 0;
+//		dax_iomap.bdev = inode->i_sb->s_bdev;
+//		dax_iomap.dax_dev = EXT4_SB(inode->i_sb)->s_daxdev;
+//		srcmap.type = 2;
+//		written = iomap_zero_range_actor(inode, 0,
+//						 inode->i_sb->s_blocksize*ex_ee_len,
+//						 NULL, &dax_iomap, &srcmap);
+////		printk(KERN_INFO "%s: Zeroout block %llu\n", __func__, block);
+//		ex--;
+//		ex_ee_block = le32_to_cpu(ex->ee_block);
+//		ex_ee_len = ext4_ext_get_actual_len(ex);
+//
+//	}
+//	if(path[depth].p_bh != NULL)
+//		zeroout_rm_idx(inode, path, depth);
+//	return 0;
+//}
+
+/*
+ * Zeroing the data blocks in the inode when unlink
+ * Find the physical block numbers by traversing the extent tree and call
+ * iomap_zero_range_actor for each contiguous blocks
+ */
+//int ext4_ext_more_to_rm(struct ext4_ext_path *path);
+//struct buffer_head *
+//__read_extent_tree_block(const char *function, unsigned int line, 
+//			 struct inode *inode, ext4_fsblk_t pblk, int depth, 
+//			 int flags);
+//#define read_extent_tree_block(inode, pblk, depth, flags)		\
+//	__read_extent_tree_block(__func__, __LINE__, (inode), (pblk),	\
+//				 (depth), (flags))
+//int __ext4_ext_check(const char *function, unsigned int line,
+//			    struct inode *inode, struct ext4_extent_header *eh,
+//			    int depth, ext4_fsblk_t pblk);
+//#define ext4_ext_check(inode, eh, depth, pblk)			\
+//	__ext4_ext_check(__func__, __LINE__, (inode), (eh), (depth), (pblk))
+//
+//static int zeroout(struct inode *inode)
+//{
+//	struct super_block *sb = inode->i_sb; 	
+//	ext4_lblk_t start = (sb->s_blocksize - 1) >> EXT4_BLOCK_SIZE_BITS(sb); 
+//	ext4_lblk_t end = EXT_MAX_BLOCKS -1;
+//	//struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);
+//	int depth = ext_depth(inode);
+//	struct ext4_ext_path *path = NULL;
+//	struct partial_cluster partial;
+//	int i, err = 0;
+//	__le16 *entries = kcalloc(depth+1, sizeof(__le16), GFP_NOFS | __GFP_NOFAIL);
+//	partial.pclu = 0;
+//	partial.lblk = 0;
+//	partial.state = initial;
+//	/*
+//	 * Initial extent setup to make a list of blocks in chunck
+//	 */
+//	path = kcalloc(depth + 1, sizeof(struct ext4_ext_path),
+//			GFP_NOFS | __GFP_NOFAIL);
+//	path[0].p_maxdepth = path[0].p_depth = depth;
+//	path[0].p_hdr = ext_inode_hdr(inode);
+//	entries[0] = path[0].p_hdr->eh_entries;
+//	i = 0;
+//
+//	if (ext4_ext_check(inode, path[0].p_hdr, depth, 0)) {
+//		err = -EFSCORRUPTED;
+//		return err;
+//	}
+//	err = 0;
+//	while (i >= 0 && err == 0) {
+//		if (i == depth) {
+//			/* this is leaf block */
+//			err = zeroout_leaf(inode, path,
+//					start, end);
+//			brelse(path[i].p_bh);
+//			path[i].p_bh = NULL;	
+//			i--;
+//			continue;
+//		}
+//
+//		/* this is index block */
+//		if (!path[i].p_hdr) {
+//			path[i].p_hdr = ext_block_hdr(path[i].p_bh);
+//			entries[i] = path[i].p_hdr->eh_entries;
+//		}
+//
+//		if (!path[i].p_idx) {
+//			/* this level hasn't been touched yet */
+//			path[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);
+//			path[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;
+//		} else {
+//			/* we were already here, see at next index */
+//			path[i].p_idx--;
+//		}
+//
+//		if (ext4_ext_more_to_rm(path + i)) {
+//			struct buffer_head *bh;
+//			/* go to the next level */
+//			memset(path + i + 1, 0, sizeof(*path));
+//			bh = read_extent_tree_block(inode,
+//				ext4_idx_pblock(path[i].p_idx), depth - i - 1,
+//				EXT4_EX_NOCACHE);
+//			if (IS_ERR(bh)) {
+//				/* should we reset i_size? */
+//				err = PTR_ERR(bh);
+//				break;
+//			}
+//			if (WARN_ON(i + 1 > depth)) {
+//				err = -EFSCORRUPTED;
+//				break;
+//			}
+//			path[i + 1].p_bh = bh;
+//			path[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);
+//			i++;
+//		} else {
+//			if(path[i].p_hdr->eh_entries == 0 && i > 0) {
+//				zeroout_rm_idx(inode, path, i);
+//			}
+//			brelse(path[i].p_bh);
+//			path[i].p_bh = NULL;
+//			i--;
+//		}
+//	}
+//	for(i = 0; i < depth; i++) {
+//		path[i].p_hdr->eh_entries = entries[i];
+//	}
+//	kfree(path);
+//	kfree(entries);
+//	return err;
+//}
+
+void ext4_evict_zero_inode(struct inode *inode)
+{
+	handle_t *handle;
+	int err;
+	/*
+	 * Credits for final inode cleanup and freeing:
+	 * sb + inode (ext4_orphan_del()), block bitmap, group descriptor
+	 * (xattr block freeing), bitmap, group descriptor (inode freeing)
+	 */
+	int extra_credits = 6;
+	struct ext4_xattr_inode_array *ea_inode_array = NULL;
+
+	trace_ext4_evict_inode(inode);
+
+	if (inode->i_nlink) {
+		/*
+		 * When journalling data dirty buffers are tracked only in the
+		 * journal. So although mm thinks everything is clean and
+		 * ready for reaping the inode might still have some pages to
+		 * write in the running transaction or waiting to be
+		 * checkpointed. Thus calling jbd2_journal_invalidatepage()
+		 * (via truncate_inode_pages()) to discard these buffers can
+		 * cause data loss. Also even if we did not discard these
+		 * buffers, we would have no way to find them after the inode
+		 * is reaped and thus user could see stale data if he tries to
+		 * read them before the transaction is checkpointed. So be
+		 * careful and force everything to disk here... We use
+		 * ei->i_datasync_tid to store the newest transaction
+		 * containing inode's data.
+		 *
+		 * Note that directories do not have this problem because they
+		 * don't use page cache.
+		 */
+		if (inode->i_ino != EXT4_JOURNAL_INO &&
+		    ext4_should_journal_data(inode) &&
+		    (S_ISLNK(inode->i_mode) || S_ISREG(inode->i_mode)) &&
+		    inode->i_data.nrpages) {
+			journal_t *journal = EXT4_SB(inode->i_sb)->s_journal;
+			tid_t commit_tid = EXT4_I(inode)->i_datasync_tid;
+
+			jbd2_complete_transaction(journal, commit_tid);
+			filemap_write_and_wait(&inode->i_data);
+		}
+		truncate_inode_pages_final(&inode->i_data);
+
+		goto no_delete;
+	}
+
+	if (is_bad_inode(inode))
+		goto no_delete;
+	
+	/* 
+	 * Zeroout before getting the journal handle so that the zeroing in the
+	 * data blocks does not hold the journal transaction
+	 */
+//	err = zeroout(inode);
+//	if(err) 
+//		printk(KERN_ERR "%s: Zeroing out has error", __func__);
+
+	dquot_initialize(inode);
+
+	if (ext4_should_order_data(inode))
+		ext4_begin_ordered_truncate(inode, 0);
+	truncate_inode_pages_final(&inode->i_data);
+
+	/*
+	 * For inodes with journalled data, transaction commit could have
+	 * dirtied the inode. Flush worker is ignoring it because of I_FREEING
+	 * flag but we still need to remove the inode from the writeback lists.
+	 */
+	if (!list_empty_careful(&inode->i_io_list)) {
+		WARN_ON_ONCE(!ext4_should_journal_data(inode));
+		inode_io_list_del(inode);
+	}
+
+	/*
+	 * Protect us against freezing - iput() caller didn't have to have any
+	 * protection against it
+	 */
+	sb_start_intwrite(inode->i_sb);
+
+	if (!IS_NOQUOTA(inode))
+		extra_credits += EXT4_MAXQUOTAS_DEL_BLOCKS(inode->i_sb);
+
+
+	/*
+	 * Block bitmap, group descriptor, and inode are accounted in both
+	 * ext4_blocks_for_truncate() and extra_credits. So subtract 3.
+	 */
+	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE,
+			 ext4_blocks_for_truncate(inode) + extra_credits - 3);
+	if (IS_ERR(handle)) {
+		ext4_std_error(inode->i_sb, PTR_ERR(handle));
+		/*
+		 * If we're going to skip the normal cleanup, we still need to
+		 * make sure that the in-core orphan linked list is properly
+		 * cleaned up.
+		 */
+		ext4_orphan_del(NULL, inode);
+		sb_end_intwrite(inode->i_sb);
+		goto no_delete;
+	}
+
+	if (IS_SYNC(inode))
+		ext4_handle_sync(handle);
+
+	/*
+	 * Set inode->i_size to 0 before calling ext4_truncate(). We need
+	 * special handling of symlinks here because i_size is used to
+	 * determine whether ext4_inode_info->i_data contains symlink data or
+	 * block mappings. Setting i_size to 0 will remove its fast symlink
+	 * status. Erase i_data so that it becomes a valid empty block map.
+	 */
+	if (ext4_inode_is_fast_symlink(inode))
+		memset(EXT4_I(inode)->i_data, 0, sizeof(EXT4_I(inode)->i_data));
+	inode->i_size = 0;
+	err = ext4_mark_inode_dirty(handle, inode);
+	if (err) {
+		ext4_warning(inode->i_sb,
+			     "couldn't mark inode dirty (err %d)", err);
+		goto stop_handle;
+	}
+	if (inode->i_blocks) {
+		err = ext4_truncate_zero(inode);
+		if (err) {
+			ext4_error_err(inode->i_sb, -err,
+				       "couldn't truncate inode %lu (err %d)",
+				       inode->i_ino, err);
+			goto stop_handle;
+		}
+	}
+
+	/* Remove xattr references. */
+	err = ext4_xattr_delete_inode(handle, inode, &ea_inode_array,
+				      extra_credits);
+	if (err) {
+		ext4_warning(inode->i_sb, "xattr delete (err %d)", err);
+stop_handle:
+		ext4_journal_stop(handle);
+		ext4_orphan_del(NULL, inode);
+		sb_end_intwrite(inode->i_sb);
+		ext4_xattr_inode_array_free(ea_inode_array);
+		goto no_delete;
+	}
+
+	/*
+	 * Kill off the orphan record which ext4_truncate created.
+	 * AKPM: I think this can be inside the above `if'.
+	 * Note that ext4_orphan_del() has to be able to cope with the
+	 * deletion of a non-existent orphan - this is because we don't
+	 * know if ext4_truncate() actually created an orphan record.
+	 * (Well, we could do this if we need to, but heck - it works)
+	 */
+	ext4_orphan_del(handle, inode);
+	EXT4_I(inode)->i_dtime	= (__u32)ktime_get_real_seconds();
+
+	/*
+	 * One subtle ordering requirement: if anything has gone wrong
+	 * (transaction abort, IO errors, whatever), then we can still
+	 * do these next steps (the fs will already have been marked as
+	 * having errors), but we can't free the inode if the mark_dirty
+	 * fails.
+	 */
+	if (ext4_mark_inode_dirty(handle, inode))
+		/* If that failed, just do the required in-core inode clear. */
+		ext4_clear_inode(inode);
+	else
+		ext4_free_inode(handle, inode);
+	ext4_journal_stop(handle);
+	sb_end_intwrite(inode->i_sb);
+	ext4_xattr_inode_array_free(ea_inode_array);
+	return;
+no_delete:
+	ext4_clear_inode(inode);	/* We must guarantee clearing of inode... */
+}
+
+
 #ifdef CONFIG_QUOTA
 qsize_t *ext4_get_reserved_space(struct inode *inode)
 {
@@ -664,11 +1028,13 @@ found:
 		 * unmap metadata before zeroing as otherwise writeback can
 		 * overwrite zeros with stale data from block device.
 		 */
-		if (flags & EXT4_GET_BLOCKS_ZERO &&
+		if	(!IS_DAX(inode) &&
+			flags & EXT4_GET_BLOCKS_ZERO &&
 		    map->m_flags & EXT4_MAP_MAPPED &&
 		    map->m_flags & EXT4_MAP_NEW) {
+
 			ret = ext4_issue_zeroout(inode, map->m_lblk,
-						 map->m_pblk, map->m_len);
+							 map->m_pblk, map->m_len);
 			if (ret) {
 				retval = ret;
 				goto out_sem;
@@ -1225,7 +1591,12 @@ retry_journal:
 			if (inode->i_nlink)
 				ext4_orphan_del(NULL, inode);
 		}
-
+		if(IS_DAX(inode)) {
+			if (ret == -ENOSPC &&
+					ext4_should_retry_alloc_dax(inode->i_sb,
+					&retries, needed_blocks))
+				goto retry_journal;
+		}
 		if (ret == -ENOSPC &&
 		    ext4_should_retry_alloc(inode->i_sb, &retries))
 			goto retry_journal;
@@ -3409,7 +3780,8 @@ retry:
 		ret = -ENOTBLK;
 
 	ext4_journal_stop(handle);
-	if (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))
+	if (ret == -ENOSPC && ext4_should_retry_alloc_dax(inode->i_sb, &retries,
+		map->m_len))
 		goto retry;
 
 	return ret;
@@ -4246,6 +4618,108 @@ out_trace:
 	return err;
 }
 
+int ext4_truncate_zero(struct inode *inode)
+{
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	unsigned int credits;
+	int err = 0, err2;
+	handle_t *handle;
+	struct address_space *mapping = inode->i_mapping;
+
+	/*
+	 * There is a possibility that we're either freeing the inode
+	 * or it's a completely new inode. In those cases we might not
+	 * have i_mutex locked because it's not necessary.
+	 */
+	if (!(inode->i_state & (I_NEW|I_FREEING)))
+		WARN_ON(!inode_is_locked(inode));
+	trace_ext4_truncate_enter(inode);
+
+	if (!ext4_can_truncate(inode))
+		goto out_trace;
+
+	if (inode->i_size == 0 && !test_opt(inode->i_sb, NO_AUTO_DA_ALLOC))
+		ext4_set_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE);
+
+	if (ext4_has_inline_data(inode)) {
+		int has_inline = 1;
+
+		err = ext4_inline_data_truncate(inode, &has_inline);
+		if (err || has_inline)
+			goto out_trace;
+	}
+
+	/* If we zero-out tail of the page, we have to create jinode for jbd2 */
+	if (inode->i_size & (inode->i_sb->s_blocksize - 1)) {
+		if (ext4_inode_attach_jinode(inode) < 0)
+			goto out_trace;
+	}
+
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		credits = ext4_writepage_trans_blocks(inode);
+	else
+		credits = ext4_blocks_for_truncate(inode);
+
+	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
+	if (IS_ERR(handle)) {
+		err = PTR_ERR(handle);
+		goto out_trace;
+	}
+
+	if (inode->i_size & (inode->i_sb->s_blocksize - 1))
+		ext4_block_truncate_page(handle, mapping, inode->i_size);
+
+	/*
+	 * We add the inode to the orphan list, so that if this
+	 * truncate spans multiple transactions, and we crash, we will
+	 * resume the truncate when the filesystem recovers.  It also
+	 * marks the inode dirty, to catch the new size.
+	 *
+	 * Implication: the file must always be in a sane, consistent
+	 * truncatable state while each transaction commits.
+	 */
+	err = ext4_orphan_add(handle, inode);
+	if (err)
+		goto out_stop;
+
+	down_write(&EXT4_I(inode)->i_data_sem);
+
+	ext4_discard_preallocations(inode, 0);
+
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		err = ext4_ext_truncate_zero(handle, inode);
+	else
+		ext4_ind_truncate(handle, inode);
+
+	up_write(&ei->i_data_sem);
+	if (err)
+		goto out_stop;
+
+	if (IS_SYNC(inode))
+		ext4_handle_sync(handle);
+
+out_stop:
+	/*
+	 * If this was a simple ftruncate() and the file will remain alive,
+	 * then we need to clear up the orphan record which we created above.
+	 * However, if this was a real unlink then we were called by
+	 * ext4_evict_inode(), and we allow that function to clean up the
+	 * orphan info for us.
+	 */
+	if (inode->i_nlink)
+		ext4_orphan_del(handle, inode);
+
+	inode->i_mtime = inode->i_ctime = current_time(inode);
+	err2 = ext4_mark_inode_dirty(handle, inode);
+	if (unlikely(err2 && !err))
+		err = err2;
+	ext4_journal_stop(handle);
+
+out_trace:
+	trace_ext4_truncate_exit(inode);
+	return err;
+}
+
 /*
  * ext4_get_inode_loc returns with an extra refcount against the inode's
  * underlying buffer_head on success. If 'in_mem' is true, we have all
@@ -5407,6 +5881,12 @@ int ext4_setattr(struct dentry *dentry,
 		 */
 		if (attr->ia_size <= oldsize) {
 			rc = ext4_truncate(inode);
+		//	if(IS_DAX(inode)) {
+		//		rc = ext4_delay_truncate(inode);
+		//	}
+		//	else {
+		//		rc = ext4_truncate(inode);
+		//	}
 			if (rc)
 				error = rc;
 		}
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/ioctl.c pm-opzero/fs/ext4/ioctl.c
--- linux-5.9.1/fs/ext4/ioctl.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/ioctl.c	2022-12-12 17:21:19.572572663 +0900
@@ -815,7 +815,6 @@ long ext4_ioctl(struct file *filp, unsig
 	unsigned int flags;
 
 	ext4_debug("cmd = %u, arg = %lu\n", cmd, arg);
-
 	switch (cmd) {
 	case FS_IOC_GETFSMAP:
 		return ext4_ioc_getfsmap(sb, (void __user *)arg);
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/Makefile pm-opzero/fs/ext4/Makefile
--- linux-5.9.1/fs/ext4/Makefile	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/Makefile	2022-12-12 17:21:19.568572644 +0900
@@ -10,7 +10,7 @@ ext4-y	:= balloc.o bitmap.o block_validi
 		indirect.o inline.o inode.o ioctl.o mballoc.o migrate.o \
 		mmp.o move_extent.o namei.o page-io.o readpage.o resize.o \
 		super.o symlink.o sysfs.o xattr.o xattr_hurd.o xattr_trusted.o \
-		xattr_user.o
+		xattr_user.o delay_free_block.o 
 
 ext4-$(CONFIG_EXT4_FS_POSIX_ACL)	+= acl.o
 ext4-$(CONFIG_EXT4_FS_SECURITY)		+= xattr_security.o
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/mballoc.c pm-opzero/fs/ext4/mballoc.c
--- linux-5.9.1/fs/ext4/mballoc.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/mballoc.c	2022-12-12 17:21:19.572572663 +0900
@@ -8,7 +8,6 @@
 /*
  * mballoc.c contains the multiblocks allocation routines
  */
-
 #include "ext4_jbd2.h"
 #include "mballoc.h"
 #include <linux/log2.h>
@@ -18,6 +17,7 @@
 #include <linux/backing-dev.h>
 #include <trace/events/ext4.h>
 
+#include <linux/iomap.h>
 /*
  * MUSTDO:
  *   - test ext4_ext_search_left() and ext4_ext_search_right()
@@ -351,6 +351,9 @@ static void ext4_mb_generate_from_freeli
 						ext4_group_t group);
 static void ext4_mb_new_preallocation(struct ext4_allocation_context *ac);
 
+void ext4_delay_free_block(struct inode * inode, ext4_fsblk_t block, 
+	   		   unsigned long count, int flag);
+
 /*
  * The algorithm using this percpu seq counter goes below:
  * 1. We sample the percpu discard_pa_seq counter before trying for block
@@ -1156,7 +1159,7 @@ err:
  * block group lock of all groups for this page; do not hold the BG lock when
  * calling this routine!
  */
-static noinline_for_stack int
+noinline_for_stack int
 ext4_mb_load_buddy_gfp(struct super_block *sb, ext4_group_t group,
 		       struct ext4_buddy *e4b, gfp_t gfp)
 {
@@ -1292,6 +1295,7 @@ err:
 	e4b->bd_bitmap = NULL;
 	return ret;
 }
+EXPORT_SYMBOL(ext4_mb_load_buddy_gfp);
 
 static int ext4_mb_load_buddy(struct super_block *sb, ext4_group_t group,
 			      struct ext4_buddy *e4b)
@@ -1299,14 +1303,14 @@ static int ext4_mb_load_buddy(struct sup
 	return ext4_mb_load_buddy_gfp(sb, group, e4b, GFP_NOFS);
 }
 
-static void ext4_mb_unload_buddy(struct ext4_buddy *e4b)
+void ext4_mb_unload_buddy(struct ext4_buddy *e4b)
 {
 	if (e4b->bd_bitmap_page)
 		put_page(e4b->bd_bitmap_page);
 	if (e4b->bd_buddy_page)
 		put_page(e4b->bd_buddy_page);
 }
-
+EXPORT_SYMBOL(ext4_mb_unload_buddy);
 
 static int mb_find_order_for_block(struct ext4_buddy *e4b, int block)
 {
@@ -1331,7 +1335,7 @@ static int mb_find_order_for_block(struc
 	return 0;
 }
 
-static void mb_clear_bits(void *bm, int cur, int len)
+void mb_clear_bits(void *bm, int cur, int len)
 {
 	__u32 *addr;
 
@@ -1348,7 +1352,7 @@ static void mb_clear_bits(void *bm, int
 		cur++;
 	}
 }
-
+EXPORT_SYMBOL(mb_clear_bits);
 /* clear bits in given range
  * will return first found zero bit if any, -1 otherwise
  */
@@ -1468,7 +1472,7 @@ static void mb_buddy_mark_free(struct ex
 	}
 }
 
-static void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,
+void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,
 			   int first, int count)
 {
 	int left_is_free = 0;
@@ -1548,6 +1552,7 @@ done:
 	mb_set_largest_free_order(sb, e4b->bd_info);
 	mb_check_buddy(e4b);
 }
+EXPORT_SYMBOL(mb_free_blocks);
 
 static int mb_find_extent(struct ext4_buddy *e4b, int block,
 				int needed, struct ext4_free_extent *ex)
@@ -3019,7 +3024,7 @@ int ext4_mb_release(struct super_block *
 	return 0;
 }
 
-static inline int ext4_issue_discard(struct super_block *sb,
+inline int ext4_issue_discard(struct super_block *sb,
 		ext4_group_t block_group, ext4_grpblk_t cluster, int count,
 		struct bio **biop)
 {
@@ -3038,6 +3043,7 @@ static inline int ext4_issue_discard(str
 	} else
 		return sb_issue_discard(sb, discard_block, count, GFP_NOFS, 0);
 }
+EXPORT_SYMBOL(ext4_issue_discard);
 
 static void ext4_free_data_in_buddy(struct super_block *sb,
 				    struct ext4_free_data *entry)
@@ -5007,7 +5013,7 @@ static void ext4_try_merge_freed_extent(
 	kmem_cache_free(ext4_free_data_cachep, entry);
 }
 
-static noinline_for_stack int
+noinline_for_stack int
 ext4_mb_free_metadata(handle_t *handle, struct ext4_buddy *e4b,
 		      struct ext4_free_data *new_entry)
 {
@@ -5077,7 +5083,7 @@ ext4_mb_free_metadata(handle_t *handle,
 	spin_unlock(&sbi->s_md_lock);
 	return 0;
 }
-
+EXPORT_SYMBOL(ext4_mb_free_metadata);
 /**
  * ext4_free_blocks() -- Free given blocks and update quota
  * @handle:		handle for this transaction
@@ -5173,6 +5179,282 @@ void ext4_free_blocks(handle_t *handle,
 			ext4_forget(handle, is_metadata, inode, bh, block + i);
 		}
 	}
+	if (IS_DAX(inode)) {
+		ext4_delay_free_block(inode, block, count, flags);
+	} else {
+do_more:
+	overflow = 0;
+	ext4_get_group_no_and_offset(sb, block, &block_group, &bit);
+
+	if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(
+			ext4_get_group_info(sb, block_group))))
+		return;
+
+	/*
+	 * Check to see if we are freeing blocks across a group
+	 * boundary.
+	 */
+	if (EXT4_C2B(sbi, bit) + count > EXT4_BLOCKS_PER_GROUP(sb)) {
+		overflow = EXT4_C2B(sbi, bit) + count -
+			EXT4_BLOCKS_PER_GROUP(sb);
+		count -= overflow;
+	}
+	count_clusters = EXT4_NUM_B2C(sbi, count);
+	bitmap_bh = ext4_read_block_bitmap(sb, block_group);
+	if (IS_ERR(bitmap_bh)) {
+		err = PTR_ERR(bitmap_bh);
+		bitmap_bh = NULL;
+		goto error_return;
+	}
+	gdp = ext4_get_group_desc(sb, block_group, &gd_bh);
+	if (!gdp) {
+		err = -EIO;
+		goto error_return;
+	}
+
+	if (in_range(ext4_block_bitmap(sb, gdp), block, count) ||
+	    in_range(ext4_inode_bitmap(sb, gdp), block, count) ||
+	    in_range(block, ext4_inode_table(sb, gdp),
+		     sbi->s_itb_per_group) ||
+	    in_range(block + count - 1, ext4_inode_table(sb, gdp),
+		     sbi->s_itb_per_group)) {
+
+		ext4_error(sb, "Freeing blocks in system zone - "
+			   "Block = %llu, count = %lu", block, count);
+		/* err = 0. ext4_std_error should be a no op */
+		goto error_return;
+	}
+
+	BUFFER_TRACE(bitmap_bh, "getting write access");
+	err = ext4_journal_get_write_access(handle, bitmap_bh);
+	if (err)
+		goto error_return;
+
+	/*
+	 * We are about to modify some metadata.  Call the journal APIs
+	 * to unshare ->b_data if a currently-committing transaction is
+	 * using it
+	 */
+	BUFFER_TRACE(gd_bh, "get_write_access");
+	err = ext4_journal_get_write_access(handle, gd_bh);
+	if (err)
+		goto error_return;
+#ifdef AGGRESSIVE_CHECK
+	{
+		int i;
+		for (i = 0; i < count_clusters; i++)
+			BUG_ON(!mb_test_bit(bit + i, bitmap_bh->b_data));
+	}
+#endif
+	trace_ext4_mballoc_free(sb, inode, block_group, bit, count_clusters);
+
+	/* __GFP_NOFAIL: retry infinitely, ignore TIF_MEMDIE and memcg limit. */
+	err = ext4_mb_load_buddy_gfp(sb, block_group, &e4b,
+				     GFP_NOFS|__GFP_NOFAIL);
+	if (err)
+		goto error_return;
+
+	/* Modified for zeroout data blocks while trucate for dax
+	 * */
+//	if(IS_DAX(inode))
+//	{
+//		/* use iomap_zero_range need to find from and length */
+//		struct iomap dax_iomap, srcmap;
+//		loff_t written;
+//		dax_iomap.addr = block << inode->i_blkbits;
+//		dax_iomap.offset = 0;
+//		dax_iomap.bdev = inode -> i_sb -> s_bdev;
+//		dax_iomap.dax_dev = EXT4_SB(inode -> i_sb)->s_daxdev;
+//		srcmap.type = 2;
+//
+//		written = iomap_zero_range_actor(inode, 0, inode->i_sb->s_blocksize*count, 
+//			NULL, &dax_iomap, &srcmap);
+//	}
+
+	/*
+	 * We need to make sure we don't reuse the freed block until after the
+	 * transaction is committed. We make an exception if the inode is to be
+	 * written in writeback mode since writeback mode has weak data
+	 * consistency guarantees.
+	 */
+	if (ext4_handle_valid(handle) &&
+	    ((flags & EXT4_FREE_BLOCKS_METADATA) ||
+	     !ext4_should_writeback_data(inode))) {
+		struct ext4_free_data *new_entry;
+		/*
+		 * We use __GFP_NOFAIL because ext4_free_blocks() is not allowed
+		 * to fail.
+		 */
+		new_entry = kmem_cache_alloc(ext4_free_data_cachep,
+				GFP_NOFS|__GFP_NOFAIL);
+		new_entry->efd_start_cluster = bit;
+		new_entry->efd_group = block_group;
+		new_entry->efd_count = count_clusters;
+		new_entry->efd_tid = handle->h_transaction->t_tid;
+
+		ext4_lock_group(sb, block_group);
+		mb_clear_bits(bitmap_bh->b_data, bit, count_clusters);
+		ext4_mb_free_metadata(handle, &e4b, new_entry);
+	} else {
+		/* need to update group_info->bb_free and bitmap
+		 * with group lock held. generate_buddy look at
+		 * them with group lock_held
+		 */
+		if (test_opt(sb, DISCARD)) {
+			err = ext4_issue_discard(sb, block_group, bit, count,
+						 NULL);
+			if (err && err != -EOPNOTSUPP)
+				ext4_msg(sb, KERN_WARNING, "discard request in"
+					 " group:%d block:%d count:%lu failed"
+					 " with %d", block_group, bit, count,
+					 err);
+		} else
+			EXT4_MB_GRP_CLEAR_TRIMMED(e4b.bd_info);
+
+		ext4_lock_group(sb, block_group);
+		mb_clear_bits(bitmap_bh->b_data, bit, count_clusters);
+		mb_free_blocks(inode, &e4b, bit, count_clusters);
+	}
+
+	ret = ext4_free_group_clusters(sb, gdp) + count_clusters;
+	ext4_free_group_clusters_set(sb, gdp, ret);
+	ext4_block_bitmap_csum_set(sb, block_group, gdp, bitmap_bh);
+	ext4_group_desc_csum_set(sb, block_group, gdp);
+	ext4_unlock_group(sb, block_group);
+
+	if (sbi->s_log_groups_per_flex) {
+		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
+		atomic64_add(count_clusters,
+			     &sbi_array_rcu_deref(sbi, s_flex_groups,
+						  flex_group)->free_clusters);
+	}
+
+	/*
+	 * on a bigalloc file system, defer the s_freeclusters_counter
+	 * update to the caller (ext4_remove_space and friends) so they
+	 * can determine if a cluster freed here should be rereserved
+	 */
+	if (!(flags & EXT4_FREE_BLOCKS_RERESERVE_CLUSTER)) {
+		if (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE))
+			dquot_free_block(inode, EXT4_C2B(sbi, count_clusters));
+		percpu_counter_add(&sbi->s_freeclusters_counter,
+				   count_clusters);
+	}
+
+	ext4_mb_unload_buddy(&e4b);
+
+	/* We dirtied the bitmap block */
+	BUFFER_TRACE(bitmap_bh, "dirtied bitmap block");
+	err = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);
+
+	/* And the group descriptor block */
+	BUFFER_TRACE(gd_bh, "dirtied group descriptor block");
+	ret = ext4_handle_dirty_metadata(handle, NULL, gd_bh);
+	if (!err)
+		err = ret;
+
+	if (overflow && !err) {
+		block += count;
+		count = overflow;
+		put_bh(bitmap_bh);
+		goto do_more;
+	}
+	}
+error_return:
+	brelse(bitmap_bh);
+	ext4_std_error(sb, err);
+	return;
+}
+
+void ext4_free_blocks_zero(handle_t *handle, struct inode *inode,
+		      struct buffer_head *bh, ext4_fsblk_t block,
+		      unsigned long count, int flags)
+{
+	struct buffer_head *bitmap_bh = NULL;
+	struct super_block *sb = inode->i_sb;
+	struct ext4_group_desc *gdp;
+	unsigned int overflow;
+	ext4_grpblk_t bit;
+	struct buffer_head *gd_bh;
+	ext4_group_t block_group;
+	struct ext4_sb_info *sbi;
+	struct ext4_buddy e4b;
+	unsigned int count_clusters;
+	int err = 0;
+	int ret;
+
+	might_sleep();
+	if (bh) {
+		if (block)
+			BUG_ON(block != bh->b_blocknr);
+		else
+			block = bh->b_blocknr;
+	}
+
+	sbi = EXT4_SB(sb);
+	if (!(flags & EXT4_FREE_BLOCKS_VALIDATED) &&
+	    !ext4_inode_block_valid(inode, block, count)) {
+		ext4_error(sb, "Freeing blocks not in datazone - "
+			   "block = %llu, count = %lu", block, count);
+		goto error_return;
+	}
+
+	ext4_debug("freeing block %llu\n", block);
+	trace_ext4_free_blocks(inode, block, count, flags);
+
+	if (bh && (flags & EXT4_FREE_BLOCKS_FORGET)) {
+		BUG_ON(count > 1);
+
+		ext4_forget(handle, flags & EXT4_FREE_BLOCKS_METADATA,
+			    inode, bh, block);
+	}
+
+	/*
+	 * If the extent to be freed does not begin on a cluster
+	 * boundary, we need to deal with partial clusters at the
+	 * beginning and end of the extent.  Normally we will free
+	 * blocks at the beginning or the end unless we are explicitly
+	 * requested to avoid doing so.
+	 */
+	overflow = EXT4_PBLK_COFF(sbi, block);
+	if (overflow) {
+		if (flags & EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER) {
+			overflow = sbi->s_cluster_ratio - overflow;
+			block += overflow;
+			if (count > overflow)
+				count -= overflow;
+			else
+				return;
+		} else {
+			block -= overflow;
+			count += overflow;
+		}
+	}
+	overflow = EXT4_LBLK_COFF(sbi, count);
+	if (overflow) {
+		if (flags & EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER) {
+			if (count > overflow)
+				count -= overflow;
+			else
+				return;
+		} else
+			count += sbi->s_cluster_ratio - overflow;
+	}
+
+	if (!bh && (flags & EXT4_FREE_BLOCKS_FORGET)) {
+		int i;
+		int is_metadata = flags & EXT4_FREE_BLOCKS_METADATA;
+
+		for (i = 0; i < count; i++) {
+			cond_resched();
+			if (is_metadata)
+				bh = sb_find_get_block(inode->i_sb, block + i);
+			ext4_forget(handle, is_metadata, inode, bh, block + i);
+		}
+	}
+	if (IS_DAX(inode)) {
+		ext4_delay_free_block(inode, block, count, flags);
+	} else {
 
 do_more:
 	overflow = 0;
@@ -5217,6 +5499,25 @@ do_more:
 		goto error_return;
 	}
 
+	/* 
+	 * Modified for zeroout data blocks while trucate for dax
+	 * Since zeroing is done on the data blocks, there is no need for
+	 * journaling. Crash consistency will be still held through fsck.
+	 */
+//	if (IS_DAX(inode)) {
+//		/* use iomap_zero_range need to find from and length */
+//		struct iomap dax_iomap, srcmap;
+//		loff_t written;
+//		dax_iomap.addr = block << inode->i_blkbits;
+//		dax_iomap.offset = 0;
+//		dax_iomap.bdev = inode -> i_sb -> s_bdev;
+//		dax_iomap.dax_dev = EXT4_SB(inode -> i_sb)->s_daxdev;
+//		srcmap.type = 2;
+//
+//		written = iomap_zero_range_actor(inode, 0, inode->i_sb->s_blocksize*count, 
+//			NULL, &dax_iomap, &srcmap);
+//	}
+	
 	BUFFER_TRACE(bitmap_bh, "getting write access");
 	err = ext4_journal_get_write_access(handle, bitmap_bh);
 	if (err)
@@ -5334,6 +5635,7 @@ do_more:
 		put_bh(bitmap_bh);
 		goto do_more;
 	}
+	}
 error_return:
 	brelse(bitmap_bh);
 	ext4_std_error(sb, err);
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/namei.c pm-opzero/fs/ext4/namei.c
--- linux-5.9.1/fs/ext4/namei.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/namei.c	2022-12-12 17:21:19.572572663 +0900
@@ -2613,7 +2613,7 @@ static int ext4_create(struct inode *dir
 	handle_t *handle;
 	struct inode *inode;
 	int err, credits, retries = 0;
-
+	int is_dax = 0;
 	err = dquot_initialize(dir);
 	if (err)
 		return err;
@@ -2630,11 +2630,18 @@ retry:
 		inode->i_fop = &ext4_file_operations;
 		ext4_set_aops(inode);
 		err = ext4_add_nondir(handle, dentry, &inode);
+		if(err == -ENOSPC && inode->i_flags & S_DAX)
+			is_dax = 1;
 	}
 	if (handle)
 		ext4_journal_stop(handle);
 	if (!IS_ERR_OR_NULL(inode))
 		iput(inode);
+	if(is_dax) {
+		if(err == -ENOSPC && ext4_should_retry_alloc_dax(dir->i_sb,
+			&retries, 1))
+			goto retry;
+	}
 	if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries))
 		goto retry;
 	return err;
@@ -2667,6 +2674,12 @@ retry:
 		ext4_journal_stop(handle);
 	if (!IS_ERR_OR_NULL(inode))
 		iput(inode);
+	
+	if(IS_DAX(dir)) {
+		if(err == -ENOSPC && ext4_should_retry_alloc_dax(dir->i_sb,
+			&retries, 1))
+			goto retry;
+	}
 	if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries))
 		goto retry;
 	return err;
@@ -2703,6 +2716,12 @@ retry:
 	}
 	if (handle)
 		ext4_journal_stop(handle);
+
+	if(IS_DAX(dir)) {
+		if(err == -ENOSPC && ext4_should_retry_alloc_dax(dir->i_sb,
+			&retries, 1))
+			goto retry;
+	}
 	if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries))
 		goto retry;
 	return err;
@@ -2838,6 +2857,11 @@ out_stop:
 	if (handle)
 		ext4_journal_stop(handle);
 out_retry:
+	if (IS_DAX(dir)) {
+		if (err == -ENOSPC && ext4_should_retry_alloc_dax(dir->i_sb,
+			&retries, 1))
+			goto retry;
+	}
 	if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries))
 		goto retry;
 	return err;
@@ -3451,6 +3475,11 @@ retry:
 		iput(inode);
 	}
 	ext4_journal_stop(handle);
+	if(IS_DAX(dir)) {
+		if (err == -ENOSPC && ext4_should_retry_alloc_dax(dir->i_sb,
+			&retries, 1))
+			goto retry;
+	}
 	if (err == -ENOSPC && ext4_should_retry_alloc(dir->i_sb, &retries))
 		goto retry;
 	return err;
@@ -3660,6 +3689,11 @@ retry:
 	if (IS_ERR(wh)) {
 		if (handle)
 			ext4_journal_stop(handle);
+		if(IS_DAX(wh)) {
+			if (PTR_ERR(wh) == -ENOSPC &&
+					ext4_should_retry_alloc_dax(ent->dir->i_sb, &retries, 1))
+				goto retry;
+		}	
 		if (PTR_ERR(wh) == -ENOSPC &&
 		    ext4_should_retry_alloc(ent->dir->i_sb, &retries))
 			goto retry;
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/super.c pm-opzero/fs/ext4/super.c
--- linux-5.9.1/fs/ext4/super.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/super.c	2022-12-12 17:21:19.572572663 +0900
@@ -1006,7 +1006,6 @@ static void ext4_put_super(struct super_
 	struct flex_groups **flex_groups;
 	int aborted = 0;
 	int i, err;
-
 	ext4_unregister_li_request(sb);
 	ext4_quota_off_umount(sb);
 
@@ -1481,6 +1480,7 @@ static const struct super_operations ext
 	.dirty_inode	= ext4_dirty_inode,
 	.drop_inode	= ext4_drop_inode,
 	.evict_inode	= ext4_evict_inode,
+	.evict_zero_inode	= ext4_evict_zero_inode,
 	.put_super	= ext4_put_super,
 	.sync_fs	= ext4_sync_fs,
 	.freeze_fs	= ext4_freeze,
@@ -6284,13 +6284,23 @@ static ssize_t ext4_quota_write(struct s
 			(unsigned long long)off, (unsigned long long)len);
 		return -EIO;
 	}
+	if(!IS_DAX(inode)) {
+		do {
+			bh = ext4_bread(handle, inode, blk,
+					EXT4_GET_BLOCKS_CREATE |
+					EXT4_GET_BLOCKS_METADATA_NOFAIL);
+		} while (PTR_ERR(bh) == -ENOSPC &&
+				ext4_should_retry_alloc(inode->i_sb, &retries));
+	} else {
+		do {
+			bh = ext4_bread(handle, inode, blk,
+					EXT4_GET_BLOCKS_CREATE |
+					EXT4_GET_BLOCKS_METADATA_NOFAIL);
+		} while (PTR_ERR(bh) == -ENOSPC &&
+				ext4_should_retry_alloc_dax(inode->i_sb,
+				&retries, 1));
 
-	do {
-		bh = ext4_bread(handle, inode, blk,
-				EXT4_GET_BLOCKS_CREATE |
-				EXT4_GET_BLOCKS_METADATA_NOFAIL);
-	} while (PTR_ERR(bh) == -ENOSPC &&
-		 ext4_should_retry_alloc(inode->i_sb, &retries));
+	}
 	if (IS_ERR(bh))
 		return PTR_ERR(bh);
 	if (!bh)
@@ -6437,6 +6447,9 @@ static int __init ext4_init_fs(void)
 	err = init_inodecache();
 	if (err)
 		goto out1;
+
+        kt_free_block_init();
+
 	register_as_ext3();
 	register_as_ext2();
 	err = register_filesystem(&ext4_fs_type);
@@ -6472,6 +6485,9 @@ static void __exit ext4_exit_fs(void)
 	unregister_as_ext2();
 	unregister_as_ext3();
 	unregister_filesystem(&ext4_fs_type);
+
+        kt_free_block_cleanup();
+
 	destroy_inodecache();
 	ext4_exit_mballoc();
 	ext4_exit_sysfs();
diff '--color=auto' -uprN linux-5.9.1/fs/ext4/xattr.c pm-opzero/fs/ext4/xattr.c
--- linux-5.9.1/fs/ext4/xattr.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/ext4/xattr.c	2022-12-12 17:21:19.572572663 +0900
@@ -1342,6 +1342,14 @@ retry:
 				      EXT4_GET_BLOCKS_CREATE);
 		if (ret <= 0) {
 			ext4_mark_inode_dirty(handle, ea_inode);
+			if(IS_DAX(ea_inode)) {
+				if(ret == -ENOSPC &&
+					ext4_should_retry_alloc_dax(ea_inode->i_sb,
+					&retries, max_blocks)) {
+					ret = 0;
+					goto retry;
+				}
+			}
 			if (ret == -ENOSPC &&
 			    ext4_should_retry_alloc(ea_inode->i_sb, &retries)) {
 				ret = 0;
@@ -2490,6 +2498,12 @@ retry:
 		error = ext4_xattr_set_handle(handle, inode, name_index, name,
 					      value, value_len, flags);
 		error2 = ext4_journal_stop(handle);
+		if(IS_DAX(inode)) {
+			if (error == -ENOSPC &&
+					ext4_should_retry_alloc_dax(sb,
+					&retries, value_len))
+				goto retry;
+		}
 		if (error == -ENOSPC &&
 		    ext4_should_retry_alloc(sb, &retries))
 			goto retry;
diff '--color=auto' -uprN linux-5.9.1/fs/inode.c pm-opzero/fs/inode.c
--- linux-5.9.1/fs/inode.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/inode.c	2022-12-12 17:21:19.588572736 +0900
@@ -593,6 +593,47 @@ static void evict(struct inode *inode)
 	destroy_inode(inode);
 }
 
+static void evict_zero(struct inode *inode)
+{
+	const struct super_operations *op = inode->i_sb->s_op;
+
+	BUG_ON(!(inode->i_state & I_FREEING));
+	BUG_ON(!list_empty(&inode->i_lru));
+
+	if (!list_empty(&inode->i_io_list))
+		inode_io_list_del(inode);
+
+	inode_sb_list_del(inode);
+
+	/*
+	 * Wait for flusher thread to be done with the inode so that filesystem
+	 * does not start destroying it while writeback is still running. Since
+	 * the inode has I_FREEING set, flusher thread won't start new work on
+	 * the inode.  We just have to wait for running writeback to finish.
+	 */
+	inode_wait_for_writeback(inode);
+
+	if (op->evict_zero_inode) {
+		op->evict_zero_inode(inode);
+	} else {
+		truncate_inode_pages_final(&inode->i_data);
+		clear_inode(inode);
+	}
+	if (S_ISBLK(inode->i_mode) && inode->i_bdev)
+		bd_forget(inode);
+	if (S_ISCHR(inode->i_mode) && inode->i_cdev)
+		cd_forget(inode);
+
+	remove_inode_hash(inode);
+
+//	spin_lock(&inode->i_lock);
+//	wake_up_bit(&inode->i_state, __I_NEW);
+//	BUG_ON(inode->i_state != (I_FREEING | I_CLEAR));
+//	spin_unlock(&inode->i_lock);
+//
+//	destroy_inode(inode);
+}
+
 /*
  * dispose_list - dispose of the contents of a local list
  * @head: the head of the list to free
@@ -1652,6 +1693,48 @@ static void iput_final(struct inode *ino
 	evict(inode);
 }
 
+static void iput_zero_final(struct inode *inode)
+{
+	struct super_block *sb = inode->i_sb;
+	const struct super_operations *op = inode->i_sb->s_op;
+	unsigned long state;
+	int drop;
+
+	WARN_ON(inode->i_state & I_NEW);
+
+	if (op->drop_inode)
+		drop = op->drop_inode(inode);
+	else
+		drop = generic_drop_inode(inode);
+
+	if (!drop && (sb->s_flags & SB_ACTIVE)) {
+		inode_add_lru(inode);
+		spin_unlock(&inode->i_lock);
+		return;
+	}
+
+	state = inode->i_state;
+	if (!drop) {
+		WRITE_ONCE(inode->i_state, state | I_WILL_FREE);
+		spin_unlock(&inode->i_lock);
+
+		write_inode_now(inode, 1);
+
+		spin_lock(&inode->i_lock);
+		state = inode->i_state;
+		WARN_ON(state & I_NEW);
+		state &= ~I_WILL_FREE;
+	}
+
+	WRITE_ONCE(inode->i_state, state | I_FREEING);
+	if (!list_empty(&inode->i_lru))
+		inode_lru_list_del(inode);
+	spin_unlock(&inode->i_lock);
+
+	evict_zero(inode);
+}
+
+
 /**
  *	iput	- put an inode
  *	@inode: inode to put
@@ -1680,6 +1763,26 @@ retry:
 }
 EXPORT_SYMBOL(iput);
 
+void iput_zero(struct inode *inode)
+{
+	if (!inode)
+		return;
+	BUG_ON(inode->i_state & I_CLEAR);
+retry:
+	if (atomic_dec_and_lock(&inode->i_count, &inode->i_lock)) {
+		if (inode->i_nlink && (inode->i_state & I_DIRTY_TIME)) {
+			atomic_inc(&inode->i_count);
+			spin_unlock(&inode->i_lock);
+			trace_writeback_lazytime_iput(inode);
+			mark_inode_dirty_sync(inode);
+			goto retry;
+		}
+		iput_zero_final(inode);
+	}
+}
+EXPORT_SYMBOL(iput_zero);
+
+
 #ifdef CONFIG_BLOCK
 /**
  *	bmap	- find a block number in a file
diff '--color=auto' -uprN linux-5.9.1/fs/iomap/buffered-io.c pm-opzero/fs/iomap/buffered-io.c
--- linux-5.9.1/fs/iomap/buffered-io.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/iomap/buffered-io.c	2022-12-12 17:21:19.592572754 +0900
@@ -944,7 +944,7 @@ static int iomap_zero(struct inode *inod
 	return iomap_write_end(inode, pos, bytes, bytes, page, iomap, srcmap);
 }
 
-static loff_t
+loff_t
 iomap_zero_range_actor(struct inode *inode, loff_t pos, loff_t count,
 		void *data, struct iomap *iomap, struct iomap *srcmap)
 {
@@ -979,6 +979,7 @@ iomap_zero_range_actor(struct inode *ino
 
 	return written;
 }
+EXPORT_SYMBOL_GPL(iomap_zero_range_actor);
 
 int
 iomap_zero_range(struct inode *inode, loff_t pos, loff_t len, bool *did_zero,
diff '--color=auto' -uprN linux-5.9.1/fs/Makefile pm-opzero/fs/Makefile
--- linux-5.9.1/fs/Makefile	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/Makefile	2022-12-12 17:21:19.532572481 +0900
@@ -13,7 +13,7 @@ obj-y :=	open.o read_write.o file_table.
 		seq_file.o xattr.o libfs.o fs-writeback.o \
 		pnode.o splice.o sync.o utimes.o d_path.o \
 		stack.o fs_struct.o statfs.o fs_pin.o nsfs.o \
-		fs_types.o fs_context.o fs_parser.o fsopen.o init.o
+		fs_types.o fs_context.o fs_parser.o fsopen.o init.o 
 
 ifeq ($(CONFIG_BLOCK),y)
 obj-y +=	buffer.o block_dev.o direct-io.o mpage.o
diff '--color=auto' -uprN linux-5.9.1/fs/namei.c pm-opzero/fs/namei.c
--- linux-5.9.1/fs/namei.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/namei.c	2022-12-12 17:21:19.604572808 +0900
@@ -13,7 +13,7 @@
  * lookup logic.
  */
 /* [Feb-Apr 2000, AV] Rewrite to the new namespace architecture.
- */
+*/
 
 #include <linux/init.h>
 #include <linux/export.h>
@@ -124,7 +124,7 @@
 
 #define EMBEDDED_NAME_MAX	(PATH_MAX - offsetof(struct filename, iname))
 
-struct filename *
+	struct filename *
 getname_flags(const char __user *filename, int flags, int *empty)
 {
 	struct filename *result;
@@ -203,13 +203,13 @@ getname_flags(const char __user *filenam
 	return result;
 }
 
-struct filename *
+	struct filename *
 getname(const char __user * filename)
 {
 	return getname_flags(filename, 0, NULL);
 }
 
-struct filename *
+	struct filename *
 getname_kernel(const char * filename)
 {
 	struct filename *result;
@@ -266,21 +266,21 @@ static int check_acl(struct inode *inode
 
 	if (mask & MAY_NOT_BLOCK) {
 		acl = get_cached_acl_rcu(inode, ACL_TYPE_ACCESS);
-	        if (!acl)
-	                return -EAGAIN;
+		if (!acl)
+			return -EAGAIN;
 		/* no ->get_acl() calls in RCU mode... */
 		if (is_uncached_acl(acl))
 			return -ECHILD;
-	        return posix_acl_permission(inode, acl, mask);
+		return posix_acl_permission(inode, acl, mask);
 	}
 
 	acl = get_acl(inode, ACL_TYPE_ACCESS);
 	if (IS_ERR(acl))
 		return PTR_ERR(acl);
 	if (acl) {
-	        int error = posix_acl_permission(inode, acl, mask);
-	        posix_acl_release(acl);
-	        return error;
+		int error = posix_acl_permission(inode, acl, mask);
+		posix_acl_release(acl);
+		return error;
 	}
 #endif
 
@@ -358,7 +358,7 @@ int generic_permission(struct inode *ino
 		/* DACs are overridable for directories */
 		if (!(mask & MAY_WRITE))
 			if (capable_wrt_inode_uidgid(inode,
-						     CAP_DAC_READ_SEARCH))
+						CAP_DAC_READ_SEARCH))
 				return 0;
 		if (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))
 			return 0;
@@ -550,7 +550,7 @@ static bool nd_alloc_stack(struct nameid
 	struct saved *p;
 
 	p= kmalloc_array(MAXSYMLINKS, sizeof(struct saved),
-			 nd->flags & LOOKUP_RCU ? GFP_ATOMIC : GFP_KERNEL);
+			nd->flags & LOOKUP_RCU ? GFP_ATOMIC : GFP_KERNEL);
 	if (unlikely(!p))
 		return false;
 	memcpy(p, nd->internal, sizeof(nd->internal));
@@ -622,7 +622,7 @@ static bool __legitimize_path(struct pat
 }
 
 static inline bool legitimize_path(struct nameidata *nd,
-			    struct path *path, unsigned seq)
+		struct path *path, unsigned seq)
 {
 	return __legitimize_path(path, seq, nd->m_seq);
 }
@@ -1067,22 +1067,22 @@ int may_linkat(struct path *link)
  * Returns 0 if the open is allowed, -ve on error.
  */
 static int may_create_in_sticky(umode_t dir_mode, kuid_t dir_uid,
-				struct inode * const inode)
+		struct inode * const inode)
 {
 	if ((!sysctl_protected_fifos && S_ISFIFO(inode->i_mode)) ||
-	    (!sysctl_protected_regular && S_ISREG(inode->i_mode)) ||
-	    likely(!(dir_mode & S_ISVTX)) ||
-	    uid_eq(inode->i_uid, dir_uid) ||
-	    uid_eq(current_fsuid(), inode->i_uid))
+			(!sysctl_protected_regular && S_ISREG(inode->i_mode)) ||
+			likely(!(dir_mode & S_ISVTX)) ||
+			uid_eq(inode->i_uid, dir_uid) ||
+			uid_eq(current_fsuid(), inode->i_uid))
 		return 0;
 
 	if (likely(dir_mode & 0002) ||
-	    (dir_mode & 0020 &&
-	     ((sysctl_protected_fifos >= 2 && S_ISFIFO(inode->i_mode)) ||
-	      (sysctl_protected_regular >= 2 && S_ISREG(inode->i_mode))))) {
+			(dir_mode & 0020 &&
+			 ((sysctl_protected_fifos >= 2 && S_ISFIFO(inode->i_mode)) ||
+			  (sysctl_protected_regular >= 2 && S_ISREG(inode->i_mode))))) {
 		const char *operation = S_ISFIFO(inode->i_mode) ?
-					"sticky_create_fifo" :
-					"sticky_create_regular";
+			"sticky_create_fifo" :
+			"sticky_create_regular";
 		audit_log_path_denied(AUDIT_ANOM_CREAT, operation);
 		return -EACCES;
 	}
@@ -1123,14 +1123,14 @@ int follow_up(struct path *path)
 EXPORT_SYMBOL(follow_up);
 
 static bool choose_mountpoint_rcu(struct mount *m, const struct path *root,
-				  struct path *path, unsigned *seqp)
+		struct path *path, unsigned *seqp)
 {
 	while (mnt_has_parent(m)) {
 		struct dentry *mountpoint = m->mnt_mountpoint;
 
 		m = m->mnt_parent;
 		if (unlikely(root->dentry == mountpoint &&
-			     root->mnt == &m->mnt))
+					root->mnt == &m->mnt))
 			break;
 		if (mountpoint != m->mnt.mnt_root) {
 			path->mnt = &m->mnt;
@@ -1143,7 +1143,7 @@ static bool choose_mountpoint_rcu(struct
 }
 
 static bool choose_mountpoint(struct mount *m, const struct path *root,
-			      struct path *path)
+		struct path *path)
 {
 	bool found;
 
@@ -1188,8 +1188,8 @@ static int follow_automount(struct path
 	 * of the daemon to instantiate them before they can be used.
 	 */
 	if (!(lookup_flags & (LOOKUP_PARENT | LOOKUP_DIRECTORY |
-			   LOOKUP_OPEN | LOOKUP_CREATE | LOOKUP_AUTOMOUNT)) &&
-	    dentry->d_inode)
+					LOOKUP_OPEN | LOOKUP_CREATE | LOOKUP_AUTOMOUNT)) &&
+			dentry->d_inode)
 		return -EISDIR;
 
 	if (count && (*count)++ >= MAXSYMLINKS)
@@ -1205,7 +1205,7 @@ static int follow_automount(struct path
  * sufficient for ->d_inode and ->d_flags consistency.
  */
 static int __traverse_mounts(struct path *path, unsigned flags, bool *jumped,
-			     int *count, unsigned lookup_flags)
+		int *count, unsigned lookup_flags)
 {
 	struct vfsmount *mnt = path->mnt;
 	bool need_mntput = false;
@@ -1258,7 +1258,7 @@ static int __traverse_mounts(struct path
 }
 
 static inline int traverse_mounts(struct path *path, bool *jumped,
-				  int *count, unsigned lookup_flags)
+		int *count, unsigned lookup_flags)
 {
 	unsigned flags = smp_load_acquire(&path->dentry->d_flags);
 
@@ -1310,7 +1310,7 @@ EXPORT_SYMBOL(follow_down);
  * we meet a managed dentry that would need blocking.
  */
 static bool __follow_mount_rcu(struct nameidata *nd, struct path *path,
-			       struct inode **inode, unsigned *seqp)
+		struct inode **inode, unsigned *seqp)
 {
 	struct dentry *dentry = path->dentry;
 	unsigned int flags = dentry->d_flags;
@@ -1358,8 +1358,8 @@ static bool __follow_mount_rcu(struct na
 }
 
 static inline int handle_mounts(struct nameidata *nd, struct dentry *dentry,
-			  struct path *path, struct inode **inode,
-			  unsigned int *seqp)
+		struct path *path, struct inode **inode,
+		unsigned int *seqp)
 {
 	bool jumped;
 	int ret;
@@ -1401,8 +1401,8 @@ static inline int handle_mounts(struct n
  * NULL is returned if the dentry does not exist in the cache.
  */
 static struct dentry *lookup_dcache(const struct qstr *name,
-				    struct dentry *dir,
-				    unsigned int flags)
+		struct dentry *dir,
+		unsigned int flags)
 {
 	struct dentry *dentry = d_lookup(dir, name);
 	if (dentry) {
@@ -1451,8 +1451,8 @@ static struct dentry *__lookup_hash(cons
 }
 
 static struct dentry *lookup_fast(struct nameidata *nd,
-				  struct inode **inode,
-			          unsigned *seqp)
+		struct inode **inode,
+		unsigned *seqp)
 {
 	struct dentry *dentry, *parent = nd->path.dentry;
 	int status = 1;
@@ -1515,8 +1515,8 @@ static struct dentry *lookup_fast(struct
 
 /* Fast lookup failed, do it the slow way */
 static struct dentry *__lookup_slow(const struct qstr *name,
-				    struct dentry *dir,
-				    unsigned int flags)
+		struct dentry *dir,
+		unsigned int flags)
 {
 	struct dentry *dentry, *old;
 	struct inode *inode = dir->d_inode;
@@ -1552,8 +1552,8 @@ again:
 }
 
 static struct dentry *lookup_slow(const struct qstr *name,
-				  struct dentry *dir,
-				  unsigned int flags)
+		struct dentry *dir,
+		unsigned int flags)
 {
 	struct inode *inode = dir->d_inode;
 	struct dentry *res;
@@ -1604,7 +1604,7 @@ static int reserve_stack(struct nameidat
 enum {WALK_TRAILING = 1, WALK_MORE = 2, WALK_NOFOLLOW = 4};
 
 static const char *pick_link(struct nameidata *nd, struct path *link,
-		     struct inode *inode, unsigned seq, int flags)
+		struct inode *inode, unsigned seq, int flags)
 {
 	struct saved *last;
 	const char *res;
@@ -1639,7 +1639,7 @@ static const char *pick_link(struct name
 	}
 
 	error = security_inode_follow_link(link->dentry, inode,
-					   nd->flags & LOOKUP_RCU);
+			nd->flags & LOOKUP_RCU);
 	if (unlikely(error))
 		return ERR_PTR(error);
 
@@ -1684,7 +1684,7 @@ all_done: // pure jump
  * for the common case.
  */
 static const char *step_into(struct nameidata *nd, int flags,
-		     struct dentry *dentry, struct inode *inode, unsigned seq)
+		struct dentry *dentry, struct inode *inode, unsigned seq)
 {
 	struct path path;
 	int err = handle_mounts(nd, dentry, &path, &inode, &seq);
@@ -1692,8 +1692,8 @@ static const char *step_into(struct name
 	if (err < 0)
 		return ERR_PTR(err);
 	if (likely(!d_is_symlink(path.dentry)) ||
-	   ((flags & WALK_TRAILING) && !(nd->flags & LOOKUP_FOLLOW)) ||
-	   (flags & WALK_NOFOLLOW)) {
+			((flags & WALK_TRAILING) && !(nd->flags & LOOKUP_FOLLOW)) ||
+			(flags & WALK_NOFOLLOW)) {
 		/* not a symlink or should not follow */
 		if (!(nd->flags & LOOKUP_RCU)) {
 			dput(nd->path.dentry);
@@ -1717,8 +1717,8 @@ static const char *step_into(struct name
 }
 
 static struct dentry *follow_dotdot_rcu(struct nameidata *nd,
-					struct inode **inodep,
-					unsigned *seqp)
+		struct inode **inodep,
+		unsigned *seqp)
 {
 	struct dentry *parent, *old;
 
@@ -1728,7 +1728,7 @@ static struct dentry *follow_dotdot_rcu(
 		struct path path;
 		unsigned seq;
 		if (!choose_mountpoint_rcu(real_mount(nd->path.mnt),
-					   &nd->root, &path, &seq))
+					&nd->root, &path, &seq))
 			goto in_root;
 		if (unlikely(nd->flags & LOOKUP_NO_XDEV))
 			return ERR_PTR(-ECHILD);
@@ -1757,8 +1757,8 @@ in_root:
 }
 
 static struct dentry *follow_dotdot(struct nameidata *nd,
-				 struct inode **inodep,
-				 unsigned *seqp)
+		struct inode **inodep,
+		unsigned *seqp)
 {
 	struct dentry *parent;
 
@@ -1768,7 +1768,7 @@ static struct dentry *follow_dotdot(stru
 		struct path path;
 
 		if (!choose_mountpoint(real_mount(nd->path.mnt),
-				       &nd->root, &path))
+					&nd->root, &path))
 			goto in_root;
 		path_put(&nd->path);
 		nd->path = path;
@@ -1814,10 +1814,10 @@ static const char *handle_dots(struct na
 			return ERR_CAST(parent);
 		if (unlikely(!parent))
 			error = step_into(nd, WALK_NOFOLLOW,
-					 nd->path.dentry, nd->inode, nd->seq);
+					nd->path.dentry, nd->inode, nd->seq);
 		else
 			error = step_into(nd, WALK_NOFOLLOW,
-					 parent, inode, seq);
+					parent, inode, seq);
 		if (unlikely(error))
 			return error;
 
@@ -1923,9 +1923,9 @@ static const char *walk_component(struct
  */
 #define HASH_MIX(x, y, a)	\
 	(	x ^= (a),	\
-	y ^= x,	x = rol64(x,12),\
-	x += y,	y = rol64(y,45),\
-	y *= 9			)
+		y ^= x,	x = rol64(x,12),\
+		x += y,	y = rol64(y,45),\
+		y *= 9			)
 
 /*
  * Fold two longs into one 32-bit hash value.  This must be fast, but
@@ -1953,9 +1953,9 @@ static inline unsigned int fold_hash(uns
  */
 #define HASH_MIX(x, y, a)	\
 	(	x ^= (a),	\
-	y ^= x,	x = rol32(x, 7),\
-	x += y,	y = rol32(y,20),\
-	y *= 9			)
+		y ^= x,	x = rol32(x, 7),\
+		x += y,	y = rol32(y,20),\
+		y *= 9			)
 
 static inline unsigned int fold_hash(unsigned long x, unsigned long y)
 {
@@ -2314,7 +2314,7 @@ static int handle_lookup_down(struct nam
 	if (!(nd->flags & LOOKUP_RCU))
 		dget(nd->path.dentry);
 	return PTR_ERR(step_into(nd, WALK_NOFOLLOW,
-			nd->path.dentry, nd->inode, nd->seq));
+				nd->path.dentry, nd->inode, nd->seq));
 }
 
 /* Returns 0 and nd will be valid on success; Retuns error, otherwise. */
@@ -2330,7 +2330,7 @@ static int path_lookupat(struct nameidat
 	}
 
 	while (!(err = link_path_walk(s, nd)) &&
-	       (s = lookup_last(nd)) != NULL)
+			(s = lookup_last(nd)) != NULL)
 		;
 	if (!err)
 		err = complete_walk(nd);
@@ -2352,7 +2352,7 @@ static int path_lookupat(struct nameidat
 }
 
 int filename_lookup(int dfd, struct filename *name, unsigned flags,
-		    struct path *path, struct path *root)
+		struct path *path, struct path *root)
 {
 	int retval;
 	struct nameidata nd;
@@ -2371,7 +2371,7 @@ int filename_lookup(int dfd, struct file
 
 	if (likely(!retval))
 		audit_inode(name, path->dentry,
-			    flags & LOOKUP_MOUNTPOINT ? AUDIT_INODE_NOEVAL : 0);
+				flags & LOOKUP_MOUNTPOINT ? AUDIT_INODE_NOEVAL : 0);
 	restore_nameidata();
 	putname(name);
 	return retval;
@@ -2379,7 +2379,7 @@ int filename_lookup(int dfd, struct file
 
 /* Returns 0 and nd will be valid on success; Retuns error, otherwise. */
 static int path_parentat(struct nameidata *nd, unsigned flags,
-				struct path *parent)
+		struct path *parent)
 {
 	const char *s = path_init(nd, flags);
 	int err = link_path_walk(s, nd);
@@ -2395,8 +2395,8 @@ static int path_parentat(struct nameidat
 }
 
 static struct filename *filename_parentat(int dfd, struct filename *name,
-				unsigned int flags, struct path *parent,
-				struct qstr *last, int *type)
+		unsigned int flags, struct path *parent,
+		struct qstr *last, int *type)
 {
 	int retval;
 	struct nameidata nd;
@@ -2430,7 +2430,7 @@ struct dentry *kern_path_locked(const ch
 	int type;
 
 	filename = filename_parentat(AT_FDCWD, getname_kernel(name), 0, path,
-				    &last, &type);
+			&last, &type);
 	if (IS_ERR(filename))
 		return ERR_CAST(filename);
 	if (unlikely(type != LAST_NORM)) {
@@ -2451,7 +2451,7 @@ struct dentry *kern_path_locked(const ch
 int kern_path(const char *name, unsigned int flags, struct path *path)
 {
 	return filename_lookup(AT_FDCWD, getname_kernel(name),
-			       flags, path, NULL);
+			flags, path, NULL);
 }
 EXPORT_SYMBOL(kern_path);
 
@@ -2464,18 +2464,18 @@ EXPORT_SYMBOL(kern_path);
  * @path: pointer to struct path to fill
  */
 int vfs_path_lookup(struct dentry *dentry, struct vfsmount *mnt,
-		    const char *name, unsigned int flags,
-		    struct path *path)
+		const char *name, unsigned int flags,
+		struct path *path)
 {
 	struct path root = {.mnt = mnt, .dentry = dentry};
 	/* the first argument of filename_lookup() is ignored with root */
 	return filename_lookup(AT_FDCWD, getname_kernel(name),
-			       flags , path, &root);
+			flags , path, &root);
 }
 EXPORT_SYMBOL(vfs_path_lookup);
 
 static int lookup_one_len_common(const char *name, struct dentry *base,
-				 int len, struct qstr *this)
+		int len, struct qstr *this)
 {
 	this->name = name;
 	this->len = len;
@@ -2576,7 +2576,7 @@ EXPORT_SYMBOL(lookup_one_len);
  * i_mutex held, and will take the i_mutex itself if necessary.
  */
 struct dentry *lookup_one_len_unlocked(const char *name,
-				       struct dentry *base, int len)
+		struct dentry *base, int len)
 {
 	struct qstr this;
 	int err;
@@ -2602,7 +2602,7 @@ EXPORT_SYMBOL(lookup_one_len_unlocked);
  * this one avoids such problems.
  */
 struct dentry *lookup_positive_unlocked(const char *name,
-				       struct dentry *base, int len)
+		struct dentry *base, int len)
 {
 	struct dentry *ret = lookup_one_len_unlocked(name, base, len);
 	if (!IS_ERR(ret) && d_flags_negative(smp_load_acquire(&ret->d_flags))) {
@@ -2641,10 +2641,10 @@ int path_pts(struct path *path)
 #endif
 
 int user_path_at_empty(int dfd, const char __user *name, unsigned flags,
-		 struct path *path, int *empty)
+		struct path *path, int *empty)
 {
 	return filename_lookup(dfd, getname_flags(name, flags, empty),
-			       flags, path, NULL);
+			flags, path, NULL);
 }
 EXPORT_SYMBOL(user_path_at_empty);
 
@@ -2704,7 +2704,7 @@ static int may_delete(struct inode *dir,
 		return -EPERM;
 
 	if (check_sticky(dir, inode) || IS_APPEND(inode) ||
-	    IS_IMMUTABLE(inode) || IS_SWAPFILE(inode) || HAS_UNMAPPED_ID(inode))
+			IS_IMMUTABLE(inode) || IS_SWAPFILE(inode) || HAS_UNMAPPED_ID(inode))
 		return -EPERM;
 	if (isdir) {
 		if (!d_is_dir(victim))
@@ -2739,7 +2739,7 @@ static inline int may_create(struct inod
 		return -ENOENT;
 	s_user_ns = dir->i_sb->s_user_ns;
 	if (!kuid_has_mapping(s_user_ns, current_fsuid()) ||
-	    !kgid_has_mapping(s_user_ns, current_fsgid()))
+			!kgid_has_mapping(s_user_ns, current_fsgid()))
 		return -EOVERFLOW;
 	return inode_permission(dir, MAY_WRITE | MAY_EXEC);
 }
@@ -2846,29 +2846,29 @@ static int may_open(const struct path *p
 		return -ENOENT;
 
 	switch (inode->i_mode & S_IFMT) {
-	case S_IFLNK:
-		return -ELOOP;
-	case S_IFDIR:
-		if (acc_mode & MAY_WRITE)
-			return -EISDIR;
-		if (acc_mode & MAY_EXEC)
-			return -EACCES;
-		break;
-	case S_IFBLK:
-	case S_IFCHR:
-		if (!may_open_dev(path))
-			return -EACCES;
-		fallthrough;
-	case S_IFIFO:
-	case S_IFSOCK:
-		if (acc_mode & MAY_EXEC)
-			return -EACCES;
-		flag &= ~O_TRUNC;
-		break;
-	case S_IFREG:
-		if ((acc_mode & MAY_EXEC) && path_noexec(path))
-			return -EACCES;
-		break;
+		case S_IFLNK:
+			return -ELOOP;
+		case S_IFDIR:
+			if (acc_mode & MAY_WRITE)
+				return -EISDIR;
+			if (acc_mode & MAY_EXEC)
+				return -EACCES;
+			break;
+		case S_IFBLK:
+		case S_IFCHR:
+			if (!may_open_dev(path))
+				return -EACCES;
+			fallthrough;
+		case S_IFIFO:
+		case S_IFSOCK:
+			if (acc_mode & MAY_EXEC)
+				return -EACCES;
+			flag &= ~O_TRUNC;
+			break;
+		case S_IFREG:
+			if ((acc_mode & MAY_EXEC) && path_noexec(path))
+				return -EACCES;
+			break;
 	}
 
 	error = inode_permission(inode, MAY_OPEN | acc_mode);
@@ -2907,8 +2907,8 @@ static int handle_truncate(struct file *
 		error = security_path_truncate(path);
 	if (!error) {
 		error = do_truncate(path->dentry, 0,
-				    ATTR_MTIME|ATTR_CTIME|ATTR_OPEN,
-				    filp);
+				ATTR_MTIME|ATTR_CTIME|ATTR_OPEN,
+				filp);
 	}
 	put_write_access(inode);
 	return error;
@@ -2930,7 +2930,7 @@ static int may_o_create(const struct pat
 
 	s_user_ns = dir->dentry->d_sb->s_user_ns;
 	if (!kuid_has_mapping(s_user_ns, current_fsuid()) ||
-	    !kgid_has_mapping(s_user_ns, current_fsgid()))
+			!kgid_has_mapping(s_user_ns, current_fsgid()))
 		return -EOVERFLOW;
 
 	error = inode_permission(dir->dentry->d_inode, MAY_WRITE | MAY_EXEC);
@@ -2954,8 +2954,8 @@ static int may_o_create(const struct pat
  * Returns an error code otherwise.
  */
 static struct dentry *atomic_open(struct nameidata *nd, struct dentry *dentry,
-				  struct file *file,
-				  int open_flag, umode_t mode)
+		struct file *file,
+		int open_flag, umode_t mode)
 {
 	struct dentry *const DENTRY_NOT_SET = (void *) -1UL;
 	struct inode *dir =  nd->path.dentry->d_inode;
@@ -2967,7 +2967,7 @@ static struct dentry *atomic_open(struct
 	file->f_path.dentry = DENTRY_NOT_SET;
 	file->f_path.mnt = nd->path.mnt;
 	error = dir->i_op->atomic_open(dir, dentry, file,
-				       open_to_namei_flags(open_flag), mode);
+			open_to_namei_flags(open_flag), mode);
 	d_lookup_done(dentry);
 	if (!error) {
 		if (file->f_mode & FMODE_OPENED) {
@@ -3009,8 +3009,8 @@ static struct dentry *atomic_open(struct
  * An error code is returned on failure.
  */
 static struct dentry *lookup_open(struct nameidata *nd, struct file *file,
-				  const struct open_flags *op,
-				  bool got_write)
+		const struct open_flags *op,
+		bool got_write)
 {
 	struct dentry *dir = nd->path.dentry;
 	struct inode *dir_inode = dir->d_inode;
@@ -3080,7 +3080,7 @@ static struct dentry *lookup_open(struct
 
 	if (d_in_lookup(dentry)) {
 		struct dentry *res = dir_inode->i_op->lookup(dir_inode, dentry,
-							     nd->flags);
+				nd->flags);
 		d_lookup_done(dentry);
 		if (unlikely(res)) {
 			if (IS_ERR(res)) {
@@ -3101,7 +3101,7 @@ static struct dentry *lookup_open(struct
 			goto out_dput;
 		}
 		error = dir_inode->i_op->create(dir_inode, dentry, mode,
-						open_flag & O_EXCL);
+				open_flag & O_EXCL);
 		if (error)
 			goto out_dput;
 	}
@@ -3117,7 +3117,7 @@ out_dput:
 }
 
 static const char *open_last_lookups(struct nameidata *nd,
-		   struct file *file, const struct open_flags *op)
+		struct file *file, const struct open_flags *op)
 {
 	struct dentry *dir = nd->path.dentry;
 	int open_flag = op->open_flag;
@@ -3207,7 +3207,7 @@ finish_lookup:
  * Handle the last step of open()
  */
 static int do_open(struct nameidata *nd,
-		   struct file *file, const struct open_flags *op)
+		struct file *file, const struct open_flags *op)
 {
 	int open_flag = op->open_flag;
 	bool do_truncate;
@@ -3227,7 +3227,7 @@ static int do_open(struct nameidata *nd,
 		if (d_is_dir(nd->path.dentry))
 			return -EISDIR;
 		error = may_create_in_sticky(nd->dir_mode, nd->dir_uid,
-					     d_backing_inode(nd->path.dentry));
+				d_backing_inode(nd->path.dentry));
 		if (unlikely(error))
 			return error;
 	}
@@ -3346,7 +3346,7 @@ static int do_o_path(struct nameidata *n
 }
 
 static struct file *path_openat(struct nameidata *nd,
-			const struct open_flags *op, unsigned flags)
+		const struct open_flags *op, unsigned flags)
 {
 	struct file *file;
 	int error;
@@ -3362,7 +3362,7 @@ static struct file *path_openat(struct n
 	} else {
 		const char *s = path_init(nd, flags);
 		while (!(error = link_path_walk(s, nd)) &&
-		       (s = open_last_lookups(nd, file, op)) != NULL)
+				(s = open_last_lookups(nd, file, op)) != NULL)
 			;
 		if (!error)
 			error = do_open(nd, file, op);
@@ -3431,7 +3431,7 @@ struct file *do_file_open_root(struct de
 }
 
 static struct dentry *filename_create(int dfd, struct filename *name,
-				struct path *path, unsigned int lookup_flags)
+		struct path *path, unsigned int lookup_flags)
 {
 	struct dentry *dentry = ERR_PTR(-EEXIST);
 	struct qstr last;
@@ -3502,10 +3502,10 @@ out:
 }
 
 struct dentry *kern_path_create(int dfd, const char *pathname,
-				struct path *path, unsigned int lookup_flags)
+		struct path *path, unsigned int lookup_flags)
 {
 	return filename_create(dfd, getname_kernel(pathname),
-				path, lookup_flags);
+			path, lookup_flags);
 }
 EXPORT_SYMBOL(kern_path_create);
 
@@ -3519,7 +3519,7 @@ void done_path_create(struct path *path,
 EXPORT_SYMBOL(done_path_create);
 
 inline struct dentry *user_path_create(int dfd, const char __user *pathname,
-				struct path *path, unsigned int lookup_flags)
+		struct path *path, unsigned int lookup_flags)
 {
 	return filename_create(dfd, getname(pathname), path, lookup_flags);
 }
@@ -3534,7 +3534,7 @@ int vfs_mknod(struct inode *dir, struct
 		return error;
 
 	if ((S_ISCHR(mode) || S_ISBLK(mode)) && !is_whiteout &&
-	    !capable(CAP_MKNOD))
+			!capable(CAP_MKNOD))
 		return -EPERM;
 
 	if (!dir->i_op->mknod)
@@ -3558,17 +3558,17 @@ EXPORT_SYMBOL(vfs_mknod);
 static int may_mknod(umode_t mode)
 {
 	switch (mode & S_IFMT) {
-	case S_IFREG:
-	case S_IFCHR:
-	case S_IFBLK:
-	case S_IFIFO:
-	case S_IFSOCK:
-	case 0: /* zero mode translates to S_IFREG */
-		return 0;
-	case S_IFDIR:
-		return -EPERM;
-	default:
-		return -EINVAL;
+		case S_IFREG:
+		case S_IFCHR:
+		case S_IFBLK:
+		case S_IFIFO:
+		case S_IFSOCK:
+		case 0: /* zero mode translates to S_IFREG */
+			return 0;
+		case S_IFDIR:
+			return -EPERM;
+		default:
+			return -EINVAL;
 	}
 }
 
@@ -3738,20 +3738,20 @@ long do_rmdir(int dfd, struct filename *
 	unsigned int lookup_flags = 0;
 retry:
 	name = filename_parentat(dfd, name, lookup_flags,
-				&path, &last, &type);
+			&path, &last, &type);
 	if (IS_ERR(name))
 		return PTR_ERR(name);
 
 	switch (type) {
-	case LAST_DOTDOT:
-		error = -ENOTEMPTY;
-		goto exit1;
-	case LAST_DOT:
-		error = -EINVAL;
-		goto exit1;
-	case LAST_ROOT:
-		error = -EBUSY;
-		goto exit1;
+		case LAST_DOTDOT:
+			error = -ENOTEMPTY;
+			goto exit1;
+		case LAST_DOT:
+			error = -EINVAL;
+			goto exit1;
+		case LAST_ROOT:
+			error = -EBUSY;
+			goto exit1;
 	}
 
 	error = mnt_want_write(path.mnt);
@@ -3898,9 +3898,14 @@ exit2:
 		dput(dentry);
 	}
 	inode_unlock(path.dentry->d_inode);
-	if (inode)
-		iput(inode);	/* truncate the inode here */
-	inode = NULL;
+	if (inode) {
+		if (!IS_DAX(inode))
+			iput(inode);	/* truncate the inode here */
+		else {
+			iput_zero(inode);
+		}
+		//inode = NULL;
+	}
 	if (delegated_inode) {
 		error = break_deleg_wait(&delegated_inode);
 		if (!error)
@@ -3964,7 +3969,7 @@ int vfs_symlink(struct inode *dir, struc
 EXPORT_SYMBOL(vfs_symlink);
 
 static long do_symlinkat(const char __user *oldname, int newdfd,
-		  const char __user *newname)
+		const char __user *newname)
 {
 	int error;
 	struct filename *from;
@@ -4095,7 +4100,7 @@ EXPORT_SYMBOL(vfs_link);
  * and other special files.  --ADM
  */
 static int do_linkat(int olddfd, const char __user *oldname, int newdfd,
-	      const char __user *newname, int flags)
+		const char __user *newname, int flags)
 {
 	struct dentry *new_dentry;
 	struct path old_path, new_path;
@@ -4124,7 +4129,7 @@ retry:
 		return error;
 
 	new_dentry = user_path_create(newdfd, newname, &new_path,
-					(how & LOOKUP_REVAL));
+			(how & LOOKUP_REVAL));
 	error = PTR_ERR(new_dentry);
 	if (IS_ERR(new_dentry))
 		goto out;
@@ -4221,8 +4226,8 @@ SYSCALL_DEFINE2(link, const char __user
  *	   locking].
  */
 int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,
-	       struct inode *new_dir, struct dentry *new_dentry,
-	       struct inode **delegated_inode, unsigned int flags)
+		struct inode *new_dir, struct dentry *new_dentry,
+		struct inode **delegated_inode, unsigned int flags)
 {
 	int error;
 	bool is_dir = d_is_dir(old_dentry);
@@ -4273,7 +4278,7 @@ int vfs_rename(struct inode *old_dir, st
 	}
 
 	error = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,
-				      flags);
+			flags);
 	if (error)
 		return error;
 
@@ -4293,7 +4298,7 @@ int vfs_rename(struct inode *old_dir, st
 		if (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)
 			goto out;
 		if ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&
-		    old_dir->i_nlink >= max_links)
+				old_dir->i_nlink >= max_links)
 			goto out;
 	}
 	if (!is_dir) {
@@ -4307,7 +4312,7 @@ int vfs_rename(struct inode *old_dir, st
 			goto out;
 	}
 	error = old_dir->i_op->rename(old_dir, old_dentry,
-				       new_dir, new_dentry, flags);
+			new_dir, new_dentry, flags);
 	if (error)
 		goto out;
 
@@ -4333,10 +4338,10 @@ out:
 	dput(new_dentry);
 	if (!error) {
 		fsnotify_move(old_dir, new_dir, &old_name.name, is_dir,
-			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
+				!(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 		if (flags & RENAME_EXCHANGE) {
 			fsnotify_move(new_dir, old_dir, &old_dentry->d_name,
-				      new_is_dir, NULL, new_dentry);
+					new_is_dir, NULL, new_dentry);
 		}
 	}
 	release_dentry_name_snapshot(&old_name);
@@ -4346,7 +4351,7 @@ out:
 EXPORT_SYMBOL(vfs_rename);
 
 static int do_renameat2(int olddfd, const char __user *oldname, int newdfd,
-			const char __user *newname, unsigned int flags)
+		const char __user *newname, unsigned int flags)
 {
 	struct dentry *old_dentry, *new_dentry;
 	struct dentry *trap;
@@ -4364,7 +4369,7 @@ static int do_renameat2(int olddfd, cons
 		return -EINVAL;
 
 	if ((flags & (RENAME_NOREPLACE | RENAME_WHITEOUT)) &&
-	    (flags & RENAME_EXCHANGE))
+			(flags & RENAME_EXCHANGE))
 		return -EINVAL;
 
 	if (flags & RENAME_EXCHANGE)
@@ -4372,14 +4377,14 @@ static int do_renameat2(int olddfd, cons
 
 retry:
 	from = filename_parentat(olddfd, getname(oldname), lookup_flags,
-				&old_path, &old_last, &old_type);
+			&old_path, &old_last, &old_type);
 	if (IS_ERR(from)) {
 		error = PTR_ERR(from);
 		goto exit;
 	}
 
 	to = filename_parentat(newdfd, getname(newname), lookup_flags,
-				&new_path, &new_last, &new_type);
+			&new_path, &new_last, &new_type);
 	if (IS_ERR(to)) {
 		error = PTR_ERR(to);
 		goto exit1;
@@ -4450,12 +4455,12 @@ retry_deleg:
 		goto exit5;
 
 	error = security_path_rename(&old_path, old_dentry,
-				     &new_path, new_dentry, flags);
+			&new_path, new_dentry, flags);
 	if (error)
 		goto exit5;
 	error = vfs_rename(old_path.dentry->d_inode, old_dentry,
-			   new_path.dentry->d_inode, new_dentry,
-			   &delegated_inode, flags);
+			new_path.dentry->d_inode, new_dentry,
+			&delegated_inode, flags);
 exit5:
 	dput(new_dentry);
 exit4:
@@ -4585,7 +4590,7 @@ EXPORT_SYMBOL(vfs_get_link);
 
 /* get the link contents into pagecache */
 const char *page_get_link(struct dentry *dentry, struct inode *inode,
-			  struct delayed_call *callback)
+		struct delayed_call *callback)
 {
 	char *kaddr;
 	struct page *page;
@@ -4623,8 +4628,8 @@ int page_readlink(struct dentry *dentry,
 {
 	DEFINE_DELAYED_CALL(done);
 	int res = readlink_copy(buffer, buflen,
-				page_get_link(dentry, d_inode(dentry),
-					      &done));
+			page_get_link(dentry, d_inode(dentry),
+				&done));
 	do_delayed_call(&done);
 	return res;
 }
@@ -4645,14 +4650,14 @@ int __page_symlink(struct inode *inode,
 
 retry:
 	err = pagecache_write_begin(NULL, mapping, 0, len-1,
-				flags, &page, &fsdata);
+			flags, &page, &fsdata);
 	if (err)
 		goto fail;
 
 	memcpy(page_address(page), symname, len-1);
 
 	err = pagecache_write_end(NULL, mapping, 0, len-1, len-1,
-							page, fsdata);
+			page, fsdata);
 	if (err < 0)
 		goto fail;
 	if (err < len-1)
diff '--color=auto' -uprN linux-5.9.1/fs/super.c pm-opzero/fs/super.c
--- linux-5.9.1/fs/super.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/fs/super.c	2022-12-12 17:21:19.652573028 +0900
@@ -854,6 +854,8 @@ struct super_block *get_active_super(str
 restart:
 	spin_lock(&sb_lock);
 	list_for_each_entry(sb, &super_blocks, s_list) {
+	//	printk(KERN_ERR "%s: id(%s), bdevp(%p), dev(%u)\n",
+	//		__func__, sb->s_id, sb->s_bdev, sb->s_dev);
 		if (hlist_unhashed(&sb->s_instances))
 			continue;
 		if (sb->s_bdev == bdev) {
diff '--color=auto' -uprN linux-5.9.1/.git/config pm-opzero/.git/config
--- linux-5.9.1/.git/config	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/config	2022-12-12 17:21:16.696559520 +0900
@@ -0,0 +1,11 @@
+[core]
+	repositoryformatversion = 0
+	filemode = true
+	bare = false
+	logallrefupdates = true
+[remote "origin"]
+	url = https://github.com/EMDC-OS/pm-opzero.git
+	fetch = +refs/heads/*:refs/remotes/origin/*
+[branch "master"]
+	remote = origin
+	merge = refs/heads/master
diff '--color=auto' -uprN linux-5.9.1/.git/description pm-opzero/.git/description
--- linux-5.9.1/.git/description	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/description	2022-12-12 17:21:05.508507447 +0900
@@ -0,0 +1 @@
+Unnamed repository; edit this file 'description' to name the repository.
diff '--color=auto' -uprN linux-5.9.1/.git/HEAD pm-opzero/.git/HEAD
--- linux-5.9.1/.git/HEAD	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/HEAD	2022-12-12 17:21:16.692559502 +0900
@@ -0,0 +1 @@
+ref: refs/heads/master
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/applypatch-msg.sample pm-opzero/.git/hooks/applypatch-msg.sample
--- linux-5.9.1/.git/hooks/applypatch-msg.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/applypatch-msg.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,15 @@
+#!/bin/sh
+#
+# An example hook script to check the commit log message taken by
+# applypatch from an e-mail message.
+#
+# The hook should exit with non-zero status after issuing an
+# appropriate message if it wants to stop the commit.  The hook is
+# allowed to edit the commit message file.
+#
+# To enable this hook, rename this file to "applypatch-msg".
+
+. git-sh-setup
+commitmsg="$(git rev-parse --git-path hooks/commit-msg)"
+test -x "$commitmsg" && exec "$commitmsg" ${1+"$@"}
+:
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/commit-msg.sample pm-opzero/.git/hooks/commit-msg.sample
--- linux-5.9.1/.git/hooks/commit-msg.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/commit-msg.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,24 @@
+#!/bin/sh
+#
+# An example hook script to check the commit log message.
+# Called by "git commit" with one argument, the name of the file
+# that has the commit message.  The hook should exit with non-zero
+# status after issuing an appropriate message if it wants to stop the
+# commit.  The hook is allowed to edit the commit message file.
+#
+# To enable this hook, rename this file to "commit-msg".
+
+# Uncomment the below to add a Signed-off-by line to the message.
+# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
+# hook is more suited to it.
+#
+# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
+# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"
+
+# This example catches duplicate Signed-off-by lines.
+
+test "" = "$(grep '^Signed-off-by: ' "$1" |
+	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
+	echo >&2 Duplicate Signed-off-by lines.
+	exit 1
+}
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/fsmonitor-watchman.sample pm-opzero/.git/hooks/fsmonitor-watchman.sample
--- linux-5.9.1/.git/hooks/fsmonitor-watchman.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/fsmonitor-watchman.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,173 @@
+#!/usr/bin/perl
+
+use strict;
+use warnings;
+use IPC::Open2;
+
+# An example hook script to integrate Watchman
+# (https://facebook.github.io/watchman/) with git to speed up detecting
+# new and modified files.
+#
+# The hook is passed a version (currently 2) and last update token
+# formatted as a string and outputs to stdout a new update token and
+# all files that have been modified since the update token. Paths must
+# be relative to the root of the working tree and separated by a single NUL.
+#
+# To enable this hook, rename this file to "query-watchman" and set
+# 'git config core.fsmonitor .git/hooks/query-watchman'
+#
+my ($version, $last_update_token) = @ARGV;
+
+# Uncomment for debugging
+# print STDERR "$0 $version $last_update_token\n";
+
+# Check the hook interface version
+if ($version ne 2) {
+	die "Unsupported query-fsmonitor hook version '$version'.\n" .
+	    "Falling back to scanning...\n";
+}
+
+my $git_work_tree = get_working_dir();
+
+my $retry = 1;
+
+my $json_pkg;
+eval {
+	require JSON::XS;
+	$json_pkg = "JSON::XS";
+	1;
+} or do {
+	require JSON::PP;
+	$json_pkg = "JSON::PP";
+};
+
+launch_watchman();
+
+sub launch_watchman {
+	my $o = watchman_query();
+	if (is_work_tree_watched($o)) {
+		output_result($o->{clock}, @{$o->{files}});
+	}
+}
+
+sub output_result {
+	my ($clockid, @files) = @_;
+
+	# Uncomment for debugging watchman output
+	# open (my $fh, ">", ".git/watchman-output.out");
+	# binmode $fh, ":utf8";
+	# print $fh "$clockid\n@files\n";
+	# close $fh;
+
+	binmode STDOUT, ":utf8";
+	print $clockid;
+	print "\0";
+	local $, = "\0";
+	print @files;
+}
+
+sub watchman_clock {
+	my $response = qx/watchman clock "$git_work_tree"/;
+	die "Failed to get clock id on '$git_work_tree'.\n" .
+		"Falling back to scanning...\n" if $? != 0;
+
+	return $json_pkg->new->utf8->decode($response);
+}
+
+sub watchman_query {
+	my $pid = open2(\*CHLD_OUT, \*CHLD_IN, 'watchman -j --no-pretty')
+	or die "open2() failed: $!\n" .
+	"Falling back to scanning...\n";
+
+	# In the query expression below we're asking for names of files that
+	# changed since $last_update_token but not from the .git folder.
+	#
+	# To accomplish this, we're using the "since" generator to use the
+	# recency index to select candidate nodes and "fields" to limit the
+	# output to file names only. Then we're using the "expression" term to
+	# further constrain the results.
+	if (substr($last_update_token, 0, 1) eq "c") {
+		$last_update_token = "\"$last_update_token\"";
+	}
+	my $query = <<"	END";
+		["query", "$git_work_tree", {
+			"since": $last_update_token,
+			"fields": ["name"],
+			"expression": ["not", ["dirname", ".git"]]
+		}]
+	END
+
+	# Uncomment for debugging the watchman query
+	# open (my $fh, ">", ".git/watchman-query.json");
+	# print $fh $query;
+	# close $fh;
+
+	print CHLD_IN $query;
+	close CHLD_IN;
+	my $response = do {local $/; <CHLD_OUT>};
+
+	# Uncomment for debugging the watch response
+	# open ($fh, ">", ".git/watchman-response.json");
+	# print $fh $response;
+	# close $fh;
+
+	die "Watchman: command returned no output.\n" .
+	"Falling back to scanning...\n" if $response eq "";
+	die "Watchman: command returned invalid output: $response\n" .
+	"Falling back to scanning...\n" unless $response =~ /^\{/;
+
+	return $json_pkg->new->utf8->decode($response);
+}
+
+sub is_work_tree_watched {
+	my ($output) = @_;
+	my $error = $output->{error};
+	if ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {
+		$retry--;
+		my $response = qx/watchman watch "$git_work_tree"/;
+		die "Failed to make watchman watch '$git_work_tree'.\n" .
+		    "Falling back to scanning...\n" if $? != 0;
+		$output = $json_pkg->new->utf8->decode($response);
+		$error = $output->{error};
+		die "Watchman: $error.\n" .
+		"Falling back to scanning...\n" if $error;
+
+		# Uncomment for debugging watchman output
+		# open (my $fh, ">", ".git/watchman-output.out");
+		# close $fh;
+
+		# Watchman will always return all files on the first query so
+		# return the fast "everything is dirty" flag to git and do the
+		# Watchman query just to get it over with now so we won't pay
+		# the cost in git to look up each individual file.
+		my $o = watchman_clock();
+		$error = $output->{error};
+
+		die "Watchman: $error.\n" .
+		"Falling back to scanning...\n" if $error;
+
+		output_result($o->{clock}, ("/"));
+		$last_update_token = $o->{clock};
+
+		eval { launch_watchman() };
+		return 0;
+	}
+
+	die "Watchman: $error.\n" .
+	"Falling back to scanning...\n" if $error;
+
+	return 1;
+}
+
+sub get_working_dir {
+	my $working_dir;
+	if ($^O =~ 'msys' || $^O =~ 'cygwin') {
+		$working_dir = Win32::GetCwd();
+		$working_dir =~ tr/\\/\//;
+	} else {
+		require Cwd;
+		$working_dir = Cwd::cwd();
+	}
+
+	return $working_dir;
+}
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/post-update.sample pm-opzero/.git/hooks/post-update.sample
--- linux-5.9.1/.git/hooks/post-update.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/post-update.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,8 @@
+#!/bin/sh
+#
+# An example hook script to prepare a packed repository for use over
+# dumb transports.
+#
+# To enable this hook, rename this file to "post-update".
+
+exec git update-server-info
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/pre-applypatch.sample pm-opzero/.git/hooks/pre-applypatch.sample
--- linux-5.9.1/.git/hooks/pre-applypatch.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/pre-applypatch.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,14 @@
+#!/bin/sh
+#
+# An example hook script to verify what is about to be committed
+# by applypatch from an e-mail message.
+#
+# The hook should exit with non-zero status after issuing an
+# appropriate message if it wants to stop the commit.
+#
+# To enable this hook, rename this file to "pre-applypatch".
+
+. git-sh-setup
+precommit="$(git rev-parse --git-path hooks/pre-commit)"
+test -x "$precommit" && exec "$precommit" ${1+"$@"}
+:
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/pre-commit.sample pm-opzero/.git/hooks/pre-commit.sample
--- linux-5.9.1/.git/hooks/pre-commit.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/pre-commit.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,49 @@
+#!/bin/sh
+#
+# An example hook script to verify what is about to be committed.
+# Called by "git commit" with no arguments.  The hook should
+# exit with non-zero status after issuing an appropriate message if
+# it wants to stop the commit.
+#
+# To enable this hook, rename this file to "pre-commit".
+
+if git rev-parse --verify HEAD >/dev/null 2>&1
+then
+	against=HEAD
+else
+	# Initial commit: diff against an empty tree object
+	against=$(git hash-object -t tree /dev/null)
+fi
+
+# If you want to allow non-ASCII filenames set this variable to true.
+allownonascii=$(git config --type=bool hooks.allownonascii)
+
+# Redirect output to stderr.
+exec 1>&2
+
+# Cross platform projects tend to avoid non-ASCII filenames; prevent
+# them from being added to the repository. We exploit the fact that the
+# printable range starts at the space character and ends with tilde.
+if [ "$allownonascii" != "true" ] &&
+	# Note that the use of brackets around a tr range is ok here, (it's
+	# even required, for portability to Solaris 10's /usr/bin/tr), since
+	# the square bracket bytes happen to fall in the designated range.
+	test $(git diff --cached --name-only --diff-filter=A -z $against |
+	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
+then
+	cat <<\EOF
+Error: Attempt to add a non-ASCII file name.
+
+This can cause problems if you want to work with people on other platforms.
+
+To be portable it is advisable to rename the file.
+
+If you know what you are doing you can disable this check using:
+
+  git config hooks.allownonascii true
+EOF
+	exit 1
+fi
+
+# If there are whitespace errors, print the offending file names and fail.
+exec git diff-index --check --cached $against --
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/pre-merge-commit.sample pm-opzero/.git/hooks/pre-merge-commit.sample
--- linux-5.9.1/.git/hooks/pre-merge-commit.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/pre-merge-commit.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,13 @@
+#!/bin/sh
+#
+# An example hook script to verify what is about to be committed.
+# Called by "git merge" with no arguments.  The hook should
+# exit with non-zero status after issuing an appropriate message to
+# stderr if it wants to stop the merge commit.
+#
+# To enable this hook, rename this file to "pre-merge-commit".
+
+. git-sh-setup
+test -x "$GIT_DIR/hooks/pre-commit" &&
+        exec "$GIT_DIR/hooks/pre-commit"
+:
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/prepare-commit-msg.sample pm-opzero/.git/hooks/prepare-commit-msg.sample
--- linux-5.9.1/.git/hooks/prepare-commit-msg.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/prepare-commit-msg.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,42 @@
+#!/bin/sh
+#
+# An example hook script to prepare the commit log message.
+# Called by "git commit" with the name of the file that has the
+# commit message, followed by the description of the commit
+# message's source.  The hook's purpose is to edit the commit
+# message file.  If the hook fails with a non-zero status,
+# the commit is aborted.
+#
+# To enable this hook, rename this file to "prepare-commit-msg".
+
+# This hook includes three examples. The first one removes the
+# "# Please enter the commit message..." help message.
+#
+# The second includes the output of "git diff --name-status -r"
+# into the message, just before the "git status" output.  It is
+# commented because it doesn't cope with --amend or with squashed
+# commits.
+#
+# The third example adds a Signed-off-by line to the message, that can
+# still be edited.  This is rarely a good idea.
+
+COMMIT_MSG_FILE=$1
+COMMIT_SOURCE=$2
+SHA1=$3
+
+/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' "$COMMIT_MSG_FILE"
+
+# case "$COMMIT_SOURCE,$SHA1" in
+#  ,|template,)
+#    /usr/bin/perl -i.bak -pe '
+#       print "\n" . `git diff --cached --name-status -r`
+# 	 if /^#/ && $first++ == 0' "$COMMIT_MSG_FILE" ;;
+#  *) ;;
+# esac
+
+# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
+# git interpret-trailers --in-place --trailer "$SOB" "$COMMIT_MSG_FILE"
+# if test -z "$COMMIT_SOURCE"
+# then
+#   /usr/bin/perl -i.bak -pe 'print "\n" if !$first_line++' "$COMMIT_MSG_FILE"
+# fi
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/pre-push.sample pm-opzero/.git/hooks/pre-push.sample
--- linux-5.9.1/.git/hooks/pre-push.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/pre-push.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,53 @@
+#!/bin/sh
+
+# An example hook script to verify what is about to be pushed.  Called by "git
+# push" after it has checked the remote status, but before anything has been
+# pushed.  If this script exits with a non-zero status nothing will be pushed.
+#
+# This hook is called with the following parameters:
+#
+# $1 -- Name of the remote to which the push is being done
+# $2 -- URL to which the push is being done
+#
+# If pushing without using a named remote those arguments will be equal.
+#
+# Information about the commits which are being pushed is supplied as lines to
+# the standard input in the form:
+#
+#   <local ref> <local oid> <remote ref> <remote oid>
+#
+# This sample shows how to prevent push of commits where the log message starts
+# with "WIP" (work in progress).
+
+remote="$1"
+url="$2"
+
+zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
+
+while read local_ref local_oid remote_ref remote_oid
+do
+	if test "$local_oid" = "$zero"
+	then
+		# Handle delete
+		:
+	else
+		if test "$remote_oid" = "$zero"
+		then
+			# New branch, examine all commits
+			range="$local_oid"
+		else
+			# Update to existing branch, examine new commits
+			range="$remote_oid..$local_oid"
+		fi
+
+		# Check for WIP commit
+		commit=$(git rev-list -n 1 --grep '^WIP' "$range")
+		if test -n "$commit"
+		then
+			echo >&2 "Found WIP commit in $local_ref, not pushing"
+			exit 1
+		fi
+	fi
+done
+
+exit 0
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/pre-rebase.sample pm-opzero/.git/hooks/pre-rebase.sample
--- linux-5.9.1/.git/hooks/pre-rebase.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/pre-rebase.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,169 @@
+#!/bin/sh
+#
+# Copyright (c) 2006, 2008 Junio C Hamano
+#
+# The "pre-rebase" hook is run just before "git rebase" starts doing
+# its job, and can prevent the command from running by exiting with
+# non-zero status.
+#
+# The hook is called with the following parameters:
+#
+# $1 -- the upstream the series was forked from.
+# $2 -- the branch being rebased (or empty when rebasing the current branch).
+#
+# This sample shows how to prevent topic branches that are already
+# merged to 'next' branch from getting rebased, because allowing it
+# would result in rebasing already published history.
+
+publish=next
+basebranch="$1"
+if test "$#" = 2
+then
+	topic="refs/heads/$2"
+else
+	topic=`git symbolic-ref HEAD` ||
+	exit 0 ;# we do not interrupt rebasing detached HEAD
+fi
+
+case "$topic" in
+refs/heads/??/*)
+	;;
+*)
+	exit 0 ;# we do not interrupt others.
+	;;
+esac
+
+# Now we are dealing with a topic branch being rebased
+# on top of master.  Is it OK to rebase it?
+
+# Does the topic really exist?
+git show-ref -q "$topic" || {
+	echo >&2 "No such branch $topic"
+	exit 1
+}
+
+# Is topic fully merged to master?
+not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
+if test -z "$not_in_master"
+then
+	echo >&2 "$topic is fully merged to master; better remove it."
+	exit 1 ;# we could allow it, but there is no point.
+fi
+
+# Is topic ever merged to next?  If so you should not be rebasing it.
+only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
+only_next_2=`git rev-list ^master           ${publish} | sort`
+if test "$only_next_1" = "$only_next_2"
+then
+	not_in_topic=`git rev-list "^$topic" master`
+	if test -z "$not_in_topic"
+	then
+		echo >&2 "$topic is already up to date with master"
+		exit 1 ;# we could allow it, but there is no point.
+	else
+		exit 0
+	fi
+else
+	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
+	/usr/bin/perl -e '
+		my $topic = $ARGV[0];
+		my $msg = "* $topic has commits already merged to public branch:\n";
+		my (%not_in_next) = map {
+			/^([0-9a-f]+) /;
+			($1 => 1);
+		} split(/\n/, $ARGV[1]);
+		for my $elem (map {
+				/^([0-9a-f]+) (.*)$/;
+				[$1 => $2];
+			} split(/\n/, $ARGV[2])) {
+			if (!exists $not_in_next{$elem->[0]}) {
+				if ($msg) {
+					print STDERR $msg;
+					undef $msg;
+				}
+				print STDERR " $elem->[1]\n";
+			}
+		}
+	' "$topic" "$not_in_next" "$not_in_master"
+	exit 1
+fi
+
+<<\DOC_END
+
+This sample hook safeguards topic branches that have been
+published from being rewound.
+
+The workflow assumed here is:
+
+ * Once a topic branch forks from "master", "master" is never
+   merged into it again (either directly or indirectly).
+
+ * Once a topic branch is fully cooked and merged into "master",
+   it is deleted.  If you need to build on top of it to correct
+   earlier mistakes, a new topic branch is created by forking at
+   the tip of the "master".  This is not strictly necessary, but
+   it makes it easier to keep your history simple.
+
+ * Whenever you need to test or publish your changes to topic
+   branches, merge them into "next" branch.
+
+The script, being an example, hardcodes the publish branch name
+to be "next", but it is trivial to make it configurable via
+$GIT_DIR/config mechanism.
+
+With this workflow, you would want to know:
+
+(1) ... if a topic branch has ever been merged to "next".  Young
+    topic branches can have stupid mistakes you would rather
+    clean up before publishing, and things that have not been
+    merged into other branches can be easily rebased without
+    affecting other people.  But once it is published, you would
+    not want to rewind it.
+
+(2) ... if a topic branch has been fully merged to "master".
+    Then you can delete it.  More importantly, you should not
+    build on top of it -- other people may already want to
+    change things related to the topic as patches against your
+    "master", so if you need further changes, it is better to
+    fork the topic (perhaps with the same name) afresh from the
+    tip of "master".
+
+Let's look at this example:
+
+		   o---o---o---o---o---o---o---o---o---o "next"
+		  /       /           /           /
+		 /   a---a---b A     /           /
+		/   /               /           /
+	       /   /   c---c---c---c B         /
+	      /   /   /             \         /
+	     /   /   /   b---b C     \       /
+	    /   /   /   /             \     /
+    ---o---o---o---o---o---o---o---o---o---o---o "master"
+
+
+A, B and C are topic branches.
+
+ * A has one fix since it was merged up to "next".
+
+ * B has finished.  It has been fully merged up to "master" and "next",
+   and is ready to be deleted.
+
+ * C has not merged to "next" at all.
+
+We would want to allow C to be rebased, refuse A, and encourage
+B to be deleted.
+
+To compute (1):
+
+	git rev-list ^master ^topic next
+	git rev-list ^master        next
+
+	if these match, topic has not merged in next at all.
+
+To compute (2):
+
+	git rev-list master..topic
+
+	if this is empty, it is fully merged to "master".
+
+DOC_END
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/pre-receive.sample pm-opzero/.git/hooks/pre-receive.sample
--- linux-5.9.1/.git/hooks/pre-receive.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/pre-receive.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,24 @@
+#!/bin/sh
+#
+# An example hook script to make use of push options.
+# The example simply echoes all push options that start with 'echoback='
+# and rejects all pushes when the "reject" push option is used.
+#
+# To enable this hook, rename this file to "pre-receive".
+
+if test -n "$GIT_PUSH_OPTION_COUNT"
+then
+	i=0
+	while test "$i" -lt "$GIT_PUSH_OPTION_COUNT"
+	do
+		eval "value=\$GIT_PUSH_OPTION_$i"
+		case "$value" in
+		echoback=*)
+			echo "echo from the pre-receive-hook: ${value#*=}" >&2
+			;;
+		reject)
+			exit 1
+		esac
+		i=$((i + 1))
+	done
+fi
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/push-to-checkout.sample pm-opzero/.git/hooks/push-to-checkout.sample
--- linux-5.9.1/.git/hooks/push-to-checkout.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/push-to-checkout.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,78 @@
+#!/bin/sh
+
+# An example hook script to update a checked-out tree on a git push.
+#
+# This hook is invoked by git-receive-pack(1) when it reacts to git
+# push and updates reference(s) in its repository, and when the push
+# tries to update the branch that is currently checked out and the
+# receive.denyCurrentBranch configuration variable is set to
+# updateInstead.
+#
+# By default, such a push is refused if the working tree and the index
+# of the remote repository has any difference from the currently
+# checked out commit; when both the working tree and the index match
+# the current commit, they are updated to match the newly pushed tip
+# of the branch. This hook is to be used to override the default
+# behaviour; however the code below reimplements the default behaviour
+# as a starting point for convenient modification.
+#
+# The hook receives the commit with which the tip of the current
+# branch is going to be updated:
+commit=$1
+
+# It can exit with a non-zero status to refuse the push (when it does
+# so, it must not modify the index or the working tree).
+die () {
+	echo >&2 "$*"
+	exit 1
+}
+
+# Or it can make any necessary changes to the working tree and to the
+# index to bring them to the desired state when the tip of the current
+# branch is updated to the new commit, and exit with a zero status.
+#
+# For example, the hook can simply run git read-tree -u -m HEAD "$1"
+# in order to emulate git fetch that is run in the reverse direction
+# with git push, as the two-tree form of git read-tree -u -m is
+# essentially the same as git switch or git checkout that switches
+# branches while keeping the local changes in the working tree that do
+# not interfere with the difference between the branches.
+
+# The below is a more-or-less exact translation to shell of the C code
+# for the default behaviour for git's push-to-checkout hook defined in
+# the push_to_deploy() function in builtin/receive-pack.c.
+#
+# Note that the hook will be executed from the repository directory,
+# not from the working tree, so if you want to perform operations on
+# the working tree, you will have to adapt your code accordingly, e.g.
+# by adding "cd .." or using relative paths.
+
+if ! git update-index -q --ignore-submodules --refresh
+then
+	die "Up-to-date check failed"
+fi
+
+if ! git diff-files --quiet --ignore-submodules --
+then
+	die "Working directory has unstaged changes"
+fi
+
+# This is a rough translation of:
+#
+#   head_has_history() ? "HEAD" : EMPTY_TREE_SHA1_HEX
+if git cat-file -e HEAD 2>/dev/null
+then
+	head=HEAD
+else
+	head=$(git hash-object -t tree --stdin </dev/null)
+fi
+
+if ! git diff-index --quiet --cached --ignore-submodules $head --
+then
+	die "Working directory has staged changes"
+fi
+
+if ! git read-tree -u -m "$commit"
+then
+	die "Could not update working tree to new HEAD"
+fi
diff '--color=auto' -uprN linux-5.9.1/.git/hooks/update.sample pm-opzero/.git/hooks/update.sample
--- linux-5.9.1/.git/hooks/update.sample	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/hooks/update.sample	2022-12-12 17:21:05.512507466 +0900
@@ -0,0 +1,128 @@
+#!/bin/sh
+#
+# An example hook script to block unannotated tags from entering.
+# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
+#
+# To enable this hook, rename this file to "update".
+#
+# Config
+# ------
+# hooks.allowunannotated
+#   This boolean sets whether unannotated tags will be allowed into the
+#   repository.  By default they won't be.
+# hooks.allowdeletetag
+#   This boolean sets whether deleting tags will be allowed in the
+#   repository.  By default they won't be.
+# hooks.allowmodifytag
+#   This boolean sets whether a tag may be modified after creation. By default
+#   it won't be.
+# hooks.allowdeletebranch
+#   This boolean sets whether deleting branches will be allowed in the
+#   repository.  By default they won't be.
+# hooks.denycreatebranch
+#   This boolean sets whether remotely creating branches will be denied
+#   in the repository.  By default this is allowed.
+#
+
+# --- Command line
+refname="$1"
+oldrev="$2"
+newrev="$3"
+
+# --- Safety check
+if [ -z "$GIT_DIR" ]; then
+	echo "Don't run this script from the command line." >&2
+	echo " (if you want, you could supply GIT_DIR then run" >&2
+	echo "  $0 <ref> <oldrev> <newrev>)" >&2
+	exit 1
+fi
+
+if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
+	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
+	exit 1
+fi
+
+# --- Config
+allowunannotated=$(git config --type=bool hooks.allowunannotated)
+allowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)
+denycreatebranch=$(git config --type=bool hooks.denycreatebranch)
+allowdeletetag=$(git config --type=bool hooks.allowdeletetag)
+allowmodifytag=$(git config --type=bool hooks.allowmodifytag)
+
+# check for no description
+projectdesc=$(sed -e '1q' "$GIT_DIR/description")
+case "$projectdesc" in
+"Unnamed repository"* | "")
+	echo "*** Project description file hasn't been set" >&2
+	exit 1
+	;;
+esac
+
+# --- Check types
+# if $newrev is 0000...0000, it's a commit to delete a ref.
+zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
+if [ "$newrev" = "$zero" ]; then
+	newrev_type=delete
+else
+	newrev_type=$(git cat-file -t $newrev)
+fi
+
+case "$refname","$newrev_type" in
+	refs/tags/*,commit)
+		# un-annotated tag
+		short_refname=${refname##refs/tags/}
+		if [ "$allowunannotated" != "true" ]; then
+			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
+			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
+			exit 1
+		fi
+		;;
+	refs/tags/*,delete)
+		# delete tag
+		if [ "$allowdeletetag" != "true" ]; then
+			echo "*** Deleting a tag is not allowed in this repository" >&2
+			exit 1
+		fi
+		;;
+	refs/tags/*,tag)
+		# annotated tag
+		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
+		then
+			echo "*** Tag '$refname' already exists." >&2
+			echo "*** Modifying a tag is not allowed in this repository." >&2
+			exit 1
+		fi
+		;;
+	refs/heads/*,commit)
+		# branch
+		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
+			echo "*** Creating a branch is not allowed in this repository" >&2
+			exit 1
+		fi
+		;;
+	refs/heads/*,delete)
+		# delete branch
+		if [ "$allowdeletebranch" != "true" ]; then
+			echo "*** Deleting a branch is not allowed in this repository" >&2
+			exit 1
+		fi
+		;;
+	refs/remotes/*,commit)
+		# tracking branch
+		;;
+	refs/remotes/*,delete)
+		# delete tracking branch
+		if [ "$allowdeletebranch" != "true" ]; then
+			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
+			exit 1
+		fi
+		;;
+	*)
+		# Anything else (is there anything else?)
+		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
+		exit 1
+		;;
+esac
+
+# --- Finished
+exit 0
Binary files linux-5.9.1/.git/index and pm-opzero/.git/index differ
diff '--color=auto' -uprN linux-5.9.1/.git/info/exclude pm-opzero/.git/info/exclude
--- linux-5.9.1/.git/info/exclude	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/info/exclude	2022-12-12 17:21:05.508507447 +0900
@@ -0,0 +1,6 @@
+# git ls-files --others --exclude-from=.git/info/exclude
+# Lines that start with '#' are comments.
+# For a project mostly in C, the following would be a good set of
+# exclude patterns (uncomment them if you want to use them):
+# *.[oa]
+# *~
diff '--color=auto' -uprN linux-5.9.1/.git/logs/HEAD pm-opzero/.git/logs/HEAD
--- linux-5.9.1/.git/logs/HEAD	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/logs/HEAD	2022-12-12 17:21:16.692559502 +0900
@@ -0,0 +1 @@
+0000000000000000000000000000000000000000 0b83b19dc757b7ea9db5c051c36b8d935c7b559e Jongseok Kim <ks77sj@gmail.com> 1670833276 +0900	clone: from https://github.com/EMDC-OS/pm-opzero.git
diff '--color=auto' -uprN linux-5.9.1/.git/logs/refs/heads/master pm-opzero/.git/logs/refs/heads/master
--- linux-5.9.1/.git/logs/refs/heads/master	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/logs/refs/heads/master	2022-12-12 17:21:16.692559502 +0900
@@ -0,0 +1 @@
+0000000000000000000000000000000000000000 0b83b19dc757b7ea9db5c051c36b8d935c7b559e Jongseok Kim <ks77sj@gmail.com> 1670833276 +0900	clone: from https://github.com/EMDC-OS/pm-opzero.git
diff '--color=auto' -uprN linux-5.9.1/.git/logs/refs/remotes/origin/HEAD pm-opzero/.git/logs/refs/remotes/origin/HEAD
--- linux-5.9.1/.git/logs/refs/remotes/origin/HEAD	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/logs/refs/remotes/origin/HEAD	2022-12-12 17:21:16.692559502 +0900
@@ -0,0 +1 @@
+0000000000000000000000000000000000000000 0b83b19dc757b7ea9db5c051c36b8d935c7b559e Jongseok Kim <ks77sj@gmail.com> 1670833276 +0900	clone: from https://github.com/EMDC-OS/pm-opzero.git
Binary files linux-5.9.1/.git/objects/pack/pack-f18404361a508446826f4edfe74c02beedb76570.idx and pm-opzero/.git/objects/pack/pack-f18404361a508446826f4edfe74c02beedb76570.idx differ
Binary files linux-5.9.1/.git/objects/pack/pack-f18404361a508446826f4edfe74c02beedb76570.pack and pm-opzero/.git/objects/pack/pack-f18404361a508446826f4edfe74c02beedb76570.pack differ
diff '--color=auto' -uprN linux-5.9.1/.git/packed-refs pm-opzero/.git/packed-refs
--- linux-5.9.1/.git/packed-refs	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/packed-refs	2022-12-12 17:21:16.692559502 +0900
@@ -0,0 +1,4 @@
+# pack-refs with: peeled fully-peeled sorted 
+841f0feb03ffe0f7b216e42e5890b810ce3e720b refs/remotes/origin/ext4
+0b83b19dc757b7ea9db5c051c36b8d935c7b559e refs/remotes/origin/master
+9a7a031137e1b5bc33735fac54ec7be15228c458 refs/remotes/origin/xfs
diff '--color=auto' -uprN linux-5.9.1/.git/refs/heads/master pm-opzero/.git/refs/heads/master
--- linux-5.9.1/.git/refs/heads/master	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/refs/heads/master	2022-12-12 17:21:16.692559502 +0900
@@ -0,0 +1 @@
+0b83b19dc757b7ea9db5c051c36b8d935c7b559e
diff '--color=auto' -uprN linux-5.9.1/.git/refs/remotes/origin/HEAD pm-opzero/.git/refs/remotes/origin/HEAD
--- linux-5.9.1/.git/refs/remotes/origin/HEAD	1970-01-01 09:00:00.000000000 +0900
+++ pm-opzero/.git/refs/remotes/origin/HEAD	2022-12-12 17:21:16.692559502 +0900
@@ -0,0 +1 @@
+ref: refs/remotes/origin/master
diff '--color=auto' -uprN linux-5.9.1/include/linux/fs.h pm-opzero/include/linux/fs.h
--- linux-5.9.1/include/linux/fs.h	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/include/linux/fs.h	2022-12-12 17:21:19.724573355 +0900
@@ -1927,6 +1927,7 @@ struct super_operations {
 	int (*write_inode) (struct inode *, struct writeback_control *wbc);
 	int (*drop_inode) (struct inode *);
 	void (*evict_inode) (struct inode *);
+	void (*evict_zero_inode) (struct inode *);
 	void (*put_super) (struct super_block *);
 	int (*sync_fs)(struct super_block *sb, int wait);
 	int (*freeze_super) (struct super_block *);
@@ -2312,6 +2313,7 @@ extern int current_umask(void);
 
 extern void ihold(struct inode * inode);
 extern void iput(struct inode *);
+extern void iput_zero(struct inode *);
 extern int generic_update_time(struct inode *, struct timespec64 *, int);
 
 /* /sys/fs */
diff '--color=auto' -uprN linux-5.9.1/include/linux/iomap.h pm-opzero/include/linux/iomap.h
--- linux-5.9.1/include/linux/iomap.h	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/include/linux/iomap.h	2022-12-12 17:21:19.736573410 +0900
@@ -170,6 +170,8 @@ int iomap_migrate_page(struct address_sp
 #endif
 int iomap_file_unshare(struct inode *inode, loff_t pos, loff_t len,
 		const struct iomap_ops *ops);
+loff_t iomap_zero_range_actor(struct inode *inode, loff_t pos, loff_t count,
+ 		void *data, struct iomap *iomap, struct iomap *srcmap);
 int iomap_zero_range(struct inode *inode, loff_t pos, loff_t len,
 		bool *did_zero, const struct iomap_ops *ops);
 int iomap_truncate_page(struct inode *inode, loff_t pos, bool *did_zero,
diff '--color=auto' -uprN linux-5.9.1/Makefile pm-opzero/Makefile
--- linux-5.9.1/Makefile	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/Makefile	2022-12-12 17:21:17.008560953 +0900
@@ -2,7 +2,7 @@
 VERSION = 5
 PATCHLEVEL = 9
 SUBLEVEL = 1
-EXTRAVERSION =
+EXTRAVERSION = -zeropute
 NAME = Kleptomaniac Octopus
 
 # *DOCUMENTATION*
diff '--color=auto' -uprN linux-5.9.1/tools/testing/selftests/bpf/test_progs.c pm-opzero/tools/testing/selftests/bpf/test_progs.c
--- linux-5.9.1/tools/testing/selftests/bpf/test_progs.c	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/tools/testing/selftests/bpf/test_progs.c	1970-01-01 09:00:00.000000000 +0900
@@ -1,751 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0-only
-/* Copyright (c) 2017 Facebook
- */
-#define _GNU_SOURCE
-#include "test_progs.h"
-#include "cgroup_helpers.h"
-#include "bpf_rlimit.h"
-#include <argp.h>
-#include <pthread.h>
-#include <sched.h>
-#include <signal.h>
-#include <string.h>
-#include <execinfo.h> /* backtrace */
-
-#define EXIT_NO_TEST		2
-#define EXIT_ERR_SETUP_INFRA	3
-
-/* defined in test_progs.h */
-struct test_env env = {};
-
-struct prog_test_def {
-	const char *test_name;
-	int test_num;
-	void (*run_test)(void);
-	bool force_log;
-	int error_cnt;
-	int skip_cnt;
-	bool tested;
-	bool need_cgroup_cleanup;
-
-	char *subtest_name;
-	int subtest_num;
-
-	/* store counts before subtest started */
-	int old_error_cnt;
-};
-
-/* Override C runtime library's usleep() implementation to ensure nanosleep()
- * is always called. Usleep is frequently used in selftests as a way to
- * trigger kprobe and tracepoints.
- */
-int usleep(useconds_t usec)
-{
-	struct timespec ts = {
-		.tv_sec = usec / 1000000,
-		.tv_nsec = (usec % 1000000) * 1000,
-	};
-
-	return syscall(__NR_nanosleep, &ts, NULL);
-}
-
-static bool should_run(struct test_selector *sel, int num, const char *name)
-{
-	int i;
-
-	for (i = 0; i < sel->blacklist.cnt; i++) {
-		if (strstr(name, sel->blacklist.strs[i]))
-			return false;
-	}
-
-	for (i = 0; i < sel->whitelist.cnt; i++) {
-		if (strstr(name, sel->whitelist.strs[i]))
-			return true;
-	}
-
-	if (!sel->whitelist.cnt && !sel->num_set)
-		return true;
-
-	return num < sel->num_set_len && sel->num_set[num];
-}
-
-static void dump_test_log(const struct prog_test_def *test, bool failed)
-{
-	if (stdout == env.stdout)
-		return;
-
-	fflush(stdout); /* exports env.log_buf & env.log_cnt */
-
-	if (env.verbosity > VERBOSE_NONE || test->force_log || failed) {
-		if (env.log_cnt) {
-			env.log_buf[env.log_cnt] = '\0';
-			fprintf(env.stdout, "%s", env.log_buf);
-			if (env.log_buf[env.log_cnt - 1] != '\n')
-				fprintf(env.stdout, "\n");
-		}
-	}
-
-	fseeko(stdout, 0, SEEK_SET); /* rewind */
-}
-
-static void skip_account(void)
-{
-	if (env.test->skip_cnt) {
-		env.skip_cnt++;
-		env.test->skip_cnt = 0;
-	}
-}
-
-static void stdio_restore(void);
-
-/* A bunch of tests set custom affinity per-thread and/or per-process. Reset
- * it after each test/sub-test.
- */
-static void reset_affinity() {
-
-	cpu_set_t cpuset;
-	int i, err;
-
-	CPU_ZERO(&cpuset);
-	for (i = 0; i < env.nr_cpus; i++)
-		CPU_SET(i, &cpuset);
-
-	err = sched_setaffinity(0, sizeof(cpuset), &cpuset);
-	if (err < 0) {
-		stdio_restore();
-		fprintf(stderr, "Failed to reset process affinity: %d!\n", err);
-		exit(EXIT_ERR_SETUP_INFRA);
-	}
-	err = pthread_setaffinity_np(pthread_self(), sizeof(cpuset), &cpuset);
-	if (err < 0) {
-		stdio_restore();
-		fprintf(stderr, "Failed to reset thread affinity: %d!\n", err);
-		exit(EXIT_ERR_SETUP_INFRA);
-	}
-}
-
-static void save_netns(void)
-{
-	env.saved_netns_fd = open("/proc/self/ns/net", O_RDONLY);
-	if (env.saved_netns_fd == -1) {
-		perror("open(/proc/self/ns/net)");
-		exit(EXIT_ERR_SETUP_INFRA);
-	}
-}
-
-static void restore_netns(void)
-{
-	if (setns(env.saved_netns_fd, CLONE_NEWNET) == -1) {
-		stdio_restore();
-		perror("setns(CLONE_NEWNS)");
-		exit(EXIT_ERR_SETUP_INFRA);
-	}
-}
-
-void test__end_subtest()
-{
-	struct prog_test_def *test = env.test;
-	int sub_error_cnt = test->error_cnt - test->old_error_cnt;
-
-	if (sub_error_cnt)
-		env.fail_cnt++;
-	else
-		env.sub_succ_cnt++;
-	skip_account();
-
-	dump_test_log(test, sub_error_cnt);
-
-	fprintf(env.stdout, "#%d/%d %s:%s\n",
-	       test->test_num, test->subtest_num,
-	       test->subtest_name, sub_error_cnt ? "FAIL" : "OK");
-
-	free(test->subtest_name);
-	test->subtest_name = NULL;
-}
-
-bool test__start_subtest(const char *name)
-{
-	struct prog_test_def *test = env.test;
-
-	if (test->subtest_name)
-		test__end_subtest();
-
-	test->subtest_num++;
-
-	if (!name || !name[0]) {
-		fprintf(env.stderr,
-			"Subtest #%d didn't provide sub-test name!\n",
-			test->subtest_num);
-		return false;
-	}
-
-	if (!should_run(&env.subtest_selector, test->subtest_num, name))
-		return false;
-
-	test->subtest_name = strdup(name);
-	if (!test->subtest_name) {
-		fprintf(env.stderr,
-			"Subtest #%d: failed to copy subtest name!\n",
-			test->subtest_num);
-		return false;
-	}
-	env.test->old_error_cnt = env.test->error_cnt;
-
-	return true;
-}
-
-void test__force_log() {
-	env.test->force_log = true;
-}
-
-void test__skip(void)
-{
-	env.test->skip_cnt++;
-}
-
-void test__fail(void)
-{
-	env.test->error_cnt++;
-}
-
-int test__join_cgroup(const char *path)
-{
-	int fd;
-
-	if (!env.test->need_cgroup_cleanup) {
-		if (setup_cgroup_environment()) {
-			fprintf(stderr,
-				"#%d %s: Failed to setup cgroup environment\n",
-				env.test->test_num, env.test->test_name);
-			return -1;
-		}
-
-		env.test->need_cgroup_cleanup = true;
-	}
-
-	fd = create_and_get_cgroup(path);
-	if (fd < 0) {
-		fprintf(stderr,
-			"#%d %s: Failed to create cgroup '%s' (errno=%d)\n",
-			env.test->test_num, env.test->test_name, path, errno);
-		return fd;
-	}
-
-	if (join_cgroup(path)) {
-		fprintf(stderr,
-			"#%d %s: Failed to join cgroup '%s' (errno=%d)\n",
-			env.test->test_num, env.test->test_name, path, errno);
-		return -1;
-	}
-
-	return fd;
-}
-
-int bpf_find_map(const char *test, struct bpf_object *obj, const char *name)
-{
-	struct bpf_map *map;
-
-	map = bpf_object__find_map_by_name(obj, name);
-	if (!map) {
-		fprintf(stdout, "%s:FAIL:map '%s' not found\n", test, name);
-		test__fail();
-		return -1;
-	}
-	return bpf_map__fd(map);
-}
-
-static bool is_jit_enabled(void)
-{
-	const char *jit_sysctl = "/proc/sys/net/core/bpf_jit_enable";
-	bool enabled = false;
-	int sysctl_fd;
-
-	sysctl_fd = open(jit_sysctl, 0, O_RDONLY);
-	if (sysctl_fd != -1) {
-		char tmpc;
-
-		if (read(sysctl_fd, &tmpc, sizeof(tmpc)) == 1)
-			enabled = (tmpc != '0');
-		close(sysctl_fd);
-	}
-
-	return enabled;
-}
-
-int compare_map_keys(int map1_fd, int map2_fd)
-{
-	__u32 key, next_key;
-	char val_buf[PERF_MAX_STACK_DEPTH *
-		     sizeof(struct bpf_stack_build_id)];
-	int err;
-
-	err = bpf_map_get_next_key(map1_fd, NULL, &key);
-	if (err)
-		return err;
-	err = bpf_map_lookup_elem(map2_fd, &key, val_buf);
-	if (err)
-		return err;
-
-	while (bpf_map_get_next_key(map1_fd, &key, &next_key) == 0) {
-		err = bpf_map_lookup_elem(map2_fd, &next_key, val_buf);
-		if (err)
-			return err;
-
-		key = next_key;
-	}
-	if (errno != ENOENT)
-		return -1;
-
-	return 0;
-}
-
-int compare_stack_ips(int smap_fd, int amap_fd, int stack_trace_len)
-{
-	__u32 key, next_key, *cur_key_p, *next_key_p;
-	char *val_buf1, *val_buf2;
-	int i, err = 0;
-
-	val_buf1 = malloc(stack_trace_len);
-	val_buf2 = malloc(stack_trace_len);
-	cur_key_p = NULL;
-	next_key_p = &key;
-	while (bpf_map_get_next_key(smap_fd, cur_key_p, next_key_p) == 0) {
-		err = bpf_map_lookup_elem(smap_fd, next_key_p, val_buf1);
-		if (err)
-			goto out;
-		err = bpf_map_lookup_elem(amap_fd, next_key_p, val_buf2);
-		if (err)
-			goto out;
-		for (i = 0; i < stack_trace_len; i++) {
-			if (val_buf1[i] != val_buf2[i]) {
-				err = -1;
-				goto out;
-			}
-		}
-		key = *next_key_p;
-		cur_key_p = &key;
-		next_key_p = &next_key;
-	}
-	if (errno != ENOENT)
-		err = -1;
-
-out:
-	free(val_buf1);
-	free(val_buf2);
-	return err;
-}
-
-int extract_build_id(char *build_id, size_t size)
-{
-	FILE *fp;
-	char *line = NULL;
-	size_t len = 0;
-
-	fp = popen("readelf -n ./urandom_read | grep 'Build ID'", "r");
-	if (fp == NULL)
-		return -1;
-
-	if (getline(&line, &len, fp) == -1)
-		goto err;
-	fclose(fp);
-
-	if (len > size)
-		len = size;
-	memcpy(build_id, line, len);
-	build_id[len] = '\0';
-	free(line);
-	return 0;
-err:
-	fclose(fp);
-	return -1;
-}
-
-/* extern declarations for test funcs */
-#define DEFINE_TEST(name) extern void test_##name(void);
-#include <prog_tests/tests.h>
-#undef DEFINE_TEST
-
-static struct prog_test_def prog_test_defs[] = {
-#define DEFINE_TEST(name) {		\
-	.test_name = #name,		\
-	.run_test = &test_##name,	\
-},
-#include <prog_tests/tests.h>
-#undef DEFINE_TEST
-};
-const int prog_test_cnt = ARRAY_SIZE(prog_test_defs);
-
-const char *argp_program_version = "test_progs 0.1";
-const char *argp_program_bug_address = "<bpf@vger.kernel.org>";
-const char argp_program_doc[] = "BPF selftests test runner";
-
-enum ARG_KEYS {
-	ARG_TEST_NUM = 'n',
-	ARG_TEST_NAME = 't',
-	ARG_TEST_NAME_BLACKLIST = 'b',
-	ARG_VERIFIER_STATS = 's',
-	ARG_VERBOSE = 'v',
-	ARG_GET_TEST_CNT = 'c',
-	ARG_LIST_TEST_NAMES = 'l',
-};
-
-static const struct argp_option opts[] = {
-	{ "num", ARG_TEST_NUM, "NUM", 0,
-	  "Run test number NUM only " },
-	{ "name", ARG_TEST_NAME, "NAMES", 0,
-	  "Run tests with names containing any string from NAMES list" },
-	{ "name-blacklist", ARG_TEST_NAME_BLACKLIST, "NAMES", 0,
-	  "Don't run tests with names containing any string from NAMES list" },
-	{ "verifier-stats", ARG_VERIFIER_STATS, NULL, 0,
-	  "Output verifier statistics", },
-	{ "verbose", ARG_VERBOSE, "LEVEL", OPTION_ARG_OPTIONAL,
-	  "Verbose output (use -vv or -vvv for progressively verbose output)" },
-	{ "count", ARG_GET_TEST_CNT, NULL, 0,
-	  "Get number of selected top-level tests " },
-	{ "list", ARG_LIST_TEST_NAMES, NULL, 0,
-	  "List test names that would run (without running them) " },
-	{},
-};
-
-static int libbpf_print_fn(enum libbpf_print_level level,
-			   const char *format, va_list args)
-{
-	if (env.verbosity < VERBOSE_VERY && level == LIBBPF_DEBUG)
-		return 0;
-	vfprintf(stdout, format, args);
-	return 0;
-}
-
-static void free_str_set(const struct str_set *set)
-{
-	int i;
-
-	if (!set)
-		return;
-
-	for (i = 0; i < set->cnt; i++)
-		free((void *)set->strs[i]);
-	free(set->strs);
-}
-
-static int parse_str_list(const char *s, struct str_set *set)
-{
-	char *input, *state = NULL, *next, **tmp, **strs = NULL;
-	int cnt = 0;
-
-	input = strdup(s);
-	if (!input)
-		return -ENOMEM;
-
-	set->cnt = 0;
-	set->strs = NULL;
-
-	while ((next = strtok_r(state ? NULL : input, ",", &state))) {
-		tmp = realloc(strs, sizeof(*strs) * (cnt + 1));
-		if (!tmp)
-			goto err;
-		strs = tmp;
-
-		strs[cnt] = strdup(next);
-		if (!strs[cnt])
-			goto err;
-
-		cnt++;
-	}
-
-	set->cnt = cnt;
-	set->strs = (const char **)strs;
-	free(input);
-	return 0;
-err:
-	free(strs);
-	free(input);
-	return -ENOMEM;
-}
-
-extern int extra_prog_load_log_flags;
-
-static error_t parse_arg(int key, char *arg, struct argp_state *state)
-{
-	struct test_env *env = state->input;
-
-	switch (key) {
-	case ARG_TEST_NUM: {
-		char *subtest_str = strchr(arg, '/');
-
-		if (subtest_str) {
-			*subtest_str = '\0';
-			if (parse_num_list(subtest_str + 1,
-					   &env->subtest_selector.num_set,
-					   &env->subtest_selector.num_set_len)) {
-				fprintf(stderr,
-					"Failed to parse subtest numbers.\n");
-				return -EINVAL;
-			}
-		}
-		if (parse_num_list(arg, &env->test_selector.num_set,
-				   &env->test_selector.num_set_len)) {
-			fprintf(stderr, "Failed to parse test numbers.\n");
-			return -EINVAL;
-		}
-		break;
-	}
-	case ARG_TEST_NAME: {
-		char *subtest_str = strchr(arg, '/');
-
-		if (subtest_str) {
-			*subtest_str = '\0';
-			if (parse_str_list(subtest_str + 1,
-					   &env->subtest_selector.whitelist))
-				return -ENOMEM;
-		}
-		if (parse_str_list(arg, &env->test_selector.whitelist))
-			return -ENOMEM;
-		break;
-	}
-	case ARG_TEST_NAME_BLACKLIST: {
-		char *subtest_str = strchr(arg, '/');
-
-		if (subtest_str) {
-			*subtest_str = '\0';
-			if (parse_str_list(subtest_str + 1,
-					   &env->subtest_selector.blacklist))
-				return -ENOMEM;
-		}
-		if (parse_str_list(arg, &env->test_selector.blacklist))
-			return -ENOMEM;
-		break;
-	}
-	case ARG_VERIFIER_STATS:
-		env->verifier_stats = true;
-		break;
-	case ARG_VERBOSE:
-		env->verbosity = VERBOSE_NORMAL;
-		if (arg) {
-			if (strcmp(arg, "v") == 0) {
-				env->verbosity = VERBOSE_VERY;
-				extra_prog_load_log_flags = 1;
-			} else if (strcmp(arg, "vv") == 0) {
-				env->verbosity = VERBOSE_SUPER;
-				extra_prog_load_log_flags = 2;
-			} else {
-				fprintf(stderr,
-					"Unrecognized verbosity setting ('%s'), only -v and -vv are supported\n",
-					arg);
-				return -EINVAL;
-			}
-		}
-		break;
-	case ARG_GET_TEST_CNT:
-		env->get_test_cnt = true;
-		break;
-	case ARG_LIST_TEST_NAMES:
-		env->list_test_names = true;
-		break;
-	case ARGP_KEY_ARG:
-		argp_usage(state);
-		break;
-	case ARGP_KEY_END:
-		break;
-	default:
-		return ARGP_ERR_UNKNOWN;
-	}
-	return 0;
-}
-
-static void stdio_hijack(void)
-{
-#ifdef __GLIBC__
-	env.stdout = stdout;
-	env.stderr = stderr;
-
-	if (env.verbosity > VERBOSE_NONE) {
-		/* nothing to do, output to stdout by default */
-		return;
-	}
-
-	/* stdout and stderr -> buffer */
-	fflush(stdout);
-
-	stdout = open_memstream(&env.log_buf, &env.log_cnt);
-	if (!stdout) {
-		stdout = env.stdout;
-		perror("open_memstream");
-		return;
-	}
-
-	stderr = stdout;
-#endif
-}
-
-static void stdio_restore(void)
-{
-#ifdef __GLIBC__
-	if (stdout == env.stdout)
-		return;
-
-	fclose(stdout);
-	free(env.log_buf);
-
-	env.log_buf = NULL;
-	env.log_cnt = 0;
-
-	stdout = env.stdout;
-	stderr = env.stderr;
-#endif
-}
-
-/*
- * Determine if test_progs is running as a "flavored" test runner and switch
- * into corresponding sub-directory to load correct BPF objects.
- *
- * This is done by looking at executable name. If it contains "-flavor"
- * suffix, then we are running as a flavored test runner.
- */
-int cd_flavor_subdir(const char *exec_name)
-{
-	/* General form of argv[0] passed here is:
-	 * some/path/to/test_progs[-flavor], where -flavor part is optional.
-	 * First cut out "test_progs[-flavor]" part, then extract "flavor"
-	 * part, if it's there.
-	 */
-	const char *flavor = strrchr(exec_name, '/');
-
-	if (!flavor)
-		return 0;
-	flavor++;
-	flavor = strrchr(flavor, '-');
-	if (!flavor)
-		return 0;
-	flavor++;
-	if (env.verbosity > VERBOSE_NONE)
-		fprintf(stdout,	"Switching to flavor '%s' subdirectory...\n", flavor);
-
-	return chdir(flavor);
-}
-
-#define MAX_BACKTRACE_SZ 128
-void crash_handler(int signum)
-{
-	void *bt[MAX_BACKTRACE_SZ];
-	size_t sz;
-
-	sz = backtrace(bt, ARRAY_SIZE(bt));
-
-	if (env.test)
-		dump_test_log(env.test, true);
-	if (env.stdout)
-		stdio_restore();
-
-	fprintf(stderr, "Caught signal #%d!\nStack trace:\n", signum);
-	backtrace_symbols_fd(bt, sz, STDERR_FILENO);
-}
-
-int main(int argc, char **argv)
-{
-	static const struct argp argp = {
-		.options = opts,
-		.parser = parse_arg,
-		.doc = argp_program_doc,
-	};
-	struct sigaction sigact = {
-		.sa_handler = crash_handler,
-		.sa_flags = SA_RESETHAND,
-	};
-	int err, i;
-
-	sigaction(SIGSEGV, &sigact, NULL);
-
-	err = argp_parse(&argp, argc, argv, 0, NULL, &env);
-	if (err)
-		return err;
-
-	err = cd_flavor_subdir(argv[0]);
-	if (err)
-		return err;
-
-	libbpf_set_print(libbpf_print_fn);
-
-	srand(time(NULL));
-
-	env.jit_enabled = is_jit_enabled();
-	env.nr_cpus = libbpf_num_possible_cpus();
-	if (env.nr_cpus < 0) {
-		fprintf(stderr, "Failed to get number of CPUs: %d!\n",
-			env.nr_cpus);
-		return -1;
-	}
-
-	save_netns();
-	stdio_hijack();
-	for (i = 0; i < prog_test_cnt; i++) {
-		struct prog_test_def *test = &prog_test_defs[i];
-
-		env.test = test;
-		test->test_num = i + 1;
-
-		if (!should_run(&env.test_selector,
-				test->test_num, test->test_name))
-			continue;
-
-		if (env.get_test_cnt) {
-			env.succ_cnt++;
-			continue;
-		}
-
-		if (env.list_test_names) {
-			fprintf(env.stdout, "%s\n", test->test_name);
-			env.succ_cnt++;
-			continue;
-		}
-
-		test->run_test();
-		/* ensure last sub-test is finalized properly */
-		if (test->subtest_name)
-			test__end_subtest();
-
-		test->tested = true;
-		if (test->error_cnt)
-			env.fail_cnt++;
-		else
-			env.succ_cnt++;
-		skip_account();
-
-		dump_test_log(test, test->error_cnt);
-
-		fprintf(env.stdout, "#%d %s:%s\n",
-			test->test_num, test->test_name,
-			test->error_cnt ? "FAIL" : "OK");
-
-		reset_affinity();
-		restore_netns();
-		if (test->need_cgroup_cleanup)
-			cleanup_cgroup_environment();
-	}
-	stdio_restore();
-
-	if (env.get_test_cnt) {
-		printf("%d\n", env.succ_cnt);
-		goto out;
-	}
-
-	if (env.list_test_names)
-		goto out;
-
-	fprintf(stdout, "Summary: %d/%d PASSED, %d SKIPPED, %d FAILED\n",
-		env.succ_cnt, env.sub_succ_cnt, env.skip_cnt, env.fail_cnt);
-
-out:
-	free_str_set(&env.test_selector.blacklist);
-	free_str_set(&env.test_selector.whitelist);
-	free(env.test_selector.num_set);
-	free_str_set(&env.subtest_selector.blacklist);
-	free_str_set(&env.subtest_selector.whitelist);
-	free(env.subtest_selector.num_set);
-	close(env.saved_netns_fd);
-
-	if (env.succ_cnt + env.fail_cnt + env.skip_cnt == 0)
-		return EXIT_NO_TEST;
-
-	return env.fail_cnt ? EXIT_FAILURE : EXIT_SUCCESS;
-}
diff '--color=auto' -uprN linux-5.9.1/tools/testing/selftests/bpf/test_progs.h pm-opzero/tools/testing/selftests/bpf/test_progs.h
--- linux-5.9.1/tools/testing/selftests/bpf/test_progs.h	2020-10-17 15:31:22.000000000 +0900
+++ pm-opzero/tools/testing/selftests/bpf/test_progs.h	1970-01-01 09:00:00.000000000 +0900
@@ -1,154 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-#include <stdio.h>
-#include <unistd.h>
-#include <errno.h>
-#include <string.h>
-#include <assert.h>
-#include <stdlib.h>
-#include <stdarg.h>
-#include <time.h>
-#include <signal.h>
-
-#include <linux/types.h>
-typedef __u16 __sum16;
-#include <arpa/inet.h>
-#include <linux/if_ether.h>
-#include <linux/if_packet.h>
-#include <linux/ip.h>
-#include <linux/ipv6.h>
-#include <netinet/tcp.h>
-#include <linux/filter.h>
-#include <linux/perf_event.h>
-#include <linux/socket.h>
-#include <linux/unistd.h>
-
-#include <sys/ioctl.h>
-#include <sys/wait.h>
-#include <sys/types.h>
-#include <sys/time.h>
-#include <fcntl.h>
-#include <pthread.h>
-#include <linux/bpf.h>
-#include <linux/err.h>
-#include <bpf/bpf.h>
-#include <bpf/libbpf.h>
-
-#include "test_iptunnel_common.h"
-#include "bpf_util.h"
-#include <bpf/bpf_endian.h>
-#include "trace_helpers.h"
-#include "testing_helpers.h"
-#include "flow_dissector_load.h"
-
-enum verbosity {
-	VERBOSE_NONE,
-	VERBOSE_NORMAL,
-	VERBOSE_VERY,
-	VERBOSE_SUPER,
-};
-
-struct str_set {
-	const char **strs;
-	int cnt;
-};
-
-struct test_selector {
-	struct str_set whitelist;
-	struct str_set blacklist;
-	bool *num_set;
-	int num_set_len;
-};
-
-struct test_env {
-	struct test_selector test_selector;
-	struct test_selector subtest_selector;
-	bool verifier_stats;
-	enum verbosity verbosity;
-
-	bool jit_enabled;
-	bool get_test_cnt;
-	bool list_test_names;
-
-	struct prog_test_def *test;
-	FILE *stdout;
-	FILE *stderr;
-	char *log_buf;
-	size_t log_cnt;
-	int nr_cpus;
-
-	int succ_cnt; /* successful tests */
-	int sub_succ_cnt; /* successful sub-tests */
-	int fail_cnt; /* total failed tests + sub-tests */
-	int skip_cnt; /* skipped tests */
-
-	int saved_netns_fd;
-};
-
-extern struct test_env env;
-
-extern void test__force_log();
-extern bool test__start_subtest(const char *name);
-extern void test__skip(void);
-extern void test__fail(void);
-extern int test__join_cgroup(const char *path);
-
-#define PRINT_FAIL(format...)                                                  \
-	({                                                                     \
-		test__fail();                                                  \
-		fprintf(stdout, "%s:FAIL:%d ", __func__, __LINE__);            \
-		fprintf(stdout, ##format);                                     \
-	})
-
-#define _CHECK(condition, tag, duration, format...) ({			\
-	int __ret = !!(condition);					\
-	int __save_errno = errno;					\
-	if (__ret) {							\
-		test__fail();						\
-		fprintf(stdout, "%s:FAIL:%s ", __func__, tag);		\
-		fprintf(stdout, ##format);				\
-	} else {							\
-		fprintf(stdout, "%s:PASS:%s %d nsec\n",			\
-		       __func__, tag, duration);			\
-	}								\
-	errno = __save_errno;						\
-	__ret;								\
-})
-
-#define CHECK_FAIL(condition) ({					\
-	int __ret = !!(condition);					\
-	int __save_errno = errno;					\
-	if (__ret) {							\
-		test__fail();						\
-		fprintf(stdout, "%s:FAIL:%d\n", __func__, __LINE__);	\
-	}								\
-	errno = __save_errno;						\
-	__ret;								\
-})
-
-#define CHECK(condition, tag, format...) \
-	_CHECK(condition, tag, duration, format)
-#define CHECK_ATTR(condition, tag, format...) \
-	_CHECK(condition, tag, tattr.duration, format)
-
-static inline __u64 ptr_to_u64(const void *ptr)
-{
-	return (__u64) (unsigned long) ptr;
-}
-
-static inline void *u64_to_ptr(__u64 ptr)
-{
-	return (void *) (unsigned long) ptr;
-}
-
-int bpf_find_map(const char *test, struct bpf_object *obj, const char *name);
-int compare_map_keys(int map1_fd, int map2_fd);
-int compare_stack_ips(int smap_fd, int amap_fd, int stack_trace_len);
-int extract_build_id(char *build_id, size_t size);
-
-#ifdef __x86_64__
-#define SYS_NANOSLEEP_KPROBE_NAME "__x64_sys_nanosleep"
-#elif defined(__s390x__)
-#define SYS_NANOSLEEP_KPROBE_NAME "__s390x_sys_nanosleep"
-#else
-#define SYS_NANOSLEEP_KPROBE_NAME "sys_nanosleep"
-#endif
